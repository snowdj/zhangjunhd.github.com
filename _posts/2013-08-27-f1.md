---
layout: post
title: "F1"
description: ""
category: tech
tags: [paper, google, F1, spanner]
---
{% include JB/setup %}
paper review:[F1: A Distributed SQL Database That Scales](http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/zh-CN//pubs/archive/41344.pdf)

<!--break-->
##1. INTRODUCTION
The key goals of F1’s design are:

* Scalability: The system must be able to scale up, trivially and transparently, just by adding resources. Our sharded database based on MySQL was hard to scale up, and even more difficult to rebalance. Our users needed complex queries and joins, which meant they had to carefully shard their data, and resharding data without breaking applications was challenging.
* Availability: The system must never go down for any reason – datacenter outages, planned maintenance, schema changes, etc. The system stores data for Google’s core business. Any downtime has a significant revenue impact.
* Consistency: The system must provide ACID transactions, and must always present applications with consistent and correct data.
* Usability: The system must provide full SQL query support and other functionality users expect from a SQL database. Features like indexes and ad hoc query are not just nice to have, but absolute requirements for our business.

A key contribution of this paper is to show how we achieved all of these goals in F1’s design, and where we made trade-offs and sacrifices. 

F1 is built on top of Spanner, which provides extremely scalable data storage, synchronous replication, and strong consistency and ordering properties. F1 inherits those features from Spanner and adds several more:

* Distributed SQL queries, including joining data from external data sources
* Transactionally consistent secondary indexes
* Asynchronous schema changes including database re-organizations
* Optimistic transactions
* Automatic change history recording and publishing

Our design choices in F1 result in higher latency for typical reads and writes. We have developed techniques to hide that increased latency, and we found that user-facing transactions can be made to perform as well as in our previous MySQL system:

* An F1 schema makes data clustering explicit, using tables with hierarchical relationships and columns with structured data types. This clustering improves data locality and reduces the number and cost of RPCs required to read remote data.
* F1 users make heavy use of batching, parallelism and asynchronous reads. We use a new ORM (object-relational mapping) library that makes these concepts explicit. This places an upper bound on the number of RPCs required for typical application-level operations, making those operations scale well by default.

##2. BASIC ARCHITECTURE
Users interact with F1 through the F1 `client` library. The client sends requests to one of many F1 `servers`, which are responsible for reading and writing data from remote data sources and coordinating query execution.

![1](/assets/2013-08-27-f1/1.png)

F1 servers are mostly stateless, allowing a client to communicate with a different F1 server for each request. The one exception is when a client uses pessimistic transactions and must hold locks. The client is then bound to one F1 server for the duration of that transaction. F1 servers can be quickly added (or removed) from our system in response to the total load because F1 servers do not own any data and hence a server addition (or removal) requires no data movement.

An F1 cluster has several additional components that allow for the execution of distributed SQL queries. Distributed execution is chosen over centralized execution when the query planner estimates that increased parallelism will reduce query processing latency. The shared `slave pool` consists of F1 processes that exist only to execute parts of distributed query plans on behalf of regular F1 servers. Slave pool membership is maintained by the F1 `master`, which monitors slave process health and distributes the list of available slaves to F1 servers.

The throughput of the entire system can be scaled up by adding more Spanner servers, F1 servers, or F1 slaves. Since F1 servers do not store data, adding new servers does not involve any data re-distribution costs. Adding new Spanner servers results in data re-distribution. This process is comletely transparent to F1 servers (and therefore F1 clients).

####2.1 Spanner
Spanner handles lower-level storage issues like persistence, caching, replication, fault tolerance, data sharding and movement, location lookups, and transactions.

##3. DATA MODEL
####3.1 Hierarchical Schema
Logically, tables in the F1 schema can be organized into a `hierarchy`. Physically, F1 stores each child table `clustered` with and `interleaved` within the rows from its parent table. Tables from the logical schema cannot be arbitrarily interleaved: the child table must have a foreign key to its parent table as a prefix of its primary key. For example, the AdWords schema contains a table Customer with primary key (CustomerId), which has a child table Campaign with primary key (CustomerId, CampaignId), which in turn has a child table AdGroup with primary key (CustomerId, CampaignId, AdGroupId). A row of the root table in the hierarchy is called a `root row`. All child table rows corresponding to a root row are clustered together with that root row in a single Spanner `directory`, meaning that cluster is normally stored on a single Spanner server. Child rows are stored under their parent row ordered by primary key. Figure 2 shows an example.

![2](/assets/2013-08-27-f1/2.png)

The hierarchically clustered physical schema has several advantages over a flat relational schema.

* In this traditional schema, fetching all Campaign and AdGroup records corresponding to a given CustomerId would take two sequential steps, because there is no direct way to retrieve AdGroup records by CustomerId. In the F1 version of the schema, the hierarchical primary keys allow the fetches of Campaign and AdGroup records to be started in parallel, because both tables are keyed by CustomerId. The primary key prefix property means that reading all AdGroups for a particular Customer can be expressed as a single range read, rather than reading each row individually using an index.
* Furthermore, because the tables are both stored in primary key order, rows from the two tables can be joined using a simple ordered merge. Because the data is clustered into a single directory, we can read it all in a single Spanner request.
* Hierarchical clustering is especially useful for updates, since it reduces the number of Spanner groups involved in a transaction. Because each root row and all of its descendant rows are stored in a single Spanner directory, transactions restricted to a single root will usually avoid 2PC and the associated latency penalty, so most applications try to use single-root transactions as much as possible. Even when doing transactions across multiple roots, it is important to limit the number of roots involved because adding more participants generally increases latency and decreases the likelihood of a successful commit.

####3.2 Protocol Buffers
Protocol Buffers allow the use of repeated fields. In F1 schema designs, we often use repeated fields instead of child tables when the number of child records has a low upper bound. By using repeated fields, we avoid the performance overhead and complexity of storing and joining multiple child records.

Tables can be partitioned into columns to group together fields that are usually accessed together, to separate fields with static and frequently updated data, to allow specifying different read/write permissions per column, or to allow concurrent updates to different columns.

####3.3 Indexing
All indexes in F1 are transactional and fully consistent. Indexes are stored as separate tables in Spanner, keyed by a concatenation of the index key and the indexed table’s primary key.

`Local index` keys must contain the root row primary key as a prefix. For example, an index on (CustomerId, Keyword) used to store unique keywords for each customer is a local index. Like child tables, local indexes are stored in the same Spanner directory as the root row. Consequently, the index entries of local indexes are stored on the same Spanner server as the rows they index, and local index updates add little additional cost to any transaction.

In contrast, `global index` keys do not include the root row primary key as a prefix and hence cannot be co-located with the rows they index. For example, an index on (Keyword) that maps from all keywords in the database to Customers that use them must be global. Global indexes are often large and can have high aggregate update rates. Consequently, they are sharded across many directories and stored on multiple Spanner servers. Writing a single row that updates a global index requires adding a single extra participant to a transaction, which means the transaction must use 2PC, but that is a reasonable cost to pay for consistent global indexes.

##4. SCHEMA CHANGES
Because F1 is massively distributed, even if F1 had a global F1 server membership repository, synchronous schema change across all servers would be very disruptive to response times. To make changes atomic, at some point, servers would have to block transactions until confirming all other servers have received the change. To avoid this, F1 schema changes are applied `asynchronously`, on different F1 servers at different times. This implies that two F1 servers may update the database concurrently using different schemas.

Consider a schema change from schema S1 to schema S2 that adds index I on table T. Because the schema change is applied asynchronously on different F1 servers, assume that server M1 is using schema S1 and server M2 is using schema S2. First, server M2 inserts a new row r, which also adds a new index entry I(r) for row r. Subsequently, row r is deleted by server M1. Because the server is using schema S1 and is not aware of index I, the server deletes row r, but fails to delete the index entry I(r). Hence, the database becomes corrupt. For example, an index scan on I would return spurious data corresponding to the deleted row r.

We have implemented a schema change algorithm that prevents anomalies similar to the above by

* Enforcing that across all F1 servers, at most two different schemas are active. Each server uses either the current or next schema. We grant leases on the schema and ensure that no server uses a schema after lease expiry.
* Subdividing each schema change into multiple phases where consecutive pairs of phases are mutually compatible and cannot cause anomalies. In the above example, we first add index I in a mode where it only executes delete operations. This prohibits server M1 from adding I(r) into the database. Subsequently, we upgrade index I so servers perform all write operations. Then we initiate a MapReduce to backfill index entries for all rows in table T with carefully constructed trans- actions to handle concurrent writes. Once complete, we make index I visible for normal read operations.

##5. TRANSACTIONS
Each F1 transaction consists of multiple reads, optionally followed by a single write that commits the transaction. F1 implements three types of transactions, all built on top of Spanner’s transaction support:

* Snapshot transactions. These are read-only transactions with snapshot semantics, reading repeatable data as of a fixed Spanner snapshot timestamp. By default, snapshot transactions read at Spanner’s global safe timestamp, typically 5-10 seconds old, and read from a local Spanner replica.
* Pessimistic transactions. These transactions map directly on to Spanner transactions. Pessimistic transactions use a stateful communications protocol that requires holding locks, so all requests in a single pessimistic transaction get directed to the same F1 server. If the F1 server restarts, the pessimistic transaction aborts. Reads in pessimistic transactions can request either shared or exclusive locks.
* Optimistic transactions. Optimistic transactions consist of a read phase, which can take arbitrarily long and never takes Spanner locks, and then a short write phase. To detect row-level conflicts, F1 returns with each row its last modification timestamp, which is stored in a hidden `lock column` in that row. The new commit timestamp is automatically written into the lock column whenever the corresponding data is updated (in either pessimistic or optimistic transactions). The client library collects these timestamps, and passes them back to an F1 server with the write that commits the transaction. The F1 server creates a short-lived Spanner pessimistic transaction and re- reads the last modification timestamps for all read rows. If any of the re-read timestamps differ from what was passed in by the client, there was a conflicting update, and F1 aborts the transaction. Otherwise, F1 sends the writes on to Spanner to finish the commit.

####5.1 Flexible Locking Granularity
F1 provides row-level locking by default. Each F1 row contains one `default lock column` that covers all columns in the same row. However, concurrency levels can be changed in the schema. For example, users can increase concurrency by defining additional lock columns in the same row, with each lock column covering a subset of columns. In an extreme case, each column can be covered by a separate lock column, resulting in column-level locking.

##6. CHANGE HISTORY
Every transaction in F1 creates one or more `ChangeBatch` Protocol Buffers, which include the primary key and before and after values of changed columns for each updated row. These ChangeBatches are written into normal F1 tables that exist as children of each root table. The primary key of the ChangeBatch table includes the associated root table key and the transaction commit timestamp. When a transaction updates data under multiple root rows, possibly from different root table hierarchies, one ChangeBatch is written for each distinct root row (and these ChangeBatches include pointers to each other so the full transaction can be reassembled if necessary). This means that for each root row, the change history table includes ChangeBatches showing all changes associated with children of that root row, in commit order, and this data is easily queryable with SQL. This clustering also means that change history is stored close to the data being tracked, so these additional writes normally do not add additional participants into Spanner transactions, and therefore have minimal latency impact.

##7. CLIENT DESIGN

* Simplified ORM
* NoSQL Interface
* SQL Interface

##8. QUERY PROCESSING
The F1 SQL query processing system has the following key properties which we will elaborate on in this section:

* Queries are executed either as low-latency centrally executed queries or distributed queries with high parallelism.
* All data is remote and batching is used heavily to mitigate network latency.
* All input data and internal data is arbitrarily partitioned and has few useful ordering properties.
* Queries use many hash-based repartitioning steps.
* Individual query plan operators are designed to stream data to later operators as soon as possible, maximizing pipelining in query plans.
* Hierarchically clustered tables have optimized access methods.
* Query data can be consumed in parallel.
* Protocol Buffer-valued columns provide first-class support for structured data types.
* Spanner’s snapshot consistency model provides globally consistent results.

####8.1 Central and Distributed Queries
F1 SQL supports both centralized and distributed execution of queries. Centralized execution is used for short OLTP-style queries and the entire query runs on one F1 server node. Distributed execution is used for OLAP-style queries and spreads the query workload over worker tasks in the F1 slave pool.

####8.2 Distributed Query Example

    SELECT agcr.CampaignId, click.Region,cr.Language, SUM(click.Clicks)
    FROM AdClick click
    JOIN AdGroupCreative agcr USING (AdGroupId, CreativeId)
    JOIN Creative cr USING (CustomerId, CreativeId)
    WHERE click.Date = '2013-03-23'
    GROUP BY agcr.CampaignId, click.Region,cr.Language

![3](/assets/2013-08-27-f1/3.png)

A possible query plan for this query is shown in Figure 3. In the query plan, data is streamed bottom-up through each of the operators up until the aggregation operator. The deepest operator performs a scan of the AdClick table. In the same worker node, the data from the AdClick scan flows into a lookup join operator, which looks up AdGroupCreative records using a secondary index key. The plan then repartitions the data stream by a hash of the CustomerId and CreativeId, and performs a lookup in a hash table that is partitioned in the same way (a distributed hash join). After the distributed hash join, the data is once again repartitioned, this time by a hash of the CampaignId, Region and Language fields, and then fed into an aggregation operator that groups by those same fields (a distributed aggregation).

####8.3 Remote Data
Network latency and disk latency are fundamentally different in two ways. 

* First, network latency can be mitigated by batching or pipelining data accesses. F1 uses extensive batching to mitigate network latency.
* Secondly, disk latency is generally caused by contention for a single limited resource, the actual disk hardware. In contrast, F1’s network based storage is typically distributed over many disks, because Spanner partitions its data across many physical servers, and also at a finer-grained level because Spanner stores its data in CFS. This makes it much less likely that multiple data accesses will contend for the same resources, so scheduling multiple data accesses in parallel often results in near-linear speedup until the underlying storage system is truly overloaded.

####8.4 Distributed Execution Overview
The structure of a distributed query plan is as follows. A full query plan consists of potentially tens of `plan parts`, each of which represents a number of workers that execute the same query subplan. The plan parts are organized as a directed acyclic graph (DAG), with data flowing up from the leaves of the DAG to a single `root node`, which is the only node with no out edges, i.e. the only sink. The root node, also called the `query coordinator`, is executed by the server that received the incoming SQL query request from a client. The query coordinator plans the query for execution, receives results from the penultimate plan parts, performs any final aggregation, sorting, or filtering, and then streams the results back to the client, except in the case of partitioned consumers as described in Section 8.6.

The use of hash partitioning allows us to implement an efficient distributed hash join operator and a distributed aggregation operator. 

####8.5 Hierarchical Table Joins
F1 data model supports hierarchically clustered tables, where the rows of a child table are interleaved in the parent table. This data model allows us to efficiently join a parent table and a descendant table by their shared primary key prefix. 

####8.6 Partitioned Consumers
F1 queries can produce vast amounts of data, and pushing this data through a single query coordinator can be a bottleneck. Furthermore, a single client process receiving all the data can also be a bottleneck and likely cannot keep up with many F1 servers producing result rows in parallel. To solve this, F1 allows multiple client processes to consume sharded streams of data from the same query in parallel. This feature is used for partitioned consumers like MapReduces. The client application sends the query to F1 and requests distributed data retrieval. F1 then returns a set of endpoints to connect to. The client must connect to all of these endpoints and retrieve the data in parallel. 

####8.7 Queries with Protocol Buffers
Protocol Buffers also allow repeated fields, which may have zero or more instances, i.e., they can be regarded as variable-length arrays. When these repeated fields occur in F1 database columns, they are actually very similar to hierarchical child tables in a 1:N relationship. The main difference between a child table and a repeated field is that the child table contains an explicit foreign key to its parent table, while the repeated field has an implicit foreign key to the Protocol Buffer containing it. Capitalizing on this similarity, F1 SQL supports access to repeated fields using `PROTO JOIN`, a JOIN variant that joins by the implicit foreign key. 

Protocol Buffers have performance implications for query processing. First, we always have to fetch entire Protocol Buffer columns from Spanner, even when we are only interested in a small subset of fields. This takes both additional network and disk bandwidth. Second, in order to extract the fields that the query refers to, we always have to parse the contents of the Protocol Buffer fields. Even though we have implemented an optimized parser to extract only requested fields, the impact of this decoding step is significant.

