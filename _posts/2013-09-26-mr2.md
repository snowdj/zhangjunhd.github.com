---
layout: post
title: "MapReduce数据处理性能分析"
description: "paper1 优化Speculative Task；paper2 分析了task跑的慢的几大原因，统计显示其普遍存在，并针对read input和执行阶段提出相应的优化策略"
category: 云计算
tags: [MapReduce]
---
{% include JB/setup %}

###1 [Improving MapReduce Performance in Heterogeneous Environments][1]
####1.1 Background: Scheduling in Hadoop
1. General Concept
    * There is a single `master` managing a number of `slaves`.
    * The input file, which resides on a distributed filesystem throughout the cluster, is split into evensized `chunks` replicated for fault-tolerance.
    * Hadoop divides each MapReduce job into a set of `tasks`.
    * Each chunk of input is first processed by a `map` task, which outputs a list of key-value pairs generated by a user-defined map function. Map outputs are split into buckets based on key. When all maps have finished, `reduce` tasks apply a reduce function to the list of map outputs with each key. 
    * The goal of speculative execution is to minimize a job’s `response time`.

2. Speculative Execution in Hadoop
    * When a node has an empty task slot, Hadoop chooses a task for it from one of three categories：
        * First, any failed tasks are given highest priority.
        * Second, non-running tasks are considered. For maps, tasks with data local to the node are chosen first.
        * Finally, Hadoop looks for a task to execute speculatively.
    * To select speculative tasks, Hadoop monitors task progress using a `progress score` between 0 and 1.
        * For a map, the progress score is the fraction of input data read.
        * For a reduce task, the execution is divided into three phases, each of which accounts for 1/3 of the score:
            * The `copy` phase, when the task fetches map outputs.
            * The `sort` phase, when map outputs are sorted by key.
            * The `reduce` phase, when a user-defined function is applied to the list of map outputs with each key.
        * In each phase, the score is the fraction of data processed.
            * a task halfway through the copy phase has a progress score of 1/2 * 1/3 = 1/6
            * a task halfway through the reduce phase scores 1/3 + 1/3 + 1/2 * 1/3 = 5/6

3. Assumptions in Hadoop’s Scheduler
    * Nodes can perform work at roughly the same rate.
    * Tasks progress at a constant rate throughout time.
    * There is no cost to launching a speculative task[J注1] on a node that would otherwise have an idle slot.
    * A task’s progress score is representative of fraction of its total work that it has done. Specifically, in a reduce task, the copy, sort and reduce phases each take about 1/3 of the total time.
    * Tasks tend to finish in waves, so a task with a low progress score is likely a straggler.
    * Tasks in the same category (map or reduce) require roughly the same amount of work.

####1.2 How the Assumptions Break Down
1. Heterogeneity
    * Because the scheduler uses a fixed threshold for selecting tasks to speculate, too many speculative tasks may be launched, taking away resources from useful tasks (assumption 3 is also untrue). 
    * Also, because the scheduler ranks candidates by locality, the wrong tasks may be chosen for speculation first. For example, if the average progress was 70% and there was a 2x slower task at 35% progress and a 10x slower task at 7% progress, then the 2x slower task might be speculated before the 10x slower task if its input data was available on an idle node.

2. Other Assumptions
    * Assumption 3, that speculating tasks on idle nodes costs nothing, breaks down when resources are shared. For example, the network is a bottleneck shared resource in large MapReduce jobs. Also, speculative tasks may compete for disk I/O in I/O-bound jobs.
    * Assumption 4, that a task’s progress score is approximately equal to its percent completion, can cause incorrect speculation of reducers. In a typical MapReduce job, the copy phase of reduce tasks is the slowest, because it involves all-pairs communication over the network. Tasks quickly complete the other two phases once they have all map outputs. However, the copy phase counts for only 1/3 of the progress score. Thus, soon after the first few reducers in a job finish the copy phase, their progress goes from 1/3 to 1, greatly increasing the average progress.
    * Assumption 5, that progress score is a good proxy for progress rate because tasks begin at roughly the same time, can also be wrong. The number of reducers in a Hadoop job is typically chosen small enough so that they they can all start running right away, to copy data while maps run. However, there are potentially tens of mappers per node, one for each data chunk. The mappers tend to run in waves.

####1.3 The LATE Scheduler
1. The primary insight behind our algorithm is as follows: We always speculatively execute the task that we think will `finish farthest into the future`, because this task provides the greatest opportunity for a speculative copy to overtake the original and reduce the job’s response time. We call our strategy LATE, for Longest Approximate Time to End.
2. We estimate the progress rate of each task as `ProgressScore/T`, where T is the amount of time the task has been running for, and then estimate the time to completion as `(1 − ProgressScore)/ProgressRate`. 
3. To really get the best chance of beating the original task with the speculative task, we should also `only launch speculative tasks on fast nodes` – not stragglers. We do this through a simple heuristic – don’t launch speculative tasks on nodes that are below some threshold, `SlowNodeThreshold`, of total work performed (sum of progress scores for all succeeded and in-progress tasks on the node).
4. Finally, to handle the fact that speculative tasks cost resources, we augment the algorithm with two heuristics:
    * A cap on the number of speculative tasks that can be running at once, which we denote `SpeculativeCap`.
    * A `SlowTaskThreshold` that a task’s progress rate is compared with to determine whether it is “slow enough” to be speculated upon. This prevents needless speculation when only fast tasks are running.
5. In summary, the LATE algorithm works as follows:
    * If a node asks for a new task and there are fewer than SpeculativeCap speculative tasks running:
        * Ignore the request if the node’s total progress is below `SlowNodeThreshold`.
        * Rank currently running tasks that are not currently being speculated by estimated time left.
        * Launch a copy of the highest-ranked task with progress rate below `SlowTaskThreshold`.
6. Finally, we note that unlike Hadoop’s scheduler, LATE does not take into account data locality for launching speculative map tasks, although this is a potential extension. We assume that because most maps are data-local, network utilization during the map phase is low, so it is fine to launch a speculative task on a fast node that does not have a local copy of the data. Locality statistics available in Hadoop validate this assumption.

###2 [Reining in the Outliers in Map-Reduce Clusters using Mantrig][3]
####2.1 Introduction
We identify three categories of root causes for outliers that are induced by the interplay between `storage`, `network` and `structure of Map-Reduce jobs`. 

* First, machine characteristics play a key role in the performance of tasks.
* Second, network characteristics impact the data transfer rates of tasks. 
* Finally, the specifics of Map-Reduce leads to imbalance in work – partitioning data over a low entropy key space often leads to a skew in the input sizes of tasks.

####2.2 The Outlier Problem
* Outliers in a Phase
    * Assume a `phase` consists of n tasks and has s slots.
    * `Slot` is a virtual token, akin to a quota, for sharing cluster resources among multiple jobs.
    * One `task` can run per slot at a time(The code is the same for tasks in a phase, but differs significantly across phases (e.g., map and reduce).).
    * The `goal` is to minimize the phase completion time, i.e., the time when the last task finishes.
    * we model `\(t_i\)`, the completion time of task i,`\(t_i = f (datasize, code, machine, network)\)`
* Extending from a phase to a job
    * At `barriers` in the workflow, where none of the tasks in successive phase(s) can begin until all of the tasks in the preceding phase(s) finish, even one outlier can bring the job to a standstill. 
    * Dependency across phases also leads to outliers when task output is `lost`(disk errors, software errors and timeouts) needs to be `recomputed`. 
* Illustration of Outliers
    * ![mantri1](/assets/2013-09-26-mr2/mantri1.png)
    * The spike in phase#2(Aggregate/reduce) here is due to the outliers in phase#1(Partition/map) holding on to the job’s slots.
    * The worst cases of waiting immediately follow recomputations of lost intermediate data marked by R.

####2.3 Quantifying the Outlier Problem
* Causes of Outliers
    * DataSkew
    * Crossrack Traffic
    * Bad and Busy Machines

####2.4 Mantri Design

![mantri2](/assets/2013-09-26-mr2/mantri2.png)



`\(\)`


----

###J注
1. [Hadoop Speculative Task][2].对于这些运行速度较慢的task，Hadoop不会尝试诊断或者修复，而是分析出哪些task运行较慢，针对这些运行较慢的task，启动一些backup的task，我们称之为speculative task，speculative task会与原有task一起运行，哪个task首先执行结束，则使用这个task的结果作为整个job的输出。


[1]: http://www.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-183.pdf
[2]: http://blog.csdn.net/yfkiss/article/details/10589137
[3]: http://research.microsoft.com/en-us/UM/people/srikanth/data/mantri_tr.pdf







####23 [SCOPE: Easy and Efficient Parallel Processing of Massive Data Sets][8]

#####23.1 PLATFORM OVERVIEW

![2](/assets/2013-09-25-mr-and-processing-language/scope1.png)

An application is modeled as a dataflow graph: a directed acyclic graph (DAG) with vertices representing processes and edges representing data flows. The runtime component of the execution engine is called the `Job Manager`. The JM is the central and coordinating process for all processing vertices within an application. The primary function of the JM is to construct the runtime DAG from the compile time representation of a DAG and execute over it. The JM schedules a DAG vertex onto the system processing nodes when all the inputs are ready, monitors progress, and, on failure, re-executes part of the DAG.

`Dryad` implements a job manager and a graph building language for composing vertices of computation and edges of communication channels between the vertices.

#####23.2 SCOPE Scripting Language

The SCOPE scripting language resembles SQL but with C# expressions.

* Input and Output
* Select and Join
* Expressions and Functions
* User-Defined Operators
  * Process
  * Reduce
  * Combine
  * Importing Scripts

#####23.3 SCOPE Execution

* SCOPE Compilation
  * The result of the compilation is an internal parse tree. SCOPE has an option to translate the parsed tree directly to a physical execution plan using default plans for each command.
  * A physical execution plan is, in essence, a specification of Cosmos job.
* SCOPE Optimization
  * The SCOPE optimizer is a transformation-based optimizer based on the `Cascades` framework. 
* Runtime Optimization

####24 [DryadLINQ: A System for General-Purpose Distributed Data-Parallel Computing Using a High-Level Language][5]

#####24.1 DryadLINQ Execution Overview

![3](/assets/2013-09-25-mr-and-processing-language/DryadLINQ1.png)

* Step 1. A .NET user application runs. It creates a DryadLINQ expression object. Because of LINQ’s `deferred evaluation`, the actual execution of the expression has not occurred.
* Step 2. The application calls `ToDryadTable` triggering a data-parallel execution. The expression object is handed to DryadLINQ.
* Step 3. DryadLINQ compiles the LINQ expression into a distributed Dryad execution plan. It performs: 
  * (a) the decomposition of the expression into subexpressions, each to be run in a separate Dryad vertex; 
  * (b) the generation of code and static data for the remote Dryad vertices;
  * (c) the generation of serialization code for the required data types.
* Step 4. DryadLINQ invokes a custom, DryadLINQ-specific, Dryad job manager. The job manager may be executed behind a cluster firewall.
* Step 5. The job manager creates the job graph using the plan created in Step 3. It schedules and spawns the vertices as resources become available.
* Step 6. Each Dryad vertex executes a vertex-specific program (created in Step 3b).
* Step 7. When the Dryad job completes successfully it writes the data to the output table(s).
* Step 8. The job manager process terminates, and it returns control back to DryadLINQ. DryadLINQ creates the local DryadTable objects encapsulating the outputs of the execution. These objects may be used as inputs to subsequent expressions in the user program. Data objects within a DryadTable output are fetched to the local context only if explicitly dereferenced.
* Step 9. Control returns to the user application. The iterator interface over a DryadTable allows the user to read its contents as .NET objects.
* Step 10. The application may generate subsequent DryadLINQ expressions, to be executed by a repetition of Steps 2–9.

#####24.2 LINQ

The base type for a LINQ collection is `IEnumerable<T>`. From a programmer’s perspective, this is an abstract dataset of objects of type T that is accessed using an iterator interface. LINQ also defines the `IQueryable<T>` interface which is a subtype of `IEnumerable<T>` and represents an (unevaluated) expression constructed by combining LINQ datasets using LINQ operators. We need make only two observations about these types: (a) in general the programmer neither knows nor cares what concrete type implements any given dataset’s IEnumerable interface; and (b) DryadLINQ composes all LINQ expressions into IQueryable objects and defers evaluation until the result is needed, at which point the expression graph within the IQueryable is optimized and executed in its entirety on the cluster. Any IQueryable object can be used as an argument to multiple operators, allowing efficient re-use of common subexpressions.

#####24.3 DryadLINQ Constructs

The inputs and outputs of a DryadLINQ computation are represented by objects of type `DryadTable<T>`, which is a subtype of `IQueryable<T>`.

The inputs and outputs of a DryadLINQ computation are specified using the `GetTable<T>` and `ToDryadTable<T>` operators.

DryadLINQ offers two data re-partitioning operators: `HashPartition<T,K>` and `RangePartition<T,K>`. These operators are needed to enforce a partitioning on an output dataset and they may also be used to override the optimizer’s choice of execution plan.

The remaining new operators are `Apply` and `Fork`, which can be thought of as an “escape-hatch” that a programmer can use when a computation is needed that cannot be expressed using any of LINQ’s built-in operators. `Apply` takes a function f and passes to it an iterator over the entire input collection, allowing arbitrary streaming computations. The `Fork` operator is very similar to Apply except that it takes a single input and generates multiple output datasets. This is useful as a performance optimization to eliminate common subcomputations, e.g.

#####24.4 DryadLINQ Optimizations

* Static Optimizations
  * Pipelining
  * Removing redundancy
  * Eager Aggregation
  * I/O reduction
* Dynamic Optimizations
* Optimizations for OrderBy
* Execution Plan for MapReduce

####27 [Interpreting the Data: Parallel Analysis with Sawzall][9]

#####27.1 Sawzall Language Overview

    i: int;      # a simple integer declaration
    i: int = 0;  # a declaration with an initial value
    
    f: float;
    s: string = "1.234";
    f = float(s);
    
    string(1234, 16);
    string(utf8_bytes, "UTF-8");
    
    b: bytes = "Hello, world!\n";
    b: bytes = bytes("Hello, world!\n", "UTF-8");
    
    input: bytes = next_record_from_input();
    
    proto "some_record.proto" # define ’Record’
    r: Record = input;         # convert input to Record

#####27.2 Aggregators

* Collection:

        c: table collection of string;
* Sample:

        s: table sample(100) of string;
* Sum:

        s: table sum of { count: int, revenue: float };
* Maximum:

        m: table maximum(10) of string weight length: int;
* Quantile:

        q: table quantile(101) of response_in_ms: int;
* Top:

        t: table top(10) of language: string;
* Unique:

        u: table unique(10000) of string;

######27.3 Indexed Aggregators

    table top(1000) [country: string][hour: int] of request: string;
    
    t1: table sum[country: string] of int
    #equivalent to the values collected by
    t2: table collection of country: string
    
    t1["china"] = 123456
    t1["japan"] = 142367


[8]: http://research.microsoft.com/en-us/um/people/jrzhou/pub/Scope.pdf
[5]: http://research.microsoft.com/en-us/projects/dryadlinq/dryadlinq.pdf
[9]: http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/zh-CN//archive/sawzall-sciprog.pdf
