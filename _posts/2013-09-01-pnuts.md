---
layout: post
title: "PNUTS"
description: ""
category: 云计算
tags: [Pnuts, message]
---
{% include JB/setup %}
paper review:[PNUTS: Yahoo!’s Hosted Data Serving Platform](http://research.yahoo.com/files/pnuts.pdf)

<!--break-->
##1. INTRODUCTION
The foremost requirements of a web application are `scalability`, consistently good `response time` for geographically dispersed users, and `high availability`. At the same time, web applications can frequently tolerate `relaxed consistency guarantees`.

####1.1 PNUTS Overview
**Data Model and Features** PNUTS exposes a simple relational model to users, and supports single-table scans with predicates. Additional features include `scatter-gather` operations, a facility for `asynchronous notification` of clients and a facility for `bulk loading`.

**Fault Tolerance** PNUTS employs redundancy at multiple levels (data, metadata, serving components, etc.) and leverages our consistency model to support highly-available reads and writes even after a failure or partition.

**Pub-Sub Message System** Asynchronous operations are carried out over a topic-based pub/sub system called Yahoo! Messsage Broker (YMB), which together with PNUTS, is part of Yahoo!’s Sherpa data services platform. We chose pub/sub over other asynchronous protocols (such as gossip) because it can be optimized for geographically distant replicas and because replicas do not need to know the location of other replicas.

**Record-level Mastering** To meet response-time goals, PNUTS cannot use write-all replication protocols that are employed by systems deployed in localized clusters (such as GFS, Bigtable). However, not every read of the data necessarily needs to see the most current version. We have therefore chosen to make all high latency operations asynchronous, and to support record-level mastering. Synchronously writing to multiple copies around the world can take hundreds of milliseconds or more, while the typical latency budget for the database portion of a web request is only 50-100 milliseconds. Asynchrony allows us to satisfy this budget despite geographic distribution, while record-level mastering allows most requests, including writes, to be satisfied locally.

**Hosting** PNUTS is a hosted, centrally-managed database service shared by multiple applications.

##2. FUNCTIONALITY
####2.1 Data and Query Model
Data is organized into tables of records with attributes. Schemas are flexible: new attributes can be added at any time without halting query or update activity, and records are not required to have values for all attributes.

The query language of PNUTS supports selection and projection from a single table. Updates and deletes must specifiy the primary key. PNUTS allows applications to declare tables to be hashed or ordered, supporting both workloads efficently.

Our system is designed primarily for online serving workloads that consist mostly of queries that read and write single records or small groups of records. Thus, we expect most scans to be of just a few tens or hundreds of records, and optimize accordingly. Scans can specify predicates which are evaluated at the server. Similarly, we provide a “multiget” operation which supports retrieving multiple records (from one or more tables) in parallel by specifying a set of primary keys and an optional predicate, but again expect that the number of records retrieved will be a few thousand at most.

Our system, regrettably, also does not enforce constraints such as referential integrity, although this would be very desirable. The implementation challenges in a system with fine-grained asynchrony are significant, and require future work. Another missing feature is complex ad hoc queries (joins, group-by, etc.). While improving query functionality is a topic of future work, it must be accomplished in a way that does not jeapardize the response-time and availability currently guaranteed to the more “transactional” requests of web applications.

####2.2 Consistency Model: Hiding the Complexity of Replication
PNUTS provides a consistency model that is between the two extremes of general serializability and eventual consistency. Our model stems from our earlier observation that web applications typically manipulate one record at a time, while different records may have activity with different geographic locality. We provide `per-record timeline consistency`: all replicas of a given record apply all updates to the record in the same order. An example sequence of updates to a record is shown in this diagram:

![1](/assets/2013-09-01-pnuts/1.png)

In this diagram, the events on the timeline are inserts, updates and deletes for a particular primary key. The intervals between an insert and a delete, shown by a dark line in the diagram, represent times when the record is physically present in the database. A read of any replica will return a consistent version from this timeline, and replicas always move forward in the timeline. This model is implemented as follows. One of the replicas is designated as the `master`, independently for each record, and all updates to that record are forwarded to the master. The master replica for a record is adaptively changed to suit the workload – **the replica receiving the majority of write requests for a particular record becomes the master for that record**. The record carries a sequence number that is incremented on every write. As shown in the diagram, the sequence number consists of the generation of the record (each new insert is a new generation) and the version of the record (each update of an existing record creates a new version). Note that we (currently) keep only one version of a record at each replica.

Using this per-record timeline consistency model, we support a whole range of API calls with varying levels of consistency guarantees.

* **Read-any**: Returns a possibly stale version of the record. The returned record is always a valid one from the record’s history. Note that this call departs from strict serializability since with this call, even after doing a successful write, it is possible to see a stale version of the record. Since this call has lower latency than other read calls with stricter guarantees (described next), it provides a way for the application to explicitly indicate, on a per-read basis, that performance matters more than consistency. For example, in a social networking application, for displaying a user’s friend’s status, it is not absolutely essential to get the most up-to-date value, and hence read-any can be used.
* **Read-critical(required version)**: Returns a version of the record that is strictly newer than, or the same as the required version. A typical application of this call is when a user writes a record, and then wants to read a version of the record that definitely reflects his changes. **Our write call returns the version number of the record written**, and hence the desired read guarantee can be enforced by using a read-critical with required version set to the version returned by the write.
* **Read-latest**: Returns the latest copy of the record that reflects all writes that have succeeded. Note that `read-critical` and `read-latest` may have a higher latency than `read-any` if the local copy is too stale and the system needs to locate a newer version at a remote replica.
* **Write**: This call gives the same ACID guarantees as a transaction with a single write operation in it. This call is useful for blind writes, e.g., a user updating his status on his profile.
* **Test-and-set-write(required version)**: This call performs the requested write to the record if and only if the present version of the record is the same as required-version. This call can be used to implement transactions that first read a record, and then do a write to the record based on the read, e.g., incrementing the value of a counter. The test-and-set write ensures that two such concurrent increment transactions are properly serialized.

In the future, we plan to augment our consistency model with the following primitives:

* **Bundled updates**: Consistency guarantees for write operations that span multiple records (see Section 6).
* **Relaxed consistency**: Under normal operation, if the master copy of a record fails, our system has protocols to fail over to another replica. However, if there are major outages, e.g., the entire region that had the master copy for a record becomes unreachable, updates cannot continue at another replica without potentially violating record-timeline consistency. We will allow applications to indicate, per-table, whether they want updates to continue in the presence of major outages, potentially branching the record timeline. If so, we will provide automatic conflict resolution and notifications thereof. The application will also be able to choose from several conflict resolution policies: e.g., discarding one branch, or merging updates from branches, etc.

####2.3 Notification
Trigger-like `notifications` are important for applications such as ad serving, which must invalidate cached copies of ads when the advertising contract expires. Accordingly, we allow the user to `subscribe` to the stream of updates on a table. Notifications are easy to provide given our underlying pub/sub infrastructure (see Section 3.2.1), and thus have the same stringent reliability guarantees as our data replication mechanism.

####2.4 Bulk Load
`Bulk loading` tools are necessary for applications such as comparison shopping, which upload large blocks of new sale listings into the database every day. Bulk inserts can be done in parallel to multiple storage units for fast loading. In the hash table case, the hash function naturally load balances the inserts across storage units. However, in the ordered table case, bulk inserts of ordered records, records appended to the end of the table’s range, or records inserted into already populated key ranges require careful handling to avoid hot spots and ensure high performance.

##3. SYSTEM ARCHITECTURE

![2](/assets/2013-09-01-pnuts/2.png)

The system is divided into `regions`, where each region contains a full complement of system components and a complete copy of each table. A key feature of PNUTS is the use of a pub/sub mechanism for both reliability and replication.

####3.1 Data Storage and Retrieval
Data tables are horizontally partitioned into groups of records called `tablets`. Tablets are scattered across many `servers`; each server might have hundreds or thousands of tablets, but each tablet is stored on a single server within a region. A typical tablet in our implementation is a few hundred megabytes or a few gigabytes, and contains thousands or tens of thousands of `records`.

Three components in Figure 1 are primarily responsible for managing and providing access to data tablets: the `storage unit`, the `router`, and the `tablet controller`. Storage units store tablets, respond to get() and scan() requests by retrieving and returning matching records, and respond to set() requests by processing the update. Updates are committed by first writing them to the message broker, as described in the next section. The storage unit can use any physical storage layer that is appropriate. For hash tables, our implementation uses a UNIX filesystem-based hash table implemented originally for Yahoo!’s user database. For ordered tables, we use MySQL with InnoDB because it stores records ordered by primary key. Schema flexibility is provided for both storage engines by storing records as parsed JSON objects.

In order to determine which storage unit is responsible for a given record to be read or written by the client, we must first determine which tablet contains the record, and then determine which storage unit has that tablet. Both of these functions are carried out by the `router`. For ordered tables, the primary-key space of a table is divided into intervals, and each interval corresponds to one tablet. The router stores an interval mapping, which defines the boundaries of each tablet, and also maps each tablet to a storage unit. An example is shown in Figure 2a. This mapping is similar to a very large root node of a B+ tree. In order to find the tablet for a given primary key, we conduct a binary search over the interval mapping to find the tablet enclosing the key. Once we find the tablet, we have also found the appropriate storage server.

For hash-organized tables, we use an n-bit hash function H() that produces hash values 0 ≤ H() < 2^n. The hash space [0...2^n) is divided into intervals, and each interval corresponds to a single tablet. An example is shown in Figure 2b. To map a key to a tablet, we hash the key, and then search the set of intervals, again using binary search, to lo- cate the enclosing interval and thus the tablet and storage unit. We chose this mechanism, instead of a more traditional linear or extensible hashing mechanism, because of its symmetry with the ordered table mechanism. Thus, we can use the same code to maintain and search interval mappings for both hash and ordered tables.

![3](/assets/2013-09-01-pnuts/3.png)

Routers contain only a cached copy of the interval mapping. The mapping is owned by the `tablet controller`, and routers periodically poll the tablet controller to get any changes to the mapping. The tablet controller determines when it is time to move a tablet between storage units for load balancing or recovery and when a large tablet must be split. In each case, the controller will update the authoritative copy of the mapping. For a short time after a tablet moves or splits, the routers’ mappings will be out of date, and requests will be misdirected. A misdirected request results in a storage unit error response, causing the router to retrieve a new copy of the mapping from the controller.

####3.2 Replication and Consistency
#####3.2.1 Yahoo! Message Broker
We are able to use YMB for replication and logging for two reasons. First, YMB takes multiple steps to ensure messages are not lost before they are applied to the database. Second, YMB is designed for wide-area replication: YMB clusters reside in different, geographically separated datacenters, and messages published to one YMB cluster will be relayed to other YMB clusters for delivery to local subscribers.

YMB provides partial ordering of published messages. Messages published to a particular YMB cluster will be delivered to all subscribers in the order they were published. However, messages published to different YMB clusters may be delivered in any order. Thus, in order to provide timeline consistency, we have developed a per-record mastership mechanism, and the updates published by a record’s master to a single YMB cluster are delivered in the published order to other replicas (see the next section). While stronger ordering guarantees would simplify this protocol, global ordering is too expensive to provide when different brokers are located in geographically separated datacenters.

#####3.2.2 Consistency via YMB and mastership
Per-record timeline consistency is provided by designating one copy of a record as the master, and directing all updates to the master copy. In this record-level mastering mechanism, mastership is assigned on a record-by-record basis, and different records in the same table can be mastered in different clusters. We chose this mechanism because we have observed significant write locality on a per-record basis in our web workloads. However, since different records have update affinity for different datacenters, the granularity of mastership must be per-record, not per tablet or per table; other- wise, many writes would pay expensive cross-region latency to reach the master copy.

All updates are propagated to non-master replicas by publishing them to the message broker, and once the update is published we treat it as committed. A master publishes its updates to a single broker, and thus updates are delivered to replicas in commit order.

Updates for a record can originate in a non-master region, but must be forwarded to the master replica before being committed. Each record maintains, in a hidden metadata field, the identity of the current master. If a storage unit receives a set() request, it first reads the record to determine if it is the master, and if not, what replica to forward the request to. The mastership of a record can migrate between replicas. If a user moves from Wisconsin to California, the system will notice that the write load for the record has shifted to a different datacenter (using another hidden metadata field in the record that maintains the origin of the last N updates) and will publish a message to YMB indicating the identity of the new master. In the current implementation, N = 3, and since our region names are 2 bytes this tracking adds only a few bytes of overhead to each record.

In order to enforce primary key constraints, we must send inserts of records with the same primary key to the same storage unit; this storage unit will arbitrate and decide which insert came first and reject the others. Thus, we have to designate one copy of each tablet as the tablet master, and send all inserts into a given tablet to the `tablet master`. The tablet master can be different than the record level master assigned to each record in the tablet.

#####3.2.3 Recovery
Recovering from a failure involves copying lost tablets from another replica. Copying a tablet is a three step process. First, the tablet controller requests a copy from a particular remote replica (the “source tablet”). Second, a “checkpoint message” is published to YMB, to ensure that any in-flight updates at the time the copy is initiated are applied to the source tablet. Third, the source tablet is copied to the destination region. To support this recovery protocol, tablet boundaries are kept synchronized across replicas, and tablet splits are conducted by having all regions split a tablet at the same point (coordinated by a two-phase commit between regions). Most of the time in this protocol is spent transferring the tablet from one region to another. Note that in practice, because of the bandwidth cost and latency needed to retrieve tablets from remote regions, it may be desirable to create “backup regions” which maintain a back-up replica near serving replicas. Then, recovering a table would involve transferring it from a “region” in the same or a nearby datacenter, rather than from a geographically distant datacenter.

####3.3 Other Database System Functionality
#####3.3.1 Query Processing
The component responsible for multi-record requests is called the `scatter-gather engine`, and is a component of the router. The scatter-gather engine receives a multi-record request, splits it into multiple individual requests for single records or single tablet scans, and initiates those requests in parallel. As the requests return success or failure, the scatter-gather engine assembles the results and then passes them to the client. In our implementation, the engine can begin streaming some results back to the client as soon as they appear. We chose a server-side approach instead of having the client initiate multiple parallel requests for several reasons. First, at the TCP/IP layer, it is preferable to have one connection per client to the PNUTS service; since there are many clients (and many concurrent processes per client machine) opening one connection to PNUTS for each record being requested in parallel overloads the network stack. Second, placing this functionality on the server side allows us to optimize, for example by grouping multiple requests to the same storage server in the same web service call.

####3.4 Hosted Database Service
PNUTS is a hosted, centrally-managed database service shared by multiple applications.





