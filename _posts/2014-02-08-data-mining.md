---
layout: post
title: "读书笔记-数据挖掘导论"
description: "读书笔记-数据挖掘导论"
category: 大数据
tags: [数据挖掘]
---
{% include JB/setup %}

读[《数据挖掘导论》](http://book.douban.com/subject/5377669/)。

![zen](http://img3.douban.com/lpic/s4548758.jpg)

##第2章 数据
###2.1 数据类型
`数据集`可以看作`数据对象`的集合。数据对象有时也叫记录、点、向量、模式、事件、案例、样本、观测和实体。数据对象用一组刻画对象基本特性(如物体质量或事件发生时间)的`属性`描述。属性有时也叫做变量、特性、字段、特征或维。

####2.1.1 属性与度量
1. 什么是属性
    * `属性`(attribute)是对象的性质或特性，它因对象而异，或随时间而变化。
    * `测量标度`(measurement scale)是将数值或符号值与对象的属性相关联的规则(函数)。
2. 属性类型
3. 属性的不同类型
    * 数值的如下性质(操作)常常用来描述属性
        * 向异性:=,≠
        * 序:<,≤,>,≥
        * 加法:+,-
        * 乘法:*,/
    * 给定这些性质，我们可以定义四种属性类型：标称(nominal)、序数(ordinal)、区间(interval)和比率(ratio)。![1](/assets/2014-02-08-data-mining/1.png)
    * 标称和序数属性统称`分类的`(categorical)或`定性的`(qualitative)属性。区间和比率属性，统称为`定量的`(quantitative)或`数值的`(numeric)属性。
    * 属性的类型也可以用不改变属性意义的变换来描述。实际上，心理学家S.Smith Stevens最先用`允许的变换`(permissible transformation)定义了表2-2所示的属性类型。![2](/assets/2014-02-08-data-mining/2.png)
4. 用值的个数描述属性
    * `离散的`(discrete)(`二元属性`(binary attribute))
    * `连续的`(continuous)
5. 非对称的属性(asymmetric attribute)(出现非零值才是重要的)

####2.1.2 数据集的类型
1. 数据集的一般特性
    * `维度`(dimensionality)：数据集的维度是数据集中的对象具有的属性的数目。分析高维度数据时有时会陷入所谓`维灾难`(curse of dimensionality)。正因为如此，数据预处理的一个重要动机就是减少维度，称为`维归约`(dimensionality reduction)。
    * `稀疏性`(sparsity)：稀疏性是一个优点，因为只有非零值才需要存储和处理。
    * `分辨率`(resolution)：如果分辨率太高，模式可能看不出，或者掩埋在噪声中；如果分辨率太低，模式可能不出现。
2. 记录数据
    * 事务数据或购物篮数据
    * 数据矩阵
    * 稀疏数据矩阵(`文档-词矩阵`(document-term matrix))![2-1](/assets/2014-02-08-data-mining/2-1.png)
3. 基于图形的数据
4. 有序数据
    * 时序数据(sequential data)
    * 序列数据(sequence data)
    * 时间序列数据(`时间自相关`(temporal autocorrelation)，即如果两个测量的时间很接近，则这些测量的值通常非常相似)
    * 空间数据(`空间自相关`(spatial autocorrelation)，即物理上靠近的对象趋向于在其他方面也相似)
5. 处理非记录数据

###2.2 数据质量
####2.2.1 测量和数据收集问题
1. 测量误差和数据收集错误
    * `测量误差`(measurement error)是指测量过程中导致的问题。对于连续属性，测量值与实际值的差称为`误差`(error)。
    * `数据收集错误`(data collection error)是指诸如遗漏数据对象或属性值，或不当地包含了其他数据对象等错误。
2. 噪声和伪像
    * `噪声`(noise)是测量误差的随机部分。许多数据挖掘工作关注设计`鲁棒算法`(robust algorithm)，即在噪声干扰下也能产生可以接收的结果。
    * 数据的确定性失真称作`伪像`(artifact)。
3. 精度、偏倚和准确率
    * `精度`(precision)(同一个量的)重复测量值之间的接近程度，值集合的标准差度量。
    * `偏倚`(bias)测量值与被测量值之间的系统的变差，值集合的均值与测出的已知值之间的差度量。
    * `准确率`(accuracy)被测量的测量值与实际值之间的接近度。准确率的一个重要方面是`有效数字`(significant digit)的使用。其目标是仅使用数据精度所能确定的数字位数表示测量或计算结果。
4. `离群点`(outlier)是在某种意义上具有不同于数据集中其他大部分数据对象的特征的数据对象，或是相对于该属性的典型值来说不寻常的属性值。
5. 遗漏值(删除、估计、忽略)
6. 不一致的值(检测，更正)
7. 重复数据(`去重复`(deduplication))

####2.2.2 关于应用的问题

* 时效性
* 相关性(`抽样偏倚`(sampling bias)样本包含的不同类型的对象与它们在总体中的出现情况不成比例)
* 关于数据的知识(描述数据的文档)

###2.3 数据预处理
####2.3.1 聚集

* `聚集`(aggregation)将两个或多个对象合并成单个对象。
* 聚集的动机有多种。首先，数据归约导致的较小数据集需要较少的内存和处理时间。其次，通过高层而不是低层数据视图，聚集起到了范围或标度转换的作用。最后，对象或属性群的行为通常比单个对象或属性的行为更加稳定。

####2.3.2 抽样
1. 抽样方法
    * `简单随机抽样`(simple random sampling)(`无放回抽样`、`有放回抽样`)。
    * `分层抽样`(stratified sampling)：预先指定组的抽样。
2. `渐进抽样`(progressive sampling)：从一个小样本开始，然后增加样本容量直至足够容量的样本。

####2.3.3 维归约
1. 维灾难。随着维度增加，数据在它所占据的空间中越来越稀疏。对于分类，这可能意味着没有足够的数据对象来创建模型，将所有可能的对象可靠地指派到一个类。对于聚类，点之间的密度和距离的定义失去了意义。
2. 维归约的线性代数技术。将数据由高维度投影到低维度空间，特别对于连续数据。`主成分分析`(Principal Components Analysis,PCA)是一种用于连续属性的线性代数技术，它找出新的属性(主成分)，这些属性是原属性的线性组合，是相互`正交的`(orthogonal)，并且捕获了数据的最大变差。`奇异值分解`(Singular Value Decomposition,SVD)是一种线性代数技术，也用于维归约。

####2.3.4 特征子集选择
1. 特征子集选择体系结构![3](/assets/2014-02-08-data-mining/3.png)
2. 特征加权

####2.3.5 特征创建
1. 特征提取:由原始数据创建新的特征集称作`特征提取`(feature extraction)。
2. 映射数据到新的空间(例如`傅里叶变换`(Fourier transform))
3. 特征构造(例如密度=质量/体积)

####2.3.6 离散化和二元化
1. `二元化`(binarization)![4](/assets/2014-02-08-data-mining/4.png)![5](/assets/2014-02-08-data-mining/5.png)
2. 连续属性`离散化`(discretization)
    * 两个子任务：决定需要多少个分类值(指定n-1个`分割点`(split point))；确定如何将连续属性值映射到这些分类值。
    * `非监督离散化`(unsupervised)：不使用类信息(等宽、等频率、等深、K均值)。![6](/assets/2014-02-08-data-mining/6.png)
    * `监督离散化`(supervised)：基于`熵`(entropy)的方法。
        * 设k是不同的类标号数，`\(m_i\)`是某划分的第i个区间中值的个数，而`\(m_{ij}\)`是区间i中类j的值的个数。第i个区间的熵`\(e_i=-\sum_{j=1}^k{p_{ij}log_2 p_{ij}}\)`，其中`\(p_{ij}=\frac{m_{ij}}{m_i}\)`是第i个区间中类j的概率。该划分的总熵e是每个区间的熵的加权平均，即`\(e=\sum_{i=1}^n{w_ie_i}\)`，其中，m是值的个数。`\(w_i=\frac{m_i}{m}\)`是第i个区间的值的比例，而n是区间个数。
        * 直观上，区间的熵是区间纯度的度量。如果一个区间只包含一个类的值(该区间非常纯)，则其熵为0并且不影响总熵。如果一个区间中的值类出现的频率相等(该区间尽可能不纯)，则其熵值最大。
        * 一个划分的方法：开始，将初始值切分成两部分，让两个结果区间产生最小熵。该技术只需要把每个值看作可能的分割点即可，因为假定区间包含有序值的集合。然后，取一个区间，通常选取具有最大熵的区间，重复此分割过程，直到区间的个数达到用户指定的个数。

####2.3.7 变量变换
* `变量变换`(variable transformation)是指用于变量的所有值的变换。
    * 简单函数
    * `标准化`(standardization)或`规范化`(normalization)

###2.4 相似性和相异性的度量
####2.4.1 概念
1. `邻近度`(proximity)表示相似性或相异性。
2. 两个对象之间的`相似度`(similarity)的非正式定义是这两个对象相似程度的数值度量。两个对象之间的`相异度`(dissimilarity)是这两个对象差异程度的数值度量。对象约类似，它们的相异度就越低。`距离`(distance)用作相异度的同义词。
3. 通常使用变换把相似度转换成相异度或相反，或者把邻近度变换到一个特定区间，如[0,1]。

####2.4.2 简单属性的相似度和相异度
![7](/assets/2014-02-08-data-mining/7.png)

####2.4.3 数据对象之间的相异度
1. 多维空间中两个点x和y之间的`欧几里得距离`(Euclidean distance)：`\(d(x,y)=\sqrt{\sum_{k=1}^n{(x_k-y_k)^2}}\)`,其中n表示维数，而`\(x_k\)`和`\(y_k\)`分别是x和y的第k个属性值(分量)。
2. 欧几里得距离可以由`闵可夫斯基距离`(Minkowski distance)推广`\(d(x,y)=(\sum_{k=1}^n{|x_k-y_k|^r})^{1/r}\)`，其中r是参数。
    * r=1，城市街区(也称曼哈顿、出租车、L1范数)距离。一个常见的例子是`海明距离`(Hamming distance)，它是两个具有二元属性的对象(即两个二元向量)之间不同的二进制位个数。
    * r=2，欧几里得距离(L2范数)。
    * r=∞，上确界(Lmax或L∞范数)距离。这是对象属性之间的最大距离。更正式地，L∞距离定义：`\(d(x,y)=\lim_{r \to \infty}(\sum_{k=1}^n{|x_k-y_k|^r})^{1/r}\)`
3. 距离的性质
    * 非负性。对所有x和y，d(x,y)≥0,当且仅当x=y时d(x,y)=0
    * 对称性。对所有x和y，d(x,y)=d(y,x)
    * 三角不等式。对所有x，y和z，d(x,z)≤d(x,y) + d(y,z)
    * 满足以上三个性质的测度称为`度量`(metric)。

####2.4.4 数据对象之间的相似度
对于相似度，三角不等式通常不成立。但对称性和非负性通常成立。如果s(x,y)是数据点x和y之间的相似度：

* 仅当x=y时s(x,y)=1,0≤s≤1
* 对于所有x和y，s(x,y)=s(y,x)    

####2.4.5 邻近性度量
1. 二元数据的相似性度量
    * 两个仅包含二元属性的对象之间的相似性度量也称为`相似系数`(similarity coefficient)，并且通常在0和1之间取值，值1表明两个对象完全相似，值0表明对象一点也不相似。
    * 设x和y是两个对象，都由n个二元属性组成。这样的两个对象(即两个二元向量)的比较可生成如下四个量(频率)：
        * `\(f_{00}\)`=x取0并且y取0的属性个数
        * `\(f_{01}\)`=x取0并且y取1的属性个数
        * `\(f_{10}\)`=x取1并且y取0的属性个数
        * `\(f_{11}\)`=x取1并且y取1的属性个数
    * `简单匹配系数`(Simple Matching Coefficient,SMC):`\(SMC=\frac{值匹配的属性个数}{属性个数}=\frac{f_{11}+f_{00}}{f_{01}+f_{10}+f_{11}+f_{00}}\)`
    * `Jaccard系数`(Jaccard Coefficient):`\(J=\frac{匹配的个数}{不涉及0-0匹配的属性个数}=\frac{f_{11}}{f_{01}+f_{10}+f_{11}}\)`
2. 余弦相似度(cosine similarity)
    * 是文档相似度最常用的度量之一。如果x和y是两个文档向量，则`\(cos(x,y)=\frac{x·y}{\lVert x \rVert\lVert y \rVert}\)`
    * 另一种形式是`\(cos(x,y)=\frac{x·y}{\lVert x \rVert\lVert y \rVert}=x'·y'\)`，其中`\(x'=\frac{x}{\lVert x \rVert}\)`，而`\(y'=\frac{y}{\lVert y \rVert}\)`。x和y被它们的长度除，将它们规范化成具有长度1.这意味着在计算相似度时，余弦相似度不考虑两个对象的量值(当量值是重要的时，欧几里得距离可能是一种更好的选择)。
3. 广义Jaccard系数
    * 可以用于文档数据，并在二元属性情况下归约为Jaccard系数，又称`Tanimoto系数`。`\(EJ(x,y)=\frac{x·y}{\lVert x \rVert^2\lVert y \rVert^2-x·y}\)`
4. 相关性
    * 两个具有二元变量或连续变量的数据对象之间的相关性是对象属性之间线性联系的度量。两个数据对象x和y之间的`皮尔森相关系数`(Pearson's correlation):`\(corr(x,y)=\frac{s_{xy}}{s_xs_y}\)`
        * 协方差`\(s_{xy}=\frac{1}{n-1}\sum_{k=1}^n{(x_k-\bar{x})(y_k-\bar{y})}\)`
        * 标准差`\(s_x=\sqrt{\frac{1}{n-1}\sum_{k=1}^n{(x_k-\bar{x})^2}}\)`
        * 标准差`\(s_y=\sqrt{\frac{1}{n-1}\sum_{k=1}^n{(y_k-\bar{y})^2}}\)`
    * 相关度总是在-1到1之间取值。相关度为1(-1)意味着x和y具有完全正(负)相关性。如果相关度为0，则两个数据对象的属性之间不存在线性关系。然而，仍然可能存在非线性关系。
5. Bregman散度
    * 一族具有共同性质的邻近函数。
    * 是损失或失真函数。损失函数的目的是度量用x近似y导致的失真或损失。x和y越类似，失真或损失就越小，因而Bregman散度可以用作相异性函数。
    * 定义：给定一个严格凸函数`\(\phi\)`，由该函数生成的Bregman散度(损失函数)：`\(D(x,y)=\phi(x)-\phi(y)-<\nabla\phi(y),(x-y)>\)`，其中`\(\nabla\phi(y)\)`是在y上计算的`\(\phi\)`的梯度，x-y是x与y的向量差，而`\(<\nabla\phi(y),(x-y)>\)`是`\(\nabla\phi(y)\)`和(x-y)的内积。
    * D(x,y)可以写成`\(D(x,y)=\phi(y)-L(x)\)`，其中`\(L(x)=\phi(x)+\phi(y)-<\nabla\phi(y),(x-y)>\)`代表在y上正切于函数`\(\phi\)`的平面方程。

####2.4.6 邻近度计算问题
1. 距离度量的标准化和相关性。当属性相关、具有不同的值域(不同的方差)、并且数据分布近似于高斯分布时，欧几里得距离的推广，`Mahalanobis距离`是有用的。两个对象(向量)x和y之间的Mahalanobis距离:`\(mahalanobis(x,y)=(x-y)\sum^{-1}(x-y)^T\)`，其中`\(\sum^{-1}\)`是数据协方差矩阵的逆。
2. 组合异种属性的相似度。
    * 对于第k个属性，计算相似度`\(s_k(x,y)\)`，在区间[0,1]中。
    * 对于第k个属性，定义一个指示变量`\(\delta_k\)`：
        * `\(\delta_k=0\)`，如果第k个属性是非对称属性，并且两个对象在该属性上的值都是0，或者如果一个对象的第k个属性具有遗漏值
        * `\(\delta_k=1\)`，否则
    * 使用公式计算两个对象之间的总相似度：`\(similarity(x,y)=\frac{\sum_{k=1}^n\delta_ks_k(x,y)}{\sum_{k=1}^n\delta_k}\)`
3. 使用权值
    * 如果权`\(w_k\)`的和为1，`\(similarity(x,y)=\frac{\sum_{k=1}^nw_k\delta_ks_k(x,y)}{\sum_{k=1}^n\delta_k}\)`
    * 闵可夫斯基距离修改为`\(d(x,y)=(\sum_{k=1}^n{w_k|x_k-y_k|^r})^{1/r}\)`

##第3章 探索数据
###3.2 汇总数据
1. 给定一个在{`\(v_1,v_2,...,v_k\)`}上取值的分类属性x和m个对象的集合。值`\(v_i\)`的`频率`:`\(frequency(v_i)=\frac{具有属性值v_i的对象数}{m}\)`，分类属性的`众数`(mode)是具有最高频率的值。![8](/assets/2014-02-08-data-mining/8.png)
2. 对于有序数据，考虑值集的`百分位数`(percentile)，给定一个有序的或连续的属性x和0-100之间的数p，第p个百分位数`\(x_p\)`是一个x值，使得x的p%的观测值小于`\(x_p\)`。![9](/assets/2014-02-08-data-mining/9.png)
3. 对于连续数据，最广泛的汇总统计是`均值`(mean)和`中位数`(median)。有时使用`截断均值`(trimmed mean)，指定0和100之间的百分位数p，丢弃高端和低端(p/2)%的数据，再计算均值。![10](/assets/2014-02-08-data-mining/10.png)
4. 连续数据的散布度量：`极差`(range)和`方差`(variance)、`标准差`(standard variance)。
    * `绝对平均偏差`(absolute average deviation,AAD),`\(AAD(x)=\frac{\sum_{i=1}^m{\lVert x_i-\bar{x} \rVert}}{m}\)`
    * `中位数绝对偏差`(median absolute deviation,MAD),`\(MAD(x)=median(\{\lVert x_1-\bar{x} \rVert,...,\lVert x_m-\bar{x} \rVert\})\)`
    * `四分位数极差`(interquartile range,IQR),`\(IQR(x)=x_{75\%}-x_{25\%}\)`
5. 多元汇总统计
    * 包含多个属性的数据(多元数据)的位置度量可以通过分别计算每个属性的均值或中位数得到：`\(\bar{x}=(\bar{x_1},...,\bar{x_n})\)`
    * 对于具有连续变量的数据，数据的散布可用`协方差矩阵`(covariance matrix)S表示，如果`\(x_i\)`和`\(x_j\)`分别是第i个和第j个属性，则
        * `\(s_{ij}=covariance(x_i,x_j)\)`,矩阵S的第ij个元素`\(s_{ij}\)`是数据的第i个和第j个属性的协方差。
        * `\(covariance(x_i,x_j)=\frac{\sum_{k=1}^m{(x_{ki}-\bar{x_i})(x_{kj}-\bar{x_j})}}{m-1}\)`,其中`\(x_{ki}\)`和`\(x_{kj}\)`分别是第k个对象的第i个和第j个属性的值。注意，`\(covariance(x_i,x_i)=variance(x_i)\)`，即协方差矩阵的对角线上是属性的方差。
    * `相关矩阵`(correlation matrix)R的第ij个元素是数据的第i个和第j个属性之间的相关性。如果`\(x_i\)`和`\(x_j\)`分别是第i个和第j个属性，则
        * `\(r_{ij}=correlation(x_i,x_j)=\frac{covariance(x_i,x_j}{s_is_j}\)`,其中`\(s_i\)`和`\(s_j\)`分别是`\(x_i\)`和`\(x_j\)`的方差。R的对角线上的元素是`\(correlation(x_i,x_j)=1\)`。

##第4章 分类：基本概念、决策树与模型评估
###4.1 预备知识
1. `分类`(classification)任务就是通过学习得到一个`目标函数`(target function)f，把每个属性集x映射到一个预先定义的类标号y。
2. 目标函数也称`分类模型`(classification model)。用于以下目的
    * 描述性建模(作为解释性工具，区分不同类中的对象)
    * 预测性建模(预测未知记录的类标号)

###4.2 解决分类问题的一般方法
1. 需要一个`训练集`(training set)，由类标号已知的记录组成。使用训练集建立分类模型后，该模型随后将运用于`检验集`(test set)，检验集由类标号未知的记录组成。![11](/assets/2014-02-08-data-mining/11.png)
2. 分类模型的`性能度量`(performance metric),通过`混淆矩阵`(confusion matrix)![12](/assets/2014-02-08-data-mining/12.png)
    * `\(准确率=\frac{正确预测数}{预测总数}=\frac{f_{11}+f_{00}}{f_{11}+f_{10}+f_{01}+f_{00}}\)`
    * `\(错误率=\frac{错误预测数}{预测总数}=\frac{f_{10}+f_{01}}{f_{11}+f_{10}+f_{01}+f_{00}}\)`

###4.3 决策树归纳
1. 在决策树中，每个`叶结点`(leaf node)都赋予一个类标号。`非终结点`(non-terminal node)包含属性测试条件，用以分开具有不同特性的记录。![13](/assets/2014-02-08-data-mining/13.png)
2. 如何建立决策树
    * `Hunt算法`，设`\(D_t\)`是与结点t相关联的训练记录集，而y=`\(y_1,y_2,...,y_c\)`是类标号，Hunt算法的递归定义如下。
        * 如果`\(D_t\)`中所有记录都属于同一个类`\(y_t\)`，则t是叶结点，用`\(y_t\)`标记。
        * 如果`\(D_t\)`中包含属于多个类的记录，则选择一个`属性测试条件`(attribute test condition)，将记录划分成较小的子集。对于测试条件的每个输出，创建一个子女结点，并根据测试结果将`\(D_t\)`中的记录分布到子女结点中，然后，对于每个子女结点，递归地调用算法。![14](/assets/2014-02-08-data-mining/14.png)![15](/assets/2014-02-08-data-mining/15.png)
    * 决策树归纳的设计问题
        * 如何分裂训练记录？算法必须提供为不同类型的属性指定测试条件的方法，并且提供评估每种测试条件的客观度量。
        * 如何停止分裂过程？需要有结束条件。
3. 表示属性测试条件的方法
    * 二元属性![16](/assets/2014-02-08-data-mining/16.png)
    * 标称属性(多路划分、二元划分)![17](/assets/2014-02-08-data-mining/17.png)
    * 序数属性(不违背序数属性值的有序性)![18](/assets/2014-02-08-data-mining/18.png)
    * 连续属性(可参考2.3.6离散化策略)![19](/assets/2014-02-08-data-mining/19.png)
4. 选择最佳划分的度量
    * 通常是根据划分后子女结点的不纯度性的程度。不纯的程度越低，类分布就约倾斜。
    * 不纯性度量：
        * `\(Entropy(t)=-\sum_{i=0}^{c-1}{p(i|t)log_2p(i|t)}\)`
        * `\(Gini(t)=1-\sum_{i=0}^{c-1}[p(i|t)]^2\)`
        * `\(Classification error(t)=1-max_i[p(i|t)]\)`
        * 其中c是类的个数。
        * ![20](/assets/2014-02-08-data-mining/20.png)
        * ![21](/assets/2014-02-08-data-mining/21.png)
    * 为了确定测试条件的效果，需要比较父结点(划分前)的不纯程度和子女结点(划分后)的不纯程度，差越大，效果越好。
        * 增益`\(\triangle=I(parent)-\sum_{j=1}^k\frac{N(v_j)}{N}I(v_j)\)`
        * 其中，I(.)是给定结点的不纯性度量，N是父结点上的记录总数，k是属性值的个数，`\(N(v_j)\)`是与子女结点`\(v_j\)`相关联的记录个数。
        * 决策树归纳算法通常选择最大化增益`\(\triangle\)`的测试条件(即最小化子女结点的不纯度量的加权平均值)。
        * 当选择熵(entropy)作为公式的不纯性度量时，熵的差就是所谓`信息增益`(information gain)`\(\triangle_{info}\)`
    * 当每个划分相关联的记录太少时，无法做出可靠预测。两种解决方法：
        * 限制测试条件只能是二元属性(CART决策树)
        * `增益率`(gain ratio)来评估划分。`\(Gain ratio=\frac{\triangle_{info}}{Split Info}\)`
            * `\(Split Info=-\sum_{i=1}^kP(v_i)log_2P(v_i)\)`,k是划分总数。
5. 决策树算法
    * 输入是训练记录集E和属性集F。
    * createNode()建立新结点。该结点要么是一个测试条件node.test_cond，要么是一个类标号node.label
    * find_best_split()确定应当选择哪个属性作为划分训练记录的条件。(方法见4.3.4)
    * Classify()为叶结点确定类标号。对于每个叶结点t，令p(i|t)表示该结点上属于类i的训练记录所占的比例，大多数情况，将叶结点指派到具有多数记录的类：`\(label.leaf=argmax_ip(i|t)\)`
    * stopping_cond()通过检查是否所有记录都属于一个类，或者都具有相同的属性值，决定是否终止决策树的增长。
    * ![22](/assets/2014-02-08-data-mining/22.png)
6. 决策树特定
    * 决策树归纳是一种构建分类模型的非参数方法。
    * 找到最佳决策树是NP完全问题。4.3.5是贪心算法，自顶向下的递归划分策略建立决策树。
    * 已开发的构建决策树技术不需要昂贵的计算代价。决策树一旦建立，未知样本分类很快。最坏情况下时间复杂度O(w)，w是树的最大深度。
    * 相对容易解释。
    * 是学习离散函数的典型代表，但不能很好的推广到某些布尔问题。
    * 对于噪声干扰具有相当好的鲁棒性，采用避免`过分拟合`的方法之后有其如此。
    * 冗余属性不会对决策树的准确率造成不利影响。如果一个属性在数据中与它的另一个属性是强相关的，那么它是`冗余`的。
    * 由于大多数的决策树算法都采用自顶向下的递归划分方法，因此沿着树向下，记录会越来越少。在叶结点记录可能太少。对于叶结点代表的类，不能做出具有统计意义的判决，是所谓的`数据碎片`(data fragmentation)问题。解法是设定阈值停止分裂。
    * 子树可能在决策树中重复出现。
    * 两个不同类的相邻之间的边界称作`决策边界`(decision boundary)。由于测试条件只涉及单个属性，因此决策边界是直线(图4-20)。图4-21的边界很难用单个属性的测试条件分类。![23](/assets/2014-02-08-data-mining/23.png)
        * `斜决策树`(oblique decision tree)，允许测试条件涉及多个属性。图4-21的决策条件是x+y<1
        * `归纳构造`(constructive induction)，创建复合属性，代表已有属性的算术或逻辑组合(2.3.5)。

###4.4 模型的过分拟合
分类模型的误差分为两种：`训练误差`(training error)和`泛化误差`(generalization error)。训练误差也称`再代入误差`(resubstitution error)或`表现误差`(apparent error)，是训练记录上误分类样本比例，而泛化误差是模型在未知记录上的期望误差。当决策树很小时，训练和检验误差都很大，这种情况称作`模型拟合不足`(model underfitting)。一旦树的规模变得太大，即时训练误差还在继续降低，但检验误差开始增大，这种现象称作`模型过分拟合`(model pverfitting)。![24](/assets/2014-02-08-data-mining/24.png)

1. 噪声导致的过分拟合
2. 缺乏代表性样本导致的过分拟合
3. 过分拟合与多重比较过程
    * 所谓的`多重比较过程`(multiple comparison procedure)，举例，考虑预测股市。如果股票分析师随机预测，则预测正确的概率为0.5，10次中预测至少正确8次的概率是P=0.0547(J:3次n重伯努利试验，n分别为8，9，10)。
    * 假定现在从50个预测师中选择一个最佳预测者，至少有一人至少预测成功8次的概率是`\(1-(1-P)^50=0.9399\)`，可见概率相当高。
    * 设`\(T_0\)`是初始决策树，`\(T_x\)`是插入属性x的内部结点后的决策树。原则上，如果观察的增益`\(\triangle(T_0,T_x)\)`大于预先设定的阈值`\(\alpha\)`，就可以将x添加到树中。
    * 如果属性测试条件很多，并且从候选集{`\(x_1,x_2,...,x_k\)`}中选择最佳属性`\(x_{max}\)`，这其实就是多重比较过程。
    * 当选择属性`\(x_{max}\)`的训练集很小时，这种影响会加剧，因为当训练记录较少时，函数`\(\triangle(T_0,T_x)\)`的方差会很大。这样找到`\(\triangle(T_0,T_x)>\alpha\)`的概率就增大了。(J:?)
4. 泛化误差估计
    * `再代入估计`方法假设训练数据集可以很好地代表整体数据，因而，可以使用训练误差提供对泛化误差的乐观估计。
    * `奥多姆剃刀`(Occam's razor)或`节俭原则`(principle of parsimony)。奥多姆剃刀:给定两个具有相同泛化误差的模型，较简单的模型比较复杂的模型更可取。
    * `悲观误差评估`(pessimistic error estimate)。设n(t)是结点t分类的训练记录数，e(t)是被误分类的记录数。决策树T的悲观误差估计`\(e_g(T)=\frac{\sum_{i=1}^k[e(t_i)+\Omega(T)]}{\sum_{i=1}^kn(t_i)}=\frac{e(T)+\Omega(T)}{N_t}\)`，其中，k是决策树的叶结点数，e(T)是决策树的总训练误差，`\(N_t\)`是训练记录数，`\(\Omega(t_i)\)`是每个结点`\(t_i\)`对应的罚项。
        * 图4-27，如果`\(\Omega(t_i)=0.5\)`，左决策树`\(e_g(T_L)=\frac{4+7·0.5}{24}=0.3125\)`，右决策树`\(e_g(T_R)=\frac{6+4·0.5}{24}=0.3333\)`。对二叉树来说，0.5的罚项意味着只要至少能够改善一个训练记录的分类，结点就应当扩展，因为扩展一个结点等价于总误差增加0.5，代价比犯一个训练错误小。![25](/assets/2014-02-08-data-mining/25.png)
        * 如果`\(\Omega(t_i)=1\)`，左决策树`\(e_g(T_L)=\frac{4+7·1}{24}=0.458\)`，右决策树`\(e_g(T_R)=\frac{6+4·1}{24}=0.417\)`。因此，右边决策树比左边决策树具有更好的悲观错误率。这样，除非能够减少一个以上训练记录的误分类，否则结点不应当扩展。
    * `最小描述长度原则`(minimum description length,MDL)
        * 考虑图4-28，A和B都是已知属性x值的给定记录集。另A知道每个记录的确切类标号，B不知道。B可通过要求A顺序传送类标号而获得每个记录分类。一条消息需要`\(\Theta(n)\)`比特信息，其中n是记录总数。
        * 另一种可能是，A决定建立一个分类模型，概括x和y之间的关系。在传送给B之前，模型用压缩编码形式编码。传输代价等于模型代价(Cost(model))和哪些记录被模型错误分类(Cost(data|model))。即Cost(model,data)=Cost(model)+Cost(data|model)
    * 估计统计上界(J:统计理解)
    * 使用确认集。保留2/3的训练集来建立模型，剩余1/3用作误差估计。
5. 处理决策树归纳中的过分拟合
    * 先剪枝(提前终止规则)
    * 后剪枝

###4.5 评估分类器的性能
1. 保持方法(Holdout)。4.4.4的使用确认集提到的，将原始数据划分成两个不相交的集合。
2. 随机二次抽样(random subsampling)。多次重复保持方法。设`\(acc_i\)`是第i次迭代的模型准确率，总准确率`\(acc_{sub}=\sum_{i=1}^k\frac{acc_i}{k}\)`
3. 交叉验证(cross-validation)。每一个记录用于训练的次数相同，且恰好验证一次。`k折交叉验证`，把数据分成相同的k份，在每次运行时，选择其中一份作检验集，其余的全作训练集，该过程重复k次，使得每一份数据都用于检验恰好一次。一种特殊情况，k=N，N是数据集大小，即所谓的`留一方法`(leave-one-out)，每个检验集只有一个记录。
4. 自助法(bootstrap)。前面的方法都假定训练记录采用不放回抽样，因此训练集和检验集都不包含重复记录。此法，训练记录采用有放回抽样。可证明(J:如何证明?)大小为N的自助样本大约包含原始数据中63.2%的记录。`.632自助`(.632 bootstrap)，通过组合每个自助样本的准确率`\(\xi_i\)`和由包含所有标记样本的训练集计算的准确率`\(acc_s\)`计算总准确率`\(acc_{boot}=\frac{1}{b}\sum_{i=1}^b(0.632·\xi_i+0.368·acc_s)\)`

###4.6 比较分类器的方法



`\(\)`
