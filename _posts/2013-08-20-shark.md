---
layout: post
title: "Shark: SQL and Rich Analytics at Scale"
description: ""
category: tech
tags: [paper, spark, shark]
---
{% include JB/setup %}
paper review:[Shark: SQL and Rich Analytics at Scale](http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-214.pdf)

<!--break-->
##2 System Overview

![shark1](/assets/2013-08-20-shark/shark1.png)

When a query is submitted to the master, Shark compiles the query into operator tree represented as RDDs. These RDDs are then translated by Spark into a graph of tasks to execute on the slave nodes.

####2.1 Spark
Spark is the MapReduce-like cluster computing engine used by Shark. Spark has several features that differentiate it from traditional MapReduce engines:

* Like Dryad and Tenzing, it supports general computation DAGs, not just the two-stage MapReduce topology.
* It provides an in-memory storage abstraction called Resilient Distributed Datasets (RDDs) that lets applications keep data in memory across queries, and automatically reconstructs it after failures.
* The engine is optimized for low latency. It can efficiently manage tasks as short as 100 milliseconds on clusters of thousands of cores, while engines like Hadoop incur a latency of 5–10 seconds to launch each task.

####2.2 Resilient Distributed Datasets (RDDs)
Spark’s main abstraction is resilient distributed datasets (RDDs), which are immutable, partitioned collections that can be created through various data-parallel operators (e.g., map, group-by, hash-join). Each RDD is either a collection stored in an external storage system, such as a file in HDFS, or a derived dataset created by applying operators to other RDDs. 

![shark2](/assets/2013-08-20-shark/shark2.png)

####2.3 Fault Tolerance Guarantees
* Shark can tolerate the loss of any set of worker nodes. The execution engine will reexecute any lost tasks and recompute any lost RDD partitions using lineage.
* Recovery is parallelized across the cluster. If a failed node contained 100 RDD partitions, these can be rebuilt in parallel on 100 different nodes, quickly recovering the lost data.
* The deterministic nature of RDDs also enables straggler mitigation: if a task is slow, the system can launch a speculative “backup copy” of it on another node, as in MapReduce.
* Recovery works even in queries that combine SQL and machine learning UDFs, as these operations all compile into a single RDD lineage graph.

####2.4 Executing SQL over RDDs
Given a query, Shark uses the Hive query compiler to parse the query and generate an abstract syntax tree. The tree is then turned into a logical plan and basic logical optimization, such as predicate pushdown, is applied. Up to this point, Shark and Hive share an identical approach. Hive would then convert the operator into a physical plan consisting of multiple MapReduce stages. In the case of Shark, its optimizer applies additional rule-based optimizations, such as pushing LIMIT down to individual partitions, and creates a physical plan consisting of transformations on RDDs rather than MapReduce jobs. We use a variety of operators already present in Spark, such as map and reduce, as well as new operators we implemented for Shark, such as broadcast joins. Spark’s master then executes this graph using standard MapReduce scheduling techniques, such placing tasks close to their input data, rerunning lost tasks, and performing straggler mitigation.

##3 Engine Extensions
####3.1 Partial DAG Execution (PDE)
To support dynamic query optimization in a distributed setting, we extended Spark to support partial DAG execution (`PDE`), a technique that allows dynamic alteration of query plans based on data statistics collected at run-time.

* **Join Optimization** Partial DAG execution can be used to perform several run-time optimizations for join queries.
* **Skew-handling and Degree of Parallelism** Partial DAG execution can also be used to determine operators’ degrees of parallelism and to mitigate skew.

####3.2 Columnar Memory Store
Shark implements a columnar memory store on top of Spark’s memory store.

The approach taken by Spark’s default memory store is to store data partitions as collections of JVM objects. This avoids deserialization, since the query processor can directly use these objects, but leads to significant storage space overheads.

Shark stores all columns of primitive types as JVM primitive arrays. Complex data types supported by Hive, such as map and array, are serialized and concatenated into a single byte array.

####3.3 Distributed Data Loading
Shark also uses Spark’s execution engine for distributed data loading. During loading, a table is split into small partitions, each of which is loaded by a Spark task. The loading tasks use the data schema to extract individual fields from rows, marshals a partition of data into its columnar representation, and stores those columns in memory.

Each task to choose the best compression scheme for each partition, rather than conforming to a global compression scheme that might not be optimal for local partitions. 

####3.4 Data Co-partitioning
Shark allows co-partitioning two tables on a common key for faster joins in subsequent queries. This can be accomplished with the DISTRIBUTE BY clause:

    CREATE TABLE l_mem TBLPROPERTIES ("shark.cache"=true)
    AS SELECT * FROM lineitem DISTRIBUTE BY L_ORDERKEY;
    
    CREATE TABLE o_mem TBLPROPERTIES (
        "shark.cache"=true, "copartition"="l_mem") 
    AS SELECT * FROM order DISTRIBUTE BY O_ORDERKEY;
When joining two co-partitioned tables, Shark’s optimizer constructs a DAG that avoids the expensive shuffle and instead uses map tasks to perform the join.

####3.5 Partition Statistics and Map Pruning
Map pruning is the process of pruning data partitions based on their natural clustering columns. Since Shark’s memory store splits data into small partitions, each block contains only one or few logical groups on such columns, and Shark can avoid scanning certain blocks of data if their values fall out of the query’s filter range.