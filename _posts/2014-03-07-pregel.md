---
layout: post
title: "Pregel"
description: "一种并行化图计算模型，即是编程模型也是底层数据处理的形式，类似spark的定位"
category: 云计算
tags: [Pregel,BSP,Graph]
---
{% include JB/setup %}
paper review:[Pregel: A System for Large-Scale Graph Processing][1]

##1. INTRODUCTION

* Pregel computations consist of a sequence of iterations, called `supersteps`. 
* During a superstep the framework invokes a `user-defined function` for each `vertex`, conceptually in `parallel`. The function specifies behavior at a single vertex V and a single superstep S. It can read messages sent to V in superstep S − 1, send messages to other vertices that will be received at superstep S + 1, and modify the state of V and its outgoing edges. Messages are typically sent along outgoing edges, but a message may be sent to any vertex whose identifier is known.

##2. MODEL OF COMPUTATION

* The input to a Pregel computation is a directed graph in which each vertex is uniquely identified by a string `vertex identifier`.
* Algorithm termination is based on every vertex `voting to halt`. 
    * In superstep 0, every vertex is in the `active` state; all active vertices participate in the computation of any given superstep.
    * A vertex `deactivates itself` by `voting to halt`. This means that the vertex has no further work to do unless triggered externally, and the Pregel framework will not execute that vertex in subsequent supersteps unless it receives a message.If `reactivated` by a message, a vertex must explicitly deactivate itself again. 
    * The algorithm as a whole terminates when all vertices are simultaneously inactive and there are no messages in transit. This simple state machine is illustrated in Figure 1.

![pregel1](/assets/2014-03-07-pregel/pregel1.png)

##3. THE C++ API

{% highlight cpp %}
template <typename VertexValue,          typename EdgeValue,          typename MessageValue>
class Vertex {public:  virtual void Compute(MessageIterator* msgs) = 0;    const string& vertex_id() const;  int64 superstep() const;    const VertexValue& GetValue();  VertexValue* MutableValue();  OutEdgeIterator GetOutEdgeIterator();    void SendMessageTo(const string& dest_vertex,                     const MessageValue& message);  void VoteToHalt();};
{% endhighlight %}

* `Message Passing.` All messages sent to vertex V in superstep S are available, via an iterator, when V ’s Compute() method is called in superstep S + 1.
* `Combiners.` Sending a message, especially to a vertex on another machine, incurs some overhead. This can be reduced in some cases with help from the user. For example, suppose that Compute() receives integer messages and that only the sum matters, as opposed to the individual values. In that case the system can combine several messages intended for a vertex V into a single message containing their sum, reducing the number of messages that must be transmitted and buffered.
* `Aggregators.` Pregel aggregators are a mechanism for global communication, monitoring, and data. Each vertex can provide a value to an aggregator in superstep S, the system combines those values using a reduction operator, and the resulting value is made available to all vertices in superstep S + 1.
* `Topology Mutations.` A clustering algorithm, for example, might replace each cluster with a single vertex, and a minimum spanning tree algorithm might remove all but the tree edges. Just as a user’s Compute() function can send messages, it can also issue requests to add or remove vertices or edges.

##4. IMPLEMENTATION
####4.1 Basic architecture
The Pregel library divides a graph into `partitions`, each consisting of a set of vertices and all of those vertices’ outgoing edges. Assignment of a vertex to a partition depends solely on the vertex ID.

The assignment of vertices to worker machines is the main place where distribution is `not transparent` in Pregel. Some applications work well with the default assignment, but some benefit from defining custom assignment functions to better exploit `locality inherent` in the graph. 

In the absence of faults, the execution of a Pregel program consists of several stages:

1. Many copies of the user program begin executing on a cluster of machines. One of these copies acts as the `master`. It is not assigned any portion of the graph, but is responsible for coordinating worker activity. The `workers` use the cluster management system’s name service to discover the master’s location, and send registration messages to the master.
2. The master determines how many partitions the graph will have, and assigns one or more partitions to each worker machine. The number may be controlled by the user. 
3. The master assigns a portion of the user’s input to each worker. The input is treated as a set of records, each of which contains an arbitrary number of vertices and edges. The division of inputs is orthogonal to the partitioning of the graph itself, and is typically based on file boundaries. If a worker loads a vertex that belongs to that worker’s section of the graph, the appropriate data structures (Section 4.3) are immediately updated. Otherwise the worker enqueues a message to the remote peer that owns the vertex. After the input has finished loading, all vertices are marked as active.
4. The master instructs each worker to perform a superstep. The worker loops through its active vertices, using one thread for each partition. The worker calls Compute() for each active vertex, delivering messages that were sent in the previous superstep. Messages are sent asynchronously, to enable overlapping of computation and communication and batching, but are delivered before the end of the superstep. When the worker is finished it responds to the master, telling the master how many vertices will be active in the next superstep.This step is repeated as long as any vertices are active, or any messages are in transit.
5. After the computation halts, the master may instruct each worker to save its portion of the graph.

####4.2 Fault tolerance
* Fault tolerance is achieved through `checkpointing`.
* Worker failures are detected using regular `“ping” messages` that the master issues to workers.
* When one or more workers fail, the current state of the partitions assigned to these workers is lost. The master reassigns graph partitions to the currently available set of workers, and they all reload their partition state from the most recent available checkpoint at the beginning of a superstep S. That checkpoint may be several supersteps earlier than the latest superstep S0 completed by any partition before the failure, requiring that `recovery` repeat the missing supersteps.
* `Confined recovery`: In addition to the basic checkpoints, the workers also log outgoing messages from their assigned partitions during graph loading and supersteps. Recovery is then confined to the lost partitions, which are recovered from checkpoints. The system recomputes the missing supersteps up to S0 using logged messages from healthy partitions and recalculated ones from recovering partitions.

[1]:http://kowshik.github.io/JPregel/pregel_paper.pdf
