---
layout: post
title: "读书笔记-概率论基础教程"
description: "《概率论基础教程》Sheldon Ross "
category: 数学
tags: [概率]
---
{% include JB/setup %}

读[《概率论基础教程》](http://book.douban.com/subject/4291764/)。

![zen](http://img5.douban.com/lpic/s4205606.jpg)

##第一章 组合分析

1. `计数基本法则`阐述了如下事实：如果一个试验可分成两个阶段，第一个阶段有n种可能结果，每种结果又对应第二个阶段的m种可能的结果，那么试验一共有nm种可能结果。
2. n个元素的`排列`(permutation)一共有`\(n!=n(n-1)\cdot \cdot \cdot 3\cdot 2\cdot 1\)`种可能排列方式，特别地，0！=1.
3. 令`\({n \choose i} =\frac {n!}{(n-i)!i!} \)`,其中`\(0\le i\le n\)`,否则等于0.此式表明了从n个元素中选取i个元素的可能选取方法数，`\({n \choose i} \)`称为从n个对象中选取i个对象的`组合`(combination)数，因其在`二项式定理`中的突出地位，它也常称为`二项式系数`(binomial coefficient)，我们有`\((x+y)^n = \sum_{i=0}^n {n \choose i} x^iy^{n-i}\)`
    * `多项式定理`：`\((x_1+x_2+...+x_r)^n=\sum_{(n_1,...,n_r):n_1+...+n_r=n}{n \choose n_1,n_2,\cdot \cdot \cdot,n_r} x_1^{n_1}x_2^{n_2}...x_r^{n_r}\)`,上式的求和是对一切满足`\(n_1+n_2+...+n_r=n\)`的所有非负整向量`\((n_1,n_2,...,n_r)\)`求和。
4. 对于任意和为n的非负整数`\(n_1,\cdot \cdot \cdot,n_r, {n \choose n_1,\cdot \cdot \cdot,n_r} = \frac {n!}{(n_1)!(n_2)\cdot \cdot \cdot(n_r)!}\)`,它等于n个元素分成互不重叠的r部分，其中各个部分的元素个数分别是`\(n_1,\cdot \cdot \cdot,n_r\)`的分法数。
5. 方程的整数解个数。
    * 共有`\({n-1 \choose r-1}\)`个不同的具有正整数分量的向量`\((x_1,x_2,...,x_r)\)`满足`\(x_1+x_2+...+x_r=n,x_i>0,i=1,...,r\)`
    * 为了得到非负整数解个数，令`\(y_i=x_i+1,i=1,...,r\)`，则`\(x_1+x_2+...+x_r=n\)`的非负整数解的个数与`\(y_1+y_2+...+y_r=n+r\)`的正整数解个数相同，因此共有`\({n+r-1 \choose r-1}\)`个不同的具有非负整数分量的向量`\((x_1,x_2,...,x_r)\)`满足`\(x_1+x_2+...+x_r=n,x_i≥0,i=1,...,r\)`

##第二章 概率论公理化

1. 如果令S为表示某个试验的所有可能结果的集合，那么S称为该试验的`样本空间`。一个`事件`就是S的一个子集。如果`\(A_i，i = 1,\cdot \cdot \cdot,n\)`为一系列事件，那么称`\(\bigcup_{i=1}^{n}{A_i}\)`为这些事件的`并`，它表示至少包含在某一个`\(A_i\)`里的所有结果所构成的事件。类似地，`\(\bigcap_{i=1}^{n}{A_i}\)`称为这些事件的`交`，有时也记为`\(A_1 \cdot \cdot \cdot A_n\)`,表示包含在所有`\(A_i\)`里的所有结果所构成的事件。
2. 对任一事件A，定义`\(A^c\)`为由那些不包含在A里的所有结果所构成的事件，称为A的`对立事件`。事件`\(S^c\)`不包含任何结果，记为`\(\phi\)`,称为`空集`。如果`\(AB = \phi\)`，那么称A和B互不相容。
3. 设对于样本空间的任一事件A，对应于一个数P(A)，若集合函数P(A)满足以下条件，则称P(A)为A的`概率`:
      * `\(0 \le P(A) \le 1\)`
      * P(S) = 1
      * 对于任意互不相容事件`\(A_i, i \ge 1\)`,有`\(P(\bigcap_{i=1}^{n}{A_i}) = \sum_{i=0}^\infty{P(A_i)}\)`
4. P(A)表示试验结果包含在事件A里的概率，容易证明: `\(P(A^c) = 1 - P(A)\)`
5. 一个有用的结果：`\(P(A \cup B) = P(A) + P(B) - P(AB)\)`，可以推广为`\(P(\bigcup_{i=1}^{n}{A_i}) = \sum_{i=1}^n{P(A_i)} - \sum_{i-1}\sum{P(A_iA_j)} + \sum\sum_{i<j<k}\sum{P(A_iA_jA_k)} + \cdot \cdot \cdot + (-1)^{n+1}P(A_1 \cdot \cdot \cdot A_n)\)`
6. 如果S是有限集，其中每个结果发生的可能性是一样的，那么`\(P(A) = \frac{|A|}{|S|}\)`，其中|E|表示事件E所含的结果数。
7. 当涉及从n个对象中随机地选取k个对象的时候，我们可以认为对象是从n个对象的集合中一个一个地有序地选出来的，也可以认为从n个对象中一次取出k个对象。当一个一个地取出来的时候，我们假定每次选择对象的时候，在待选的集合中每一个对象有相同的概率被选中。在一次选择k个对象的时候，我们假定`\({n \choose k}\)`个可能的子集有相同的机会被选中。
    * 假定一个集体由10对夫妇组成，现在从中随机地选择5人。我们希望计算P(N)，其中事件N表示这5人中没有一对夫妇。
    * 首先，这20人中一共有`\({20 \choose 5}\)`个不同的人员组合，每一个可能的人原组合都有相同的机会被选中。而N这个事件中的结果可以看成6个阶段，第一阶段从10对夫妇中选定5对夫妇，后面的5个阶段是顺次地从每一对夫妇中选出一人。这样的试验共有`\({10 \choose 5}2^5\)`种不同的结果。这样，`\(P(N)=\frac{ {10 \choose 5}2^5 }{ {20 \choose 5} }\)`
    * 我们也可以从顺序地选择这样的观点来计算P(N)。从20个人中顺序地选择5个人，一共有20·19·18·17·16种等概的试验结果。但是选择5个没有夫妇关系的5人组，只有20·18·16·14·12个试验结果。这样`\(P(N)=\frac{20·18·16·14·12}{20·19·18·17·16}\)`

##第三章 条件概率和独立性

1. 对于任意事件E和F，已知F发生的条件下，E发生的`条件概率`记为P(E|F)，定义如下`\(P(E|F) = \frac{P(EF)}{P(F)}\)`
2. 等式`\(P(E_1E_2 \cdot \cdot \cdot E_n) = P(E_1)P(E_2|E_1) \cdot \cdot \cdot P(E_n|E_1 \cdot \cdot \cdot E_n-1)\)`称为概率的`乘法规则`。
3. 一个有用的等式 `\(P(E) = P(E|F)P(F) + P(E|F^c)P(F^c)\)`可以来通过以F是否发生为条件计算P(E)。此公式为`全概率公式`。
4. `\(P(H)/P(H^c)\)`称为事件H的`优势`。等式`\(\frac{P(H|E)}{P(H^c|E)} = \frac{P(H)P(E|H)}{P(H^c)P(E|H^c)}\)`证明了当得到一个新的证据E后，H的优势等于原来的优势值乘以当H成立时新证据发生的概率与H不成立时新证据发生的概率的比值。
5. 令`\(F_i = 1, \cdot \cdot \cdot, n\)`为互不相容事件列，且它们的并为整个样本空间，等式`\(P(F_j|E) = \frac{P(E|F_j)P(F_j)}{\sum_{i=1}^n{P(E|F_i)P(F_i)}}\)`称为`贝叶斯公式`。如果事件`\(F_i = 1, \cdot \cdot \cdot, n\)`为一组假设，那么贝叶斯公式说明了如何计算当新证据成立时，这些假设成立的条件概率。
6. 如果P(EF) = P(E)P(F)，那么我们称事件E和F是`独立`的。该等式等价于P(E|F) = P(E)或P(F|E) = P(F)。也即，如果知道其中之一的发生并不影响另一个的发生的概率，那么E和F独立。

##第四章 随机变量

1. 定义在试验结果上的实值函数称为`随机变量`(random variable)。如果X是一个随机变量，那么如下定义的函数F(X) = P{X ≤ x} 称为随机变量的`分布函数`(distribution function)。任意有关X的概率可以通过F进行计算。
2. 若一个随机变量的可能取值的集合是有限集，或者可数无限集，那么称该随机变量为`离散型`随机变量。如果X是一个离散型随机变量，那么函数p(x) = P{X = x}称为X的`概率分布列`或分布列。另外，如下定义的`\(E[X] = \sum_{x:p(x) > 0}{xp(x)}\)`称为X的`期望值`(expected value)，E[X]通常也称为X的`均值`(mean)或`期望`(expectation)。
3. 设g(x)是一个实值函数，则对于离散型随机变量X，关于E[g(X)]的一个有用的计算公式为:`\(E[g(X)] = \sum_{x:p(x) > 0}{g(x)p(x)}\)`。随机变量X的`方差`(variance)，记为Var(x)，定义为`\(Var(X) = E[(X - E[X])^2]\)`。方差等于X与它的期望的差的平方的期望，它度量了X可能取值的分散程度。下面是一个有用的恒等式`\(Var(X) = E[X^2] - (E[X])^2\)`。
4. `\(\sqrt{Var(X)}\)`称为X的`标准差`(standard deviation)。
5. 期望的一个重要性质是：随机变量和的期望等于它们的期望的和。即`\(E[\sum_{i=1}^n{X_i}] = \sum_{i=1}^n{E[X_i]}\)`

一些常用的离散型随机变量:

1. 若随机变量X的分布列为`\(p(i) = {n \choose i}p^i(1-p)^{n - i}, i = 0,\cdot \cdot \cdot, n\)`，则X称为参数为(n,p)的`二项随机变量`。该随机变量可解释为n次独立重复试验中试验成功的次数，而每次试验成功的概率为p。它的均值:E[X] = np,方差:Var[X] = np(1-p)。
    * 如果X是一个参数为(n,p)的二项随机变量，其中0<p<1,那么当k从0到n时，P{X=k}一开始单调递增，然后一直单调递减，它在k=[(n+1)p]时取最大值(记号[X]表示不大于X的最大整数) (命题6.1)
    * 设X是一个参数为(n,p)的二项随机变量，计算分布函数`\(P\{X \le i\} = \sum_{k=0}^i{\dbinom{n}{k}p^k(1-p)^{n-k}}, i = 0,1,...,n\)`的方法是利用命题6.1得到如下P{X=k+1}和P{X=k}之间的关系：`\(P\{X=k+1\} = \frac{p}{1-p}\frac{n-k}{k+1}P\{X=k\}\)`
2. 若随机变量X的分布列为`\(p(i) = \frac{e^{-\lambda}\lambda^i}{i!}, i = 0,1,\cdot \cdot \cdot\)`,则X称为参数为`\(\lambda\)`的`泊松随机变量`。如果进行多次(近似)独立的试验，而且每次成功的概率都较小，那么总的成功次数就近似为泊松随机变量。泊松随机变量的均值和方差都等于其参数`\(\lambda\)`，也即E[X] = Var[X] = `\(\lambda\)`。
    * 当n足够大时，p充分小，而使得np保持适当的大小时，以(n,p)为参数的二项随机变量可近似看作泊松分布(即成功的次数近似服从参数为`\(\lambda\)`=np的泊松分布)。以下例子中的随机变量通常都服从泊松分布:
        * 一本书里一页或若干页中的印刷错误
        * 某地区居民活到100岁的人数
        * 一天中拨错电话号码的总数
        * 一家便利店里每天卖出狗粮饼干的盒数
        * 某一天进入某邮局的顾客数
    * 事实上，在试验并不独立，但是弱相依条件下仍是比较好的近似。例如，匹配问题(2.5m)，其中n个人随机地从他们的帽子中取一顶帽子，考虑恰好拿着自己帽子的人数。可认为这n个选择就是n次试验，其中第i次成功就是第i个人拿到了自己的帽子，i=1,...,n。定义事件`\(E_i\)`={第i次试验成功},i=1,...,n。很容易看出`\(P\{E_i\} = \frac{1}{n}\)`，且`\(P\{E_i|E_j\} = \frac{1}{n-1},j \ne i\)`，那么很合理地认为成功的次数近似服从参数为n x 1/n = 1的泊松分布。
    * `泊松范例`:考虑n个事件，每个事件发生的概率为`\(p_i\)`,i=1,...,n。如果所有的`\(p_i\)`都很小，且试验或者独立，或者至多弱相依，那么事件发生次数近似服从参数为`\(\sum_{i=1}^n{p_i}\)`的泊松分布。
    * 泊松分布的另一应用表现在这样的情形中，“事件”发生在某些时间点上。这种事件的例子有：发生一次地震，某人进入特定地点(如银行、邮局、加油站)，爆发一次战争等。我们假设这样的事件发生在一列(随机)时间点上，并设存在某个正的常数`\(\lambda\)`使得如下条件成立:
        * 在任意长度为h的时间区间内，正好发生一个事件的概率彼此相同，都等于`\(\lambda h + \omicron(h)\)`,其中`\(\omicron(h)\)`表示任何满足`\(\lim_{h \to 0}\frac{f(h)}{h}=0\)`的函数f(h)。
        * 在任何长度为h的时间区间内发生2个或更多个事件的概率非常小，等于`\(\omicron(h)\)`。
        * 对于任意确定的自然数n与非负整数`\(j_1,j_2,...,j_n\)`，以及任意n个互不相交的时间区间，若以`\(E_i\)`表示“在第i个时间区间内上述事件正好发生`\(j_i\)`次”，则`\(E_1,E_2,...,E_n\)`相互独立。
        * 粗略地说，条件1和条件2说明，当h比较小时，在长度为h的区间内正好发生1个事件的概率等于`\(\lambda h\)`加上某个比h更小的变量，而事件发生多于一次的概率就是一个比h更小的量。条件3说明，在一个时间区间内无论发生了什么，对另一个与它不相交的区间(从概率意义上)没有影响。这样，如果事件的发生满足条件1，2，3，则在任何固定的长度为t的时间区间内，事件发生的次数是以`\(\lambda t\)`为参数的泊松随机变量。这时，我们称事件是按速率为`\(\lambda\)`的泊松过程发生的。数`\(\lambda\)`可解释为单位时间内事件发生的速率。它一定是由经验确定的常数。
    * 计算泊松分布函数:如果X服从参数为`\(\lambda\)`的泊松分布，则`\(\frac{P\{X=i+1\}}{P\{X=i\}}=\frac{e^{-\lambda}\lambda^{i+1}/(i+1)!}{e^{-\lambda}\lambda^i/i!}=\frac{\lambda}{i+1}\)`(7.6)
    * 从P{X=0}=`\(e^{-\lambda}\)`开始，利用7.6可以计算出
        * P{X=1}=`\(\lambda\)`P{X=0}
        * P{X=2}=`\(\frac{\lambda}{2}\)`P{X=1}
        * ...
        * P{X=i+1}=`\(\frac{\lambda}{i+1}\)`P{X=i}
3. 若随机变量X的分布列为`\(p(i) = p(1 - p)^{i-1}, i = 1,2,\cdot \cdot \cdot\)`，则X称为参数为p的`几何随机变量`。在独立重复试验序列中，从开始直到第一次成功为止的试验次数就是几何随机变量，其分布参数p就是每次试验成功的概率。其均值和方差分别是E[X] = `\(\frac{1}{p}\)`,Var[X] = `\(\frac{1 - p}{p^2}\)`。
4. 若随机变量X的分布列为`\(p(i) = \dbinom{i-1}{r-1}p^r(1-p)^{i-r}, i=r,r+1,\cdot \cdot \cdot\)`，则X称为参数为(r,p)的`负二项随机变量`。在独立重复试验序列中，从开始直到第r次成功为止的试验次数就是`负二项随机变量`。分布列中的参数p就是每次试验成功的概率。其均值和方差分别为E[X] = `\(\frac{r}{p}\)`,Var(X) = `\(\frac{r(1-p)}{p^2}\)`。
5. 设从一个装有N个球，其中m个白球的坛子里，随机抽取n个球，其中白球的数目就是参数为(n,N,m)的`超几何随机变量`。其分布列为`\(p(i) = \frac{\dbinom{m}{i}\dbinom{N-m}{n-i}}{\dbinom{N}{i}},i=0,1,\cdot \cdot \cdot\,m\)`，其均值为E[X] = np,方差为Var[X] =`\(\frac{N-n}{N-1}np(1-p)\)`，其中`\(p=\frac{m}{N}\)`
6. 一个随机变量称为服从`\(\zeta\)`分布(也称为`Zipf分布`)，如果其分布列如下:`\(P\{X=k\}=\frac{C}{k^{\alpha+1}},k=1,2,...\)`，其中`\(\alpha\)`>0为参数，因为概率之和必然等于1，因此有`\(C=[\sum_{k=1}^{\infty}{(\frac{1}{k})^{\alpha+1}}]^{-1}\)`。
    * `\(\zeta\)`分布的名字来源于函数`\(\zeta(s)=1+(\frac{1}{2})^s+...+(\frac{1}{k})^s+...\)`，它是数学中熟知的黎曼`\(\zeta\)`函数。

##第五章 连续型随机变量

1. 一个随机变量X称为`连续型`的，如果存在一个x的非负函数f(称为`密度函数`)，满足：对于任一集合B，有`\(P\{X \in B\} = \int_{B} f(x)dx\)`。
2. 如果X是连续型的，那么其分布函数F在f(x)的连续点处可导，且`\(\frac{d}{dx}F(x) = f(x)\)`
3. 连续型随机变量X的期望值为:`\(E[X] = \int_{-\infty}^{\infty}{xf(x)dx}\)`，对于任一函数g(x)，有一个恒等式:`\(E[g(x)] = \int_{-\infty}^{\infty}{g(x)f(x)dx}\)`,随机变量X的方差定义为：`\(Var(X) = E[(X - E[X])^2]\)`

一些常用的连续型随机变量:

1. 随机变量X称为服从区间(a,b)上的`均匀分布`，如果其密度函数为`\(f(x) = \begin{cases} \frac{1}{b-a}, a≤x≤b\\ 0,其他 \end{cases} \)`，其期望是`\(E[X] = \frac{a+b}{2}\)`,方差为`\(Var(X) = \frac{(b-a)^2}{12}\)`
2. 随机变量X称为服从参数为`\(\mu\)`和`\(\sigma^2\)`的`正态`随机变量，如果其密度函数为`\(f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^2/2\sigma^2}, -\infty < x < \infty\)`，可以证明`\(\mu = E[X], \sigma^2 = Var[X]\)`
    * 如果X服从均值为`\(\mu\)`,方差为`\(\sigma^2\)`的正态分布，那么如下定义的Z:`\(Z = \frac{X-\mu}{\sigma}\)`也是服从正态分布的随机变量，其均值为0，方差为1.这样的随机变量称为`标准正态`随机变量。
    * 参数为(n,p)的二项分布，当n足够大时，可以近似为均值为np，方差为np(1-p)的正态分布。
3. 一个随机变量称为参数为`\(\lambda\)`的`指数`随机变量，如果其密度函数为如下形式:`\(f(x) = \begin{cases} \lambda e^{-\lambda x}, x≥0\\ 0,其他 \end{cases}\)`，其期望为:`\(E[X] = \frac{1}{\lambda}\)`，方差为:`\(Var[X] = \frac{1}{\lambda^2}\)`
    * 一个只有指数随机变量才具有的重要性质是`无记忆性`，也即对于正数s和t，有`\(P\{X > s + t | X > t\} = P\{X > s\}\)`，如果X表示某个零件的寿命，那么无记忆性说明了对任意t，年龄为t的零件的剩余寿命同一个新的零件的寿命的分布是一样的。
    * 令X为一个非负连续型随机变量，其分布函数为F，密度函数为f，那么函数`\(\lambda(t) = \frac{f(t)}{1-F(t)},t≥0\)`称为F的`危险率`或`失效率`函数，如果我们认为X是某个零件的寿命，那么对于一个很小的值dt，`\(\lambda(t)dt\)`近似为年龄为t的零件在dt时间内会失效的概率。如果F是参数为`\(\lambda\)`的指数分布，那么`\(\lambda(t) = \lambda,t≥0\)`。另外，指数分布是唯一的失效率为常数的分布。
4. 一个随机变量称为参数为`\((\alpha,\lambda)\)`的`\(\Gamma\)`随机变量，如果其密度函数等于`\(f(x) = \frac{\lambda e^{-\lambda x}(\lambda x)^{\alpha-1}}{\Gamma(\alpha)}, x≥0\)`，`\(\Gamma(\alpha)\)`称为`\(\Gamma\)`函数，定义为`\(\Gamma(\alpha) = \int_0^{\infty}e^{-x}x^{\alpha-1}dx\)`。`\(\Gamma\)`随机变量的期望为：`\(E[X] = \frac{\alpha}{\lambda}\)`，方差为:`\(Var(X) = \frac{\alpha}{\lambda^2}\)`
5. 随机变量称为服从参数为(a,b)的`\(\beta\)`分布，如果其密度函数为`\(f(x) = \frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1},0≤x≤1\)`，常数B(a,b)的定义为:`\(B(a,b) = \int_0^1 x^{a-1}(1-x)^{b-1} dx\)`，`\(\beta\)`随机变量期望:`\(E[X] = \frac{a}{a+b}\)`，方差:`\(Var(X) = \frac{ab}{(a+b)^2(a+b+1)}\)`

##第六章 随机变量的联合分布

1. X和Y的`联合分布函数`定义为:`\(F(x,y) = P\{X≤x,Y≤y\}, -\infty < x,y < \infty\)`。所有关于X,Y的概率都可以由F得到。为了求X和Y各自的分布函数，利用`\(F_X(x) = \lim_{y \to \infty}F(x,y), F_Y(y) = \lim_{x \to \infty}F(x,y)\)`
    * 若X和Y均为离散型随机变量，则它们的联合分布也是离散的，其联合分布列为:`\(p(i,j) = P\{X=i,Y=j\}\)`，X,Y的各自分布列为`\(P\{X=i\} = \sum_j{p(i,j)}, P\{Y=j\} = \sum_i{p(i,j)}\)`
    * 随机变量X和Y称为联合连续的，如果存在一个二元函数，称为联合密度函数f(x,y)，使得对任意二维集合C，`\(P\{(X,Y) \in C\} = \int\int_C{f(x,y)dxdy}\)`，从此式可知`\(P\{x<X<x+dx,y<Y<y+dy\} \approx f(x,y)dxdy\)`
    * 若X和Y联合连续，则它们各自都为连续型的，且密度函数分别为：`\(f_X(x) = \int_{-\infty}^\infty{f(x,y)dy}, f_Y(y) = \int_{-\infty}^\infty{f(x,y)dx}\)`
2. 随机变量X和Y称为`独立的`，如果对任意集合A和B，有`\(P\{X \in A,Y \in B\} = P\{X \in A\}P\{Y \in B\}\)`
    * 若联合分布函数(或者离散情形下的联合分布列，或者连续情形下的联合密度)可以分解为两个因子，其中一个只依赖于x，另一个只依赖于y，则X和Y独立。
    * 一般情况下，随机变量 `\(X_1,\cdot \cdot \cdot,X_n\)`称为相互独立的，若对一切实数集`\(A_1,\cdot \cdot \cdot,A_n\)`有`\(P\{X_1 \in A_1,\cdot \cdot \cdot,X_n \in A_n\} = P\{X_1 \in A_1\}\cdot \cdot \cdot P\{X_n \in A_n\}\)`
    * 若X和Y为独立连续型随机变量，则它们的和的分布函数可以通过此式得到:`\(F_{X+Y}(a) = \int_{-\infty}^{\infty}{F_X(a-y)f_Y(y)dy}\)`

3. 若`\(X_i,i=1,\cdot \cdot \cdot,n\)`，为`独立正态`随机变量，参数分别为`\(\mu_i\)`和`\(\sigma_i^2,i=1,\cdot \cdot \cdot,n\)`，则`\(\sum_{i=1}^n{X_i}\)`也是正态分布随机变量，参数为`\(\sum_{i=1}^n{\mu_i}\)`和`\(\sum_{i=1}^n{\sigma_i^2}\)`
4. 若`\(X_i,i=1,\cdot \cdot \cdot,n\)`，为`独立泊松`随机变量，参数分别为`\(\lambda_i,i=1,\cdot \cdot \cdot,n\)`,则`\(\sum_{i=1}^n{X_i}\)`也服从泊松分布，参数为`\(\sum_{i=1}^n{\lambda_i}\)`
5. 若X和Y为离散型随机变量，则已知Y=y的条件下，X=x的条件分布列定义:`\(P\{X=x|Y=y\} = \frac{P(x,y)}{p_Y(y)}\)`，其中p(x,y)为X，Y的联合分布列，若X，Y的联合连续且其相应的联合密度为f，则X在给定Y=y之下的条件密度函数为`\(f_{X|Y}(x|y) = \frac{f(x,y)}{f_Y(y)}\)`
6. 设`\(X_1,\cdot \cdot \cdot,X_n\)`为独立同分布的随机变量序列，将它们进行排序以后得到`\(X_{(1)} ≤ X_{(2) ≤ \cdot \cdot \cdot ≤ X_{(n)}}\)`称为`\(X_1,\cdot \cdot \cdot,X_n\)`的`次序统计量`。设这些随机变量是连续的，即存在密度函数`\(f(x_i)\)`，则它们的次序统计量的密度函数为`\(f(x_1,\cdot \cdot \cdot,x_n) = n!f(x_1) \cdot \cdot \cdot f(x_n),x_1≤x_2≤\cdot \cdot \cdot≤x_n\)`
7. 随机变量序列`\(X_1,\cdot \cdot \cdot,X_n\)`称为`可交换`的，若对每一个`\(\{1,2,\cdot \cdot \cdot,n\}\)`的排列`\(i_1,i_2,\cdot \cdot \cdot,i_n\)`其相应的`\(X_{i_1},\cdot \cdot \cdot,X_{i_n}\)`的联合分布是相同的

##第七章 期望的性质

1. 设X，Y具有联合分布列p(x,y)，则`\(E[g(X,Y)] = \sum_y\sum_x{g(x,y)p(x,y)}\)`,若它们具有联合密度f(x,y)，则`\(E[g(X,Y)] = \int_{-\infty}^\infty{\int_{-\infty}^\infty{g(x,y)f(x,y)dxdy}}\)`
2. 若令g(x,y)=x+y，可得E[X+Y] = E[X] + E[Y],这个公式可以推广到`\(E[\sum_{i=1}^n{X_i}] = \sum_{i=1}^n{E[X_i]}\)`
3. X,Y的`协方差`由下式定义：Cov(X,Y) = E[(X-E[X])(Y-E[Y])] = E[XY] - E[X]E[Y]
    * 下面的恒等式十分有用:`\(Cov(\sum_{i=1}^n{X_i},\sum_{j=1}^m{Y_j}) = \sum_{i+1}^n{\sum_{j=1}^m{Cov(X_i,Y_j)}}\)`
    * 当`\(n=m,X_i=Y_j,i=1,2,\cdot \cdot \cdot,n, Var(\sum_{i=1}^n{X_i}) = \sum_{i=1}^n{Var(X_i) + 2\sum\sum_{i<j}{Cov(X_i,Y_j)}}\)`
    * X，Y之相关系数`\(\rho(X,Y)\)`由下式定义:`\(\rho(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}\)`
4. 若X，Y为二元离散随机变量，则在Y=y的条件下，X的条件期望由下式给出:`\(E[X|Y = y] = \sum_x{xP\{X=x|Y=y\}}\)`，如果是二元连续的，则`\(E[X|Y = y] = \int_{-\infty}^\infty{xf_{X|Y}(x|y)dx}\)`，其中`\(f_{X|Y}(x|y)=\frac{f(x,y)}{f_Y(y)}\)`为Y=y条件下的`条件密度`。条件密度的性质与通常期望的性质是类似的，只是在计算中，所有概率都是Y=y之下的条件概率。
    * 记E[X|Y]表示Y的函数，当Y=y时，其值为E[X|Y=y]。关于条件期望的一个重要恒等式为E[X] = E[E[X|Y]]
    * 在离散情况下，是`\(E[X]=\sum_y{E[X|Y=y]P\{Y=y\}}\)`
    * 在连续情况下，是`\(E[X]=\int_{-\infty}^\infty{E[X|Y=y]f_Y(y)dy}\)`
    * 上述公式可以用来计算E[X]。其方法是先固定Y=y，求出条件期望E[X|Y=y],再对Y求期望。此外，对于每一个事件A，`\(P(A)=E[I_A]\)`其中`\(I_A\)`是事件A的`示性函数`。我们也可以利用上述关于期望的计算公式来计算P(A)。
5. X在Y=y之下的条件方差由下式定义:`\(Var(X|Y=y) = E[(X-E[X|Y=y])^2|Y=y]\)`
    * 记Var(X|Y)表示Y的函数，在Y=y处，Var(X|Y)的值是Var(X|Y=y)。下面的公式称为`条件方差公式`:`\(Var(X)=E[Var(X|Y)] + Var(E[X|Y])\)`
    * 设我们可观察到随机变量X的值，我们希望根据X的观察值，预测随机变量Y的值，在这种情况下，E[X|Y]是使均方差达最小的预测值。
6. 随机变量X的`距母函数`由下定义:`\(M(t)=E[e^{tX}]\)`，X的各阶矩都可以从M(t)的各阶微商在t=0处的值得到。特别的，`\(E[X^n] = \frac{d^n}{dt^n}M(t)|_{t=0},n=1,2,\cdot \cdot \cdot\)`
    * 矩母函数的两个重要性质是:
        * 随机变量的矩母函数唯一地确定它的分布
        * 独立随机变量和矩母函数等于各随机变量的矩母函数的乘积，这个结果可使下列结果的证明简化：独立正态(泊松或`\(\Gamma\)`)随机变量的和的分布仍然为正态(泊松或`\(\Gamma\)`)分布。
7. 若`\(X_1,\cdot \cdot \cdot,X_m\)`均为有限个相互独立的标准正态随机变量的线性组合，则称`\(X_1,\cdot \cdot \cdot,X_m\)`为`联合正态`。其联合分布由`\(E[X_i],Cov(X_i,X_j),i,j = 1,\cdot \cdot \cdot,m\)`所确定。
8. 设`\(X_1,\cdot \cdot \cdot,X_n\)`为独立同分布的正态随机变量序列，期望为`\(\mu\)`，方差为`\(\sigma^2\)`，则其样本均值`\(\bar{X} = \frac{1}{n}\sum_{i=1}^n{X_i}\)`和样本方差`\(S^2 = \frac{1}{n-1}\sum_{i=1}^n{X_i-\bar{X}}^2\)`相互独立。样本均值`\(\bar{X}\)`是正态随机变量，期望为`\(\mu\)`，方差为`\(\frac{\sigma^2}{n}\)`；`\(\frac{(n-1)S^2}{\sigma^2}\)`是`\(\chi^2\)`随机变量，自由度为n-1。

##第八章 极限定理

1. 马尔可夫和切比雪夫不等式是概率论中十分重要的不等式，可提供有关概率的上界。
    * 马尔可夫不等式所涉及的是非负随机变量，对于非负随机变量X，`\(P\{X \ge a\}\le \frac{E[X]}{a}\)`
    * 切比雪夫不等式是马尔可夫不等式的应用。设X具有期望`\(\mu\)`，方差`\(\sigma^2\)`，则对于每一个k > 0，有`\(P\{|X - \mu| \ge k\sigma\} \le \frac{1}{k^2}\)`,此不等式中的随机变量X不再限于非负随机变量了。
2. 概率论中两个重要的理论是`中心极限定理`和`强大数律`，它们都讨论独立同分布随机变量序列的性质。若这些随机变量具有期望`\(\mu\)`，方差`\(\sigma^2\)`，则
    * 中心极限定理说明当n充分大时，这个序列的前n个变量的和的分布，近似地为正态分布，其期望为`\(n\mu\)`，方差`\(n\sigma^2\)`。设`\(\{X_i, i \ge 1\}\)`是这样一个序列，则对于每一个实数a，`\(\lim_{n->\infty}{P\{\frac{X_1 + \cdot \cdot \cdot + X_n - n\mu}{\sigma\sqrt{n}} \le a\}} = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^a{e^{\frac{-x^2}{2}}}dx\)`
    * 强大数律只要求该序列中的随机变量具有有限个均值`\(\mu\)`，强大数律说明，当n趋于无穷时，这个序列的前n项的平均值以概率为1地趋于`\(\mu\)`。由强大数律可知，在独立重复试验中，事件A出现的频率以概率为1地趋于概率P(A)。如果将以概率为1，解释成“一定”，这样，我们就验证了概率的频率定义的合理性。

##第九章 泊松过程与马尔可夫链

1. 具有速率`\(\lambda\)`的泊松过程是一组随机变量{N(t),t≥0},它涉及时间轴上的随机点上的发生的事件，例如，N(t)表示在时间区间(0,t]上发生事件的个数，泊松过程由下面几个条件所界定:
    * 在不相重的时间区间上发生的事件数是相互独立的
    * 在一个区间内发生事件的个数的分布只依赖于这个区间的长度
    * 在一个时刻只发生一个事件
    * 事件发生的速率为`\(\lambda\)`
    * 可以证明N(t)是泊松随机变量，其期望值为`\({\lambda}t\)`。此外，两个相邻的事件之间的时间间隔`\(T_i(i \ge 1)\)`是独立同分布的指数随机变量，参数为`\(\lambda\)`

2. 取值于`\(\{0,1,\cdot \cdot \cdot, M\}\)`的随机变量序列`\(X_n, n \ge 0\)`称为具有转以概率`\(P_{ij}\)`的马尔可夫链，如果对于一切`\(n,i_0,\cdot \cdot \cdot,i_n,i,j, P\{X_{n+1} = j|X_n=i,X_{n-1}=i_{n-1},\cdot \cdot \cdot,X_0=i_0\} = P_{ij}\)`,如果我们将`\(X_n\)`解释为时刻n的状态，然后马尔可夫链可以解释为一串顺序的状态序列，如果某一时刻在状态i，下一时刻在状态j的概率为`\(P_{ij}\)`，它与以前的历史相互独立。有许多马尔可夫链，处于状态j的概率当`\(n->\infty\)`时有一个极限概率，它不依赖于初始状态，若用`\(\pi_j(j=0,\cdot \cdot \cdot,M)\)`表示这些概率，它们是下列方程组的唯一解:
    * `\(\pi_i = \sum_{i=0}^M{\pi_iP_{ij}},j=0,1,\cdot \cdot \cdot,M\)`
    * `\(\sum_{j=1}^M{\pi_j} = 1\)`
    * 进一步，`\(\pi_j\)`也是马尔可夫链当n充分大时，处于状态j的时刻的比例。
3. 设X是取之于`\(\{x_1,\cdot \cdot \cdot,x_n\}\)`的随机变量，其相应的概率为`\(\{p_1,\cdot \cdot \cdot,p_n\}\)`，量`\(H(X) = -\sum_{i=1}^n{p_ilog_2(p_i)}\)`称为随机变量X的`熵`，它可以解释为X具有不确定性的大小，也可以解释为当观测到X以后所接受的平均信息量。

##第十章 模拟

1. 设F是一个连续的分布函数，U是(0,1)上的均匀分布的随机变量(称为随机数)则`\(F^{-1}(U)\)`具有分布F，其中`\(F^{-1}(u)\)`是方程F(x)=u的解，这种由随机数构造其他随机变量的方法称为`反变换方法`。
2. 另一个产生随机变量的方法称为`取舍法`。假定对于密度g，我们已经有一个产生随机变量的成熟方法，现在希望模拟一个具有密度f的随机变量。我们首先确定一个常数c，它满足`\(max\frac{f(x)}{g(x)} \le c\)`，然后经过下列步骤:
    * 产生Y，其密度为g
    * 产生随机数U
    * 若`\(U \le \frac{f(Y)}{[cg(Y)]}\)`，则令X=Y，过程中止
    * 回到第一步。此方法循环的次数具有几何分布，其平均值为c
    * 标准正态变量可通过取舍法产生(g为指数密度，期望为1)或者利用极坐标方法产生
3. 为了估计某一个参数`\(\theta\)`，首先模拟一个随机变量，使得它的期望值为`\(\theta\)`，然后，利用统计方法缩减其相应的方差。本文中介绍了三种缩减方差的方法:
    * 利用对偶变量
    * 利用条件期望
    * 利用控制变量