---
layout: post
title: "SCOPE: Easy and Efficient Parallel Processing of Massive Data Sets"
description: ""
category: tech
tags: [scope, dryad]
---
{% include JB/setup %}
paper review:[SCOPE: Easy and Efficient Parallel Processing of Massive Data Sets](http://research.microsoft.com/en-us/um/people/jrzhou/pub/Scope.pdf)

<!--break-->
##2. PLATFORMOVERVIEW

![scope1](/assets/2013-08-22-scope/scope1.png)

Figure 1 shows the main components of the Cosmos platform.

* **Cosmos storage**: A distributed storage subsystem designed to reliably and efficiently store extremely large sequential files.
* **Cosmos execution environment**: An environment for deploying, executing, and debugging distributed applications.
* **SCOPE**: A high-level scripting language for writing data analysis jobs. The SCOPE compiler and optimizer translate scripts to efficient parallel execution plans.

The Cosmos Storage System is an append-only file system that reliably stores petabytes of data. The system is optimized for large sequential I/O. 

The lowest level primitives of the Cosmos execution environment provide only the ability to run arbitrary executable code on a server. Clients upload application code and resources onto the system via a Cosmos execution protocol. A recipient server assigns the task a priority and executes it at an appropriate time.

In Cosmos, applications are programmed against the execution engine that provides a higher-level programming interface and a runtime system that automatically handles the details of optimization, fault tolerance, data partitioning, resource management, and parallelism.

An application is modeled as a dataflow graph: a directed acyclic graph (DAG) with vertices representing processes and edges representing data flows. The runtime component of the execution engine is called the **Job Manager**. The JM is the central and coordinating process for all processing vertices within an application. The primary function of the JM is to construct the runtime DAG from the compile time representation of a DAG and execute over it. The JM schedules a DAG vertex onto the system processing nodes when all the inputs are ready, monitors progress, and, on failure, re-executes part of the DAG.

`Dryad` implements a job manager and a graph building language for composing vertices of computation and edges of communication channels between the vertices.

##3. SCOPE Scripting Language
The SCOPE scripting language resembles SQL but with C# expressions. A script writer can view operators as being entirely serial; mapping the script to an efficient parallel execution plan is handled completely by the SCOPE compiler and optimizer.

* Input and Output
  * Input data for a SCOPE script is obtained by means of built-in or user-written extractors.
  * Similarly, the output command is used to write data to a Cosmos file, a regular file, or any other data sink.
* Select and Join
* Expressions and Functions
* User-Defined Operators
  * SCOPE provides three highly extensible commands that manipulate rowsets: PROCESS, REDUCE and COMBINE. Users can write customized operations by extending built-in C# components.
* Importing Scripts
  * The import command reads in the contents of the named script file (at compile time). In the process, parameter references are replaced by the values provided.

##4. SCOPE Execution

####4.1 SCOPE Compilation
The SCOPE compiler parses the script, checks the syntax, and resolves names. It tracks all column definitions and renaming. For each command in the script, the compiler checks that all the columns have been properly defined by the inputs. The result of the compilation is an internal parse tree. SCOPE has an option to translate the parsed tree directly to a physical execution plan using default plans for each command.

A physical execution plan is, in essence, a specification of Cosmos job. The job describes a data flow DAG where each vertex is a program and each edge represents a data channel. A vertex program is a serial program composed from SCOPE runtime physical operators, which may in turn call user-defined functions. All operators within a vertex program are executed in a pipelined fashion.

The job manager constructs the specified graph and schedules the execution. A vertex becomes runnable when its inputs are ready. The execution environment keeps track of the state of vertices and channels, schedules runnable vertices for execution, decides where to run a vertex, sets up the resources needed to run a vertex, and finally starts the vertex program.

####4.2 SCOPE Optimization
The SCOPE optimizer is a transformation-based optimizer based on the Cascades framework. Conceptually, the optimizer generates all possible rewritings of a query expression and chooses the one with the lowest estimated cost. Rewritings are generated by applying local transformation rules on query subexpressions, producing substitute expressions logically equivalent to the original subexpression.

####4.3 Example Query Plan

    SELECT query, COUNT() AS count
    FROM "search.log" USING LogExtractor
    GROUP BY query
    HAVING count > 1000
    ORDER BY count DESC;
    OUTPUT TO "qcount.result";

* Extract: The input file consists of multiple file extents, distributed across many machines in the cluster. Multiple extractors run in parallel, each one reading part of the file.
* Partial aggregation: In this stage, partial aggregation is applied at the rack level. That is, data from extractors running on machines within the same rack is pre-aggregated to reduce data volume. This exploits knowledge about network topology of the cluster. Partial aggregation can be done either using sorting or hashing and, in this case, it can be applied multiple times, either on a single extent or on groups of extents.
* Distribute: The result from the previous stage is partitioned on the grouping column “query”. This brings all (partially aggregated) rows with the same query string into the same partition.
* Full aggregation: Each partition can safely calculate the final aggregation in parallel, again either by sorting or hashing.
* Filter: The fully aggregated rows are then filtered in parallel and any row with a count less than 1000 is discarded.
* Sort: The remaining rows are sorted by count in parallel.
* Merge: The sorted results from all partitions are merged together on a single machine, producing the final result.
* Output: The final result is output as a Cosmos file.

![scope2](/assets/2013-08-22-scope/scope2.png)

####4.4 Runtime Optimization
As mentioned earlier, partial aggregation in Stage 3 can be applied multiple times at different levels without changing the correctness of the program. Given that partial aggregation reduces the input data size, it makes sense to aggregate the inputs within the same rack before sending them out, thereby reducing the overall network traffic between racks. The scheduler also has grouping heuristics to ensure that each vertex has not more than a set number of inputs, or a set volume of input data, in order to avoid overloading the I/O system or the vertex.