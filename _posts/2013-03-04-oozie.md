---
layout: post
title: "Apache Oozie"
description: ""
category: tech
tags: [Hadoop, Ooize]
---
{% include JB/setup %}

##Apache Hadoop-Related Projects List

- [Ambari][1] : Deployment, configuration and monitoring, see [part1][20]
- [Flume][2]:Collection and import of log and event data, see [part1][20]
- [MapReduce][4]: Parallel computation on server clusters, see [part1][20]
- [HDFS][5] Distributed redundant filesystem for Hadoop, see [part1][20]
- [HBase][3]:Column-oriented database scaling to billions of rows, see [part2][21]
- [Zookeeper][6]:Configuration management and coordination, see [part3][22]
- [Pig][7]:High-level programming language for Hadoop computations, see [part4][23]
- [Hive][8]: Data warehouse with SQL-like access, see [part7][25]
- [Oozie][9]: Orchestration and workflow management
- [Sqoop][10]: Imports data from relational databases, see [part7][25]
- [HCatalog][11]: Schema and data type sharing over Pig, Hive and MapReduce, see [part8][26]
- [Whirr][12]: Cloud-agnostic deployment of clusters, see [part8][26]
- [Mahout][13]: Library of machine learning and data mining algorithms, see [part8][26]

<!--break-->

## 1 What Is Oozie ?
Oozie is a workﬂow	scheduler for Hadoop. 

* Allows a user to create Directed Acyclic Graphs of workﬂows and these can be ran in parallel and sequential in Hadoop. 
* Run plain	java classes, Pig workﬂows,	and	interact with the HDFS (Nice if you need to delete or move ﬁles before	a job runs). 
* Run job’s sequentially (one after the other) and in parallel (multiple at a time).

__Other	Features__  

* Java Client API/Command Line	Interface
  * Launch,control,and monitor jobs from your Java Apps
* Web Service API	
  * You can control jobs from anywhere	
* Run Periodic jobs	
  * Have jobs that you need to run every hour, day, week
* Receive an email when a job is complete

## 2 Oozie Nodes
Use `Node` to support Directed Acyclical Graph (DAG) of actions, Now supported actions:

    Map-Reduce action
    Pig action
    Java action
    FS (HDFS) action
    Email action
    Shell action
    Hive action
    Sqoop action
    Sub-workflow action
    Writing a custom action

* Start	Node
  * Tells the application where to start

        <start to=“[NODE-NAME]”/>

* End Node
  * Signals the end of a Oozie Job

        <end name=“[NODE-NAME]”/>

* Error	Node
  * Signals that an error occurred and a message	describing the error should be printed out
  
        <error name=“[NODE-NAME]”/>
          <message>“[A custom message]”</message>
        </error>

* Action Nodes
  * Action Nodes specify the Map/Reduce, Pig, or java class to run
  * Basic flow control: 

        <ok to="…"/>, <error to="..."/>  
    ![okerr](/assets/2013-03-04-oozie/okerror.png)
  * More flow control nodes:
  
        <fork ...>, <join ...>, <decision ...>  
    ![okerr](/assets/2013-03-04-oozie/flowctl.png)
  * Map-Reduce Node
  
        <action name=“[NODE-NAME]”>
        <map-reduce>
            <job-tracker>[JOB-TRACKER ADDRESS]</job-tracker>
            <name-node>[NAME-NODE	ADDRESS]</name-node>
            <conﬁguration>
                [YOUR HADOOP CONFIGURATION]
            </conﬁguration>	
        </map-reduce>
        <ok to=“[NODE-NAME]” />	
        <error to=“[NODE-NAME]”	/>
        </action>
  * Java Node
  
        <action name=“[NODE-NAME]”>
        <java>
            <job-tracker>[JOB-TRACKER ADDRESS]</job-tracker>
            <name-node>[NAME-NODE ADDRESS]</name-node>
            <conﬁguration>
                [OTHER HADOOP CONFIGURATION ITEMS]
            </conﬁguration>	
            <main-class>[MAIN-CLASS PATH]</main-class>
            <java-opts>[ANY	–D JAVA ARGUMENTS]</java-opts>
            <arg>[COMMAND LINE ARGUMENTS]</arg>
        </java>	
        <ok	to=“[NODE-NAME]”/>
        <error to=“[NODE-NAME]"/>
        </action>
  * FS Node
  
        <action name=“[NODE-NAME]”>
        <fs>
            <delete	path=‘[PATH]’/>
            <mkdir path=‘[PATH]’/>
            <move source=‘[PATH]’ target=‘[PATH]’/>
            <chmod path=‘[PATH]’ permissions=‘[PERMISSIONS]’dir-ﬁle=‘false/true’/>
        </fs>
        <ok	to=“[NODE-NAME]”/>	
        <error to=“[NODE-NAME]”/>
        </action>
   * Sub-workflow Node
   
         <action name=“[NODE-NAME]”>
         <sub-workﬂow>
             <app-path>[CHILD-WORKFLOW-PATH]</app-path>
             <conﬁguration>
                 [Propagated conﬁguration]
             </conﬁguration>
         </sub-workﬂow>
         <ok to=“[NODE-NAME]”/>	
         <error to=“[NODE-NAME]”/>
         </action>
    * Fork/Join Node
      * Fork: Starts the parallel jobs
         
            <fork>
                <path start=“ﬁrstjob”>
                [OTHER JOBS]
            </fork>	
      * Join: Parallel jobs re-join at this node. All forked jobs must be completed to continue the workﬂow

            <join name=“[NAME JOBS]” to=“[NEXT-NODE]”/>  
   * Decision Node
     * Decision nodes are a	 switch statements that will run diﬀerent jobs based on the	outcome	of an expression

            <decision name=“[NODE-NAME]”>
                <switch>
                <case to=“singlethreadedJob”>
                    ${fs:ﬁleSize(lastJob) lt 1 *GB}
                </case>	
                <case to=“MRJob”>	
                    ${fs:ﬁleSize(lastJob) ge 1 *GB}
                </case>	
                </switch>
            </decision>	

## 3 Coordinator
Executes a workﬂow regularly when data sets are available.

_Example 1._ This coordinator works like a crontab runs the workflow every 60 mins.  
![cron1](/assets/2013-03-04-oozie/cron1.png)

_Example 2._  
![cron2](/assets/2013-03-04-oozie/cron2.png)

## 4 Parameterization
Parameterization helps make ﬂexible code. Items like job-trackers, name-nodes, input paths, output table, table names, other constants should be in the job.properties ﬁle.

If you want to use a parameter just put it in a ${}.

_Parameterization Example_

    <action	name=“[NODE-NAME]”>	
    <map-reduce>
        <job-tracker>${JOBTRACKER}</job-tracker>
        <name-node>${NAMENODE}</name-node>	
        <conﬁguration>
            <property>
                <name>mapred.input.dir</name>
                <value>${INPUTDIR}</value>
            </property>	
            <property>
                <name>mapred.output.dir</name>	
                <value>${OUTPUTDIR}</value>
            </property>	
            [OTHER	HADOOP	CONFIGURATION	PARAMETERS]
        </conﬁguration>
    </map-reduce>
    <ok to=“[NODE-NAME]”/>
    <error to=“[NODE-NAME]”	/>
    </action>


[1]:http://incubator.apache.org/ambari/ "Apache Ambari"
[2]:http://flume.apache.org/ "Apache Flume"
[3]:http://hbase.apache.org/ "Apache Hbase"
[4]:http://wiki.apache.org/hadoop/MapReduce "Apache MapReduce"
[5]:http://hadoop.apache.org/docs/r1.1.1/hdfs_desig5.html "HDFS Architecture Guide"
[6]:http://zookeeper.apache.org/ "Apache Zookeeper"
[7]:http://pig.apache.org/ "Apache Pig"
[8]:http://hive.apache.org/ "Apache Hive"
[9]:http://oozie.apache.org/ "Apache Oozie"
[10]:http://sqoop.apache.org/ "Apache Sqoop"
[11]:http://incubator.apache.org/hcatalog/ "Apache Hcatalog"
[12]:http://whirr.apache.org/ "Apache whirr"
[13]:http://mahout.apache.org/ "Apache Mahout"

[20]:http://zhangjunhd.github.com/2013/02/24/apache-related-projects/
[21]:http://zhangjunhd.github.com/2013/02/25/apache-hbase/
[22]:http://zhangjunhd.github.com/2013/03/01/zookeeper/
[23]:http://zhangjunhd.github.com/2013/03/03/pig/
[24]:http://zhangjunhd.github.com/2013/03/04/oozie/
[25]:http://zhangjunhd.github.com/2013/03/04/hive/
[26]:http://zhangjunhd.github.com/2013/03/06/apache-related-projects2/
