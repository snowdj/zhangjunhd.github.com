---
layout: post
title: "Spanner"
description: ""
category: 云计算
tags: [Spanner, Paxos]
---
{% include JB/setup %}
paper review:[Spanner: Google’s Globally-Distributed Database](http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/zh-CN//pubs/archive/39966.pdf)

<!--break-->
##2 Implementation

![1](/assets/2013-08-03-spanner/1.png)

A Spanner deployment is called a `universe`. 

Spanner is organized as a set of `zones`, where each zone is the rough analog of a deployment of Bigtable servers. Zones are the unit of administrative deployment. The set of zones is also the set of locations across which data can be replicated. Zones can be added to or removed from a running system as new datacenters are brought into service and old ones are turned off, respectively. Zones are also the unit of physical isolation: there may be one or more zones in a datacenter, for example, if different applications’ data must be partitioned across different sets of servers in the same datacenter.

A zone has one `zonemaster` and between one hundred and several thousand `spanservers`. The former assigns data to spanservers; the latter serve data to clients. The per-zone `location proxies` are used by clients to locate the spanservers assigned to serve their data. The `universe master` and the `placement driver` are currently singletons. The universe master is primarily a console that displays status information about all the zones for interactive debugging. The placement driver handles automated movement of data across zones on the timescale of minutes. The placement driver periodically communicates with the spanservers to find data that needs to be moved, either to meet updated replication constraints or to balance load. 

####2.1 Spanserver Software Stack

![2](/assets/2013-08-03-spanner/2.png)

At the bottom, each spanserver is responsible for between 100 and 1000 instances of a data structure called a `tablet`. A tablet is similar to Bigtable’s tablet abstraction, in that it implements a bag of the following mappings:

    (key:string, timestamp:int64) → string

To support replication, each spanserver implements a single Paxos state machine on top of each tablet. Each state machine stores its metadata and log in its corresponding tablet. Our Paxos implementation supports long-lived leaders with time-based leader leases, whose length defaults to 10 seconds.

The Paxos state machines are used to implement a consistently replicated bag of mappings. The key-value mapping state of each replica is stored in its corresponding tablet. Writes must initiate the Paxos protocol at the leader; reads access state directly from the underlying tablet at any replica that is sufficiently up-to-date. The set of replicas is collectively a `Paxos group`.

At every replica that is a leader, each spanserver implements a `lock table` to implement concurrency control. The lock table contains the state for two-phase locking: it maps ranges of keys to lock states. (Note that having a long-lived Paxos leader is critical to efficiently managing the lock table.) In both Bigtable and Spanner, we designed for long-lived transactions (for example, for report generation, which might take on the order of minutes), which perform poorly under optimistic concurrency control in the presence of conflicts. Operations that require synchronization, such as transactional reads, acquire locks in the lock table; other operations bypass the lock table.

At every replica that is a leader, each spanserver also implements a `transaction manager` to support distributed transactions. The transaction manager is used to implement a `participant leader`; the other replicas in the group will be referred to as `participant slaves`. If a transaction involves only one Paxos group (as is the case for most transactions), it can bypass the transaction manager, since the lock table and Paxos together provide transactionality. If a transaction involves more than one Paxos group, those groups’ leaders coordinate to perform twophase commit. One of the participant groups is chosen as the coordinator: the participant leader of that group will be referred to as the `coordinator leader`, and the slaves of that group as `coordinator slaves`. The state of each transaction manager is stored in the underlying Paxos group (and therefore is replicated).

####2.2 Directories and Placement
On top of the bag of key-value mappings, the Spanner implementation supports a bucketing abstraction called a `directory`, which is a set of contiguous keys that share a common prefix.

A directory is the unit of data placement. All data in a directory has the same replication configuration. When data is moved between Paxos groups, it is moved directory by directory, as shown in Figure 3. Spanner might move a directory to shed load from a Paxos group; to put directories that are frequently accessed together into the same group; or to move a directory into a group that is closer to its accessors. Directories can be moved while client operations are ongoing.

![3](/assets/2013-08-03-spanner/3.png)

The fact that a Paxos group may contain multiple directories implies that a Spanner tablet is different from a Bigtable tablet: the former is not necessarily a single lexicographically contiguous partition of the row space. Instead, a Spanner tablet is a container that may encapsulate multiple partitions of the row space. We made this decision so that it would be possible to colocate multiple directories that are frequently accessed together.

`Movedir` is the background task used to move directories between Paxos groups. Movedir is also used to add or remove replicas to Paxos groups, because Spanner does not yet support in-Paxos configuration changes. Movedir is not implemented as a single transaction, so as to avoid blocking ongoing reads and writes on a bulky data move. Instead, movedir registers the fact that it is starting to move data and moves the data in the background. When it has moved all but a nominal amount of the data, it uses a transaction to atomically move that nominal amount and update the metadata for the two Paxos groups.

A directory is also the smallest unit whose geographic-replication properties (or placement, for short) can be specified by an application. The design of our placement-specification language separates responsibilities for managing replication configurations. Administrators control two dimensions: the number and types of replicas, and the geographic placement of those replicas. They create a menu of named options in these two dimensions (e.g., North America, replicated 5 ways with 1 witness). An application controls how data is replicated, by tagging each database and/or individual directories with a combination of those options.

For expository clarity we have over-simplified. In fact, Spanner will shard a directory into multiple `fragments` if it grows too large. Fragments may be served from different Paxos groups (and therefore different servers). Movedir actually moves fragments, and not whole directories, between groups.

####2.3 Data Model
Spanner exposes the following set of data features to applications: a data model based on schematized semi-relational tables, a query language, and general-purpose transactions.

An application creates one or more `databases` in a universe. Each database can contain an unlimited number of schematized `tables`.

Spanner’s data model is not purely relational, in that rows must have names. More precisely, every table is required to have an ordered set of one or more primary-key columns. This requirement is where Spanner still looks like a key-value store: the primary keys form the name for a row, and each table defines a mapping from the primary-key columns to the non-primary-key columns. A row has existence only if some value (even if it is NULL) is defined for the row’s keys. **Imposing this structure is useful because it lets applications control data locality through their choices of keys.**

Figure 4 contains an example Spanner schema for storing photo metadata on a per-user, per-album basis. The schema language is similar to Megastore’s, with the additional requirement that every Spanner database must be partitioned by clients into one or more hierarchies of tables. Client applications declare the hierarchies in database schemas via the `INTERLEAVE IN` declarations. The table at the top of a hierarchy is a `directory table`. Each row in a directory table with key K, together with all of the rows in descendant tables that start with K in lexicographic order, forms a directory. `ON DELETE CASCADE` says that deleting a row in the directory table deletes any associated child rows. The figure also illustrates the interleaved layout for the example database: for example, Albums(2,1) represents the row from the Albums table for user id 2, album id 1. This interleaving of tables to form directories is significant because it allows clients to describe the locality relation- ships that exist between multiple tables, which is necessary for good performance in a sharded, distributed database. Without it, Spanner would not know the most important locality relationships.

![4](/assets/2013-08-03-spanner/4.png)

##3 TrueTime

![5](/assets/2013-08-03-spanner/5.png)

TrueTime explicitly represents time as a `TTinterval`, which is an interval with bounded time uncertainty (unlike standard time interfaces that give clients no notion of uncertainty). The endpoints of a TTinterval are of type `TTstamp`. The `TT.now()` method returns a TTinterval that is guaranteed to contain the absolute time during which TT.now() was invoked. The time epoch is analogous to UNIX time with leap-second smearing. Define the instantaneous error bound as ε, which is half of the interval’s width, and the average error bound as ε. The `TT.after()` and `TT.before()` methods are convenience wrappers around TT.now().

Denote the absolute time of an event e by the function `\(t_{abs}(e)\)`. In more formal terms, TrueTime guarantees that for an invocation tt = TT.now(), tt.earliest ≤ `\(t_{abs}(e_{now})\)` ≤ tt.latest, where `\(e_{now}\)` is the invocation event.

TrueTime is implemented by a set of `time master` machines per datacenter and a timeslave daemon per machine. The majority of masters have `GPS receivers` with dedicated antennas; these masters are separated physically to reduce the effects of antenna failures, radio interference, and spoofing. The remaining masters (which we refer to as `Armageddon masters`) are equipped with atomic clocks. An atomic clock is not that expensive: the cost of an Armageddon master is of the same order as that of a GPS master. All masters’ time references are regularly compared against each other. Each master also cross-checks the rate at which its reference advances time against its own local clock, and evicts itself if there is substantial divergence. Between synchronizations, Armageddon masters advertise a slowly increasing time uncertainty that is derived from conservatively applied worst-case clock drift. GPS masters advertise uncertainty that is typically close to zero.

##4 Concurrency Control
####4.1 Timestamp Management

![6](/assets/2013-08-03-spanner/6.png)

The Spanner implementation supports `read-write transactions`, `read-only transactions` (predeclared snapshot-isolation transactions), and `snapshot reads`. Standalone writes are implemented as read-write transactions; non-snapshot standalone reads are implemented as read-only transactions. Both are internally retried (clients need not write their own retry loops).

Reads in a read-only transaction execute at a system-chosen timestamp without locking, so that incoming writes are not blocked. The execution of the reads in a read-only transaction can proceed on any replica that is sufficiently up-to-date (Section 4.1.3).

A snapshot read is a read in the past that executes without locking. A client can either specify a timestamp for a snapshot read, or provide an upper bound on the desired timestamp’s staleness and let Spanner choose a timestamp. In either case, the execution of a snapshot read proceeds at any replica that is sufficiently up-to-date.

For both read-only transactions and snapshot reads, commit is inevitable once a timestamp has been chosen, unless the data at that timestamp has been garbage-collected. As a result, clients can avoid buffering results inside a retry loop. When a server fails, clients can internally continue the query on a different server by repeating the timestamp and the current read position.

#####4.1.1 Paxos Leader Leases
Spanner’s Paxos implementation uses timed leases to make leadership long-lived (10 seconds by default). A potential leader sends requests for timed `lease votes`; upon receiving a quorum of lease votes the leader knows it has a lease. A replica extends its lease vote implicitly on a successful write, and the leader requests lease-vote extensions if they are near expiration. Define a leader’s `lease interval` as starting when it discovers it has a quorum of lease votes, and as ending when it no longer has a quorum of lease votes (because some have expired). Spanner depends on the following disjointness invariant: **for each Paxos group, each Paxos leader’s lease interval is disjoint from every other leader’s.**

The Spanner implementation permits a Paxos leader to abdicate by releasing its slaves from their lease votes. To preserve the disjointness invariant, Spanner constrains when abdication is permissible. Define smax to be the maximum timestamp used by a leader. Subsequent sections will describe when smax is advanced. Before abdicating, a leader must wait until TT.after(smax) is true.

#####4.1.2 Assigning Timestamps to RW Transactions
Transactional reads and writes use two-phase locking. As a result, they can be assigned timestamps at any time when all locks have been acquired, but before any locks have been released. For a given transaction, Spanner assigns it the timestamp that Paxos assigns to the Paxos write that represents the transaction commit.

Spanner depends on the following monotonicity invariant: within each Paxos group, Spanner assigns timestamps to Paxos writes in monotonically increasing order, even across leaders. A single leader replica can trivially assign timestamps in monotonically increasing order. This invariant is enforced across leaders by making use of the disjointness invariant: **a leader must only assign timestamps within the interval of its leader lease. Note that whenever a timestamp s is assigned, `\(s_{max}\)` is advanced to s to preserve disjointness.**

Spanner also enforces the following external-consistency invariant: if the start of a transaction T2 occurs after the commit of a transaction T1, then the commit timestamp of T2 must be greater than the commit timestamp of T1. Define the start and commit events for a transaction Ti by `\(e_i^{start}\)` and `\(e_i^{commit}\)`; and the commit timestamp of a transaction Ti by si. The invariant becomes `\(t_{abs}(e_1^{commit})\)` < `\(t_{abs}(e_2^{start})\)` ⇒ s1 < s2 . The protocol for executing transactions and assigning timestamps obeys two rules, which together guarantee this invariant, as shown below. Define the arrival event of the commit request at the coordinator leader for a write Ti to be `\(e_i^{server}\)`.

**Start** The coordinator leader for a write Ti assigns a commit timestamp si no less than the value of TT.now().latest, computed after `\(e_i^{server}\)`. Note that the participant leaders do not matter here; Section 4.2.1 describes how they are involved in the implementation of the next rule.

**Commit Wait** The coordinator leader ensures that clients cannot see any data committed by Ti until TT.after(si) is true. Commit wait ensures that si is less than the absolute commit time of Ti, or si < `\(e_i^{commit}\)`. The implementation of commit wait is described in Section 4.2.1. Proof:

>s1 < `\(t_{abs}(e_1^{commit})\)`  //commit wait  
>`\(t_{abs}(e_1^{commit})\)` < `\(t_{abs}(e_2^{start})\)` //assumption  
>`\(t_{abs}(e_2^{start})\)` ≤ `\(t_{abs}(e_2^{server})\)` //causality  
>`\(t_{abs}(e_2^{server})\)` ≤ s2 //start  
>s1 < s2 //transitivity

#####4.1.3 Serving Reads at a Timestamp
The monotonicity invariant described in Section 4.1.2 allows Spanner to correctly determine whether a replica’s state is sufficiently up-to-date to satisfy a read. Every replica tracks a value called safe time `\(t_{safe}\)` which is the maximum timestamp at which a replica is up-to-date. A replica can satisfy a read at a timestamp t if t <= `\(t_{safe}\)`.

Define `\(t_{safe} = min(t_{safe}^{Paxos}, t_{safe}^{™})\)`, where each Paxos state machine has a safe time `\(t_{safe}^{Paxos}\)` and each transaction manager has a safe time `\(t_{safe}^{™}\)` . `\(t_{safe}^{Paxos}\)` is simpler: it is the timestamp of the highest-applied Paxos write. Because timestamps increase monotonically and writes are applied in order, writes will no longer occur at or below `\(t_{safe}^{Paxos}\)` with respect to Paxos.

`\(t_{safe}^{™}\)` is ∞ at a replica if there are zero prepared (but safe not committed) transactions—that is, transactions in between the two phases of two-phase commit. (For a participant slave, `\(t_{safe}^{™}\)` actually refers to the replica’s leader’s transaction manager, whose state the slave can infer through metadata passed on Paxos writes.) If there areany such transactions, then the state affected by those transactions is indeterminate: a participant replica does not know yet whether such transactions will commit. As we discuss in Section 4.2.1, the commit protocol ensures that every participant knows a lower bound on a prepared transaction’s timestamp. Every participant leader (for a group g) for a transaction Ti assigns a prepare timestamp `\(s_{i,g}^{prepare}\)`to its prepare record. The coordinator leader ensures that the transaction’s commit timestamp `\(s_i >= s_{i,g}^{prepare}\)` over all participant groups g. Therefore, for every replica in a group g, over all transactions Ti prepared at g, `\(t_{safe}^{™} = min_i(s_{i,g}^{prepare}) - 1\)` over all transactions prepared at g.
#####4.1.4 Assigning Timestamps to RO Transactions
A read-only transaction executes in two phases: assign a timestamp `\(s_{read}\)`, and then execute the transaction’s reads as snapshot reads at `\(s_{read}\)`. The snapshot reads can execute at any replicas that are sufficiently up-to-date.The simple assignment of `\(s_{read}\)` = TT.now().latest, at any time after a transaction starts, preserves external consistency by an argument analogous to that presented for writes in Section 4.1.2. However, such a timestamp may require the execution of the data reads at sread to block if tsafe has not advanced sufficiently. (In addition, note that choosing a value of `\(s_{read}\)` may also advance `\(s_{max}\)` to preserve disjointness.) To reduce the chances of blocking, Spanner should assign the oldest timestamp that preserves external consistency. Section 4.2.2 explains how such a timestamp can be chosen.
####4.2 Details
#####4.2.1 Read-Write Transactions
Like Bigtable, writes that occur in a transaction are buffered at the client until commit. As a result, reads in a transaction do not see the effects of the transaction’s writes.  

Reads within read-write transactions use `wound-wait` to avoid deadlocks. The client issues reads to the leader replica of the appropriate group, which acquires read locks and then reads the most recent data. While a client transaction remains open, it sends keepalive messages to prevent participant leaders from timing out its transaction. When a client has completed all reads and buffered all writes, it begins two-phase commit. The client chooses a coordinator group and sends a commit message to each participant’s leader with the identity of the coordinator and any buffered writes. Having the client drive two-phase commit avoids sending data twice across wide-area links.

A non-coordinator-participant leader first acquires write locks. It then chooses a prepare timestamp that must be larger than any timestamps it has assigned to previous transactions (to preserve monotonicity), and logs a prepare record through Paxos. Each participant then notifies the coordinator of its prepare timestamp.

The coordinator leader also first acquires write locks, but skips the prepare phase. It chooses a timestamp for the entire transaction after hearing from all other participant leaders. The commit timestamp s must be greater or equal to all prepare timestamps (to satisfy the constraints discussed in Section 4.1.3), greater than TT.now().latest at the time the coordinator received its commit message, and greater than any timestamps the leader has assigned to previous transactions (again, to preserve monotonicity). The coordinator leader then logs a commit record through Paxos (or an abort if it timed out while waiting on the other participants).

Before allowing any coordinator replica to apply the commit record, the coordinator leader waits until TT.after(s), so as to obey the commit-wait rule described in Section 4.1.2. Because the coordinator leader chose s based on TT.now().latest, and now waits until that timestamp is guaranteed to be in the past, the expected wait is at least 2 ∗ ε. This wait is typically overlapped with Paxos communication. After commit wait, the coordinator sends the commit timestamp to the client and all other participant leaders. Each participant leader logs the transaction’s outcome through Paxos. All participants apply at the same timestamp and then release locks.

#####4.2.2 Read-Only Transactions

Assigning a timestamp requires a negotiation phase between all of the Paxos groups that are involved in the reads. As a result, Spanner requires a `scope` expression for every read-only transaction, which is an expression that summarizes the keys that will be read by the entire transaction. Spanner automatically infers the scope for standalone queries.


If the scope’s values are served by a single Paxos group, then the client issues the read-only transaction to that group’s leader. (The current Spanner implementation only chooses a timestamp for a read-only transaction at a Paxos leader.) That leader assigns `\(s_{read}\)` and executes the read. For a single-site read, Spanner generally does better than TT.now().latest. Define LastTS() to be the timestamp of the last committed write at a Paxos group. If there are no prepared transactions, the assignment `\(s_{read} = LastTS()\)` trivially satisfies external consistency: the transaction will see the result of the last write, and therefore be ordered after it.


If the scope’s values are served by multiple Paxos groups, there are several options. The most complicated option is to do a round of communication with all of the groups’s leaders to negotiate `\(s_{read}\)` based on LastTS(). Spanner currently implements a simpler choice. The client avoids a negotiation round, and just has its reads execute at `\(s_{read} = TT.now().latest\)` (which may wait for safe time to advance). All reads in the transaction can be sent to replicas that are sufficiently up-to-date.

#####4.2.3 Schema-Change Transactions

TrueTime enables Spanner to support atomic schema changes. It would be infeasible to use a standard transaction, because the number of participants (the number of groups in a database) could be in the millions. Bigtable supports atomic schema changes in one datacenter, but its schema changes block all operations.

A Spanner schema-change transaction is a generally non-blocking variant of a standard transaction. First, it is explicitly assigned a timestamp in the future, which is registered in the prepare phase. As a result, schema changes across thousands of servers can complete with minimal disruption to other concurrent activity. Second, reads and writes, which implicitly depend on the schema, synchronize with any registered schema-change timestamp at time t: they may proceed if their timestamps precede t, but they must block behind the schema-change transaction if their timestamps are after t. Without TrueTime, defining the schema change to happen at t would be meaningless.

#####4.2.4 Refinements

`\(t_{safe}^{™}\)` as defined above has a weakness, in that a single prepared transaction prevents `\(t_{safe}\)` from advancing. As a result, no reads can occur at later timestamps, even if the reads do not conflict with the transaction. Such false conflicts can be removed by augmenting `\(t_{safe}^{™}\)` with a fine-grained mapping from key ranges to prepared-transaction timestamps. This information can be stored in the lock table, which already maps key ranges to lock metadata. When a read arrives, it only needs to be checked against the fine-grained safe time for key ranges with which the read conflicts.

LastTS() as defined above has a similar weakness: if a transaction has just committed, a non-conflicting read-only transaction must still be assigned `\(s_{read}\)` so as to follow that transaction. As a result, the execution of the read could be delayed. This weakness can be remedied similarly by augmenting LastTS() with a fine-grained mapping from key ranges to commit timestamps in the lock table. (We have not yet implemented this optimization.) When a read-only transaction arrives, its timestamp can be assigned by taking the maximum value of LastTS() for the key ranges with which the transaction conflicts, unless there is a conflicting prepared transaction (which can be determined from fine-grained safe time).

`\(t_{safe}^{Paxos}\)` as defined above has a weakness in that it cannot advance in the absence of Paxos writes. That is, a snapshot read at t cannot execute at Paxos groups whose last write happened before t. Spanner addresses this problem by taking advantage of the disjointness of leader-lease intervals. Each Paxos leader advances `\(t_{safe}^{Paxos}\)` by keeping a threshold above which future writes’ timestamps will occur: it maintains a mapping MinNextTS(n) from Paxos sequence number n to the minimum timestamp that may be assigned to Paxos sequence number n + 1. A replica can advance `\(t_{safe}^{Paxos}\)` to MinNextTS(n) − 1 when it has applied through n.

A single leader can enforce its MinNextTS() promises easily. Because the timestamps promised by MinNextTS() lie within a leader’s lease, the disjointness invariant enforces MinNextTS() promises across leaders. If a leader wishes to advance MinNextTS() beyond the end of its leader lease, it must first extend its lease. Note that smax is always advanced to the highest value in MinNextTS() to preserve disjointness.

A leader by default advances MinNextTS() values every 8 seconds. Thus, in the absence of prepared transactions, healthy slaves in an idle Paxos group can serve reads at timestamps greater than 8 seconds old in the worst case. A leader may also advance MinNextTS() values on demand from slaves.