---
layout: post
title: "Apache Pig"
description: ""
category: 云计算
tags: [Hadoop, Pig]
---
{% include JB/setup %}

- [Ambari][1] : Deployment, configuration and monitoring, see [part1][20]
- [Flume][2]:Collection and import of log and event data, see [part1][20]
- [MapReduce][4]: Parallel computation on server clusters, see [part1][20]
- [HDFS][5] Distributed redundant filesystem for Hadoop, see [part1][20]
- [HBase][3]:Column-oriented database scaling to billions of rows, see [part2][21]
- [Zookeeper][6]:Configuration management and coordination, see [part3][22]
- [Pig][7]:High-level programming language for Hadoop computations
- [Hive][8]: Data warehouse with SQL-like access, see [part7][25]
- [Oozie][9]: Orchestration and workflow management, see [part6][24]
- [Sqoop][10]: Imports data from relational databases, see [part7][25]
- [HCatalog][11]: Schema and data type sharing over Pig, Hive and MapReduce, see [part8][26]
- [Whirr][12]: Cloud-agnostic deployment of clusters, see [part8][26]
- [Mahout][13]: Library of machine learning and data mining algorithms, see [part8][26]

<!--break-->

## 1 INTRODUCTION
_Example 1._ Suppose we have a table urls: (url, category, pagerank). The following is a simple SQL query that finds, for each sufficiently large category, the average pagerank of high-pagerank urls in that category.  

_SQL:_

    SELECT category, AVG(pagerank)
    FROM urls WHERE pagerank > 0.2
    GROUP BY category HAVING COUNT(*) > 106

_PIG Latin:_

    good_urls = FILTER urls BY pagerank > 0.2;
    groups = GROUP good_urls BY category;
    big_groups = FILTER groups BY COUNT(good_urls)>106; 
    output = FOREACH big_groups GENERATE category, AVG(good_urls.pagerank);

## 2 FEATURES AND MOTIVATION
__2.1 Dataflow Language__  
_"The step-by-step method of creating a program in Pig [Latin] is much cleaner and simpler to use than the single block method of SQL."_

__2.2 Data Format__  
In Pig, stored schemas are strictly __optional__. Users may supply schema information on the fly, or perhaps not at all. Thus, in Example 1, if the user knows the the third field of the file that stores the urls table is pagerank but does not want to provide the schema, the first line of the Pig Latin program can be written as:

    good_urls = FILTER urls BY $2 > 0.2;

where __$2__ uses positional notation to refer to the third field.

__2.3 Nested Data Model__  
Pig Latin has a flexible, fully nested data model (described in Section 3.1), and allows complex, non-atomic data types such as set, map, and tuple to occur as fields of a table. 

__2.4 UDFs as First-Class Citizens__  
_Example 2._ Continuing with the setting of Example 1, suppose we want to find for each category, the top 10 urls according to pagerank. In Pig Latin, one can simply write:

    groups = GROUP urls BY category;
    output = FOREACH groups GENERATE category, top10(urls);

where _top10()_ is a UDF that accepts a set of urls (for each group at a time), and outputs a set containing the top 10 urls by pagerank for that group.

Another example of `UDF`:

    exp_q = FOREACH queries GENERATE myudfs.UPPER(qString);
![udf](/assets/2013-03-03-pig/udf.png)

## 3 PIG LATIN
__3.1 Data Model__  
Pig has a rich, yet simple data model consisting of the following four types:

* _Atom_: An atom contains a simple atomic value such as a string or anumber, e.g.,  
`‘alice’`
* _Tuple_: A tuple is a sequence of fields, each of which can be any of thedata types, e.g.,  
`(‘alice’, ‘lakers’)`
* _Bag_: A bag is a collection of tuples with possible duplicates. The schema of the constituent tuples is flexible, i.e., not all tuples in a bag need to have the same number and type of fields, e.g.,`{(‘alice’, ‘lakers’), (‘alice’, (‘ipod’, ‘apple’))`
* _Map_: A map is a collection of data items, where each item has an associated key through which it can be looked up. As with bags, the schema of the constituent data items is flexible, i.e., all the data items in the map need not be of the same type. However, the keys are requested to be data atoms, mainly for efficiency of lookups. Example   
`[‘fan of’ -> {(‘alice’), (‘lakers’)}, ‘age’ -> 20]`

Table 1 shows the expression types in Pig Latin, and how they operate.  
![exp](/assets/2013-03-03-pig/exp.png)

__3.2 Specifying Input Data:LOAD__  
An input file is assumed to contain a sequence of tuples, i.e., a bag. This step is carried out by the `LOAD` command. For example,
    
    queries = LOAD ‘query_log.txt’ USING myLoad() AS (userId, queryString, timestamp);

__3.3 Per-tuple Processing:FOREACH__  
Once input data file(s) have been specified through `LOAD`, one can specify the processing that needs to be carried out on the data.This is achieved through the `FOREACH` command. For example,

    expanded_queries = FOREACH queries GENERATE userId, expandQuery(queryString);

The first field of the output tuple is the userId field of the input tuple, and the second field of the output tuple is the result of applying the `UDF` expandQuery to the queryString field of the input tuple.

Then an example transformation carried out by the above statement is as shown in the first step of Figure 1.  
![foreach](/assets/2013-03-03-pig/foreach.png)

Nesting can be eliminated by the use of the `FLATTEN` keyword in the `GENERATE` clause. Flattening operates on bags by extracting the fields of the tuples in the bag, and making them fields of the tuple being output by `GENERATE`, thus removing one level of nesting. For example, the output of the following command is shown as the second step in Figure 1.
    
    expanded_queries = FOREACH queries GENERATE userId,
                       FLATTEN(expandQuery(queryString));

__3.4 Discarding Unwanted Data:FILTER__  
For example, to get rid of bot traffic in the bag queries:

    eal_queries = FILTER queries BY userId neq ‘bot’;

Filtering conditions in Pig Latin can involve a combination of expressions (Table 1), comparison operators such as ==, eq, !=, neq, and the logical connectors AND, OR, and NOT. Since arbitrary expressions are allowed, it follows that we can use `UDF`s while filtering. 

    real_queries = FILTER queries BY NOT isBot(userId);

__3.5 Getting Related Data Together:COGROUP__  
For example, suppose we have two data sets that we have specified through a `LOAD` command:

    results:  (queryString, url, position)
    revenue:  (queryString, adSlot, amount)

   * __results__ contains, for different query strings, the urls shown as search results, and the position at which they were shown.
   * __revenue__ contains, for different query strings, and different advertisement slots, the average amount of revenue made by the advertisements for that query string at that slot.

Then to group together all search result data and revenue data for the same query string, we can write:

    grouped_data = COGROUP results BY queryString,
                           revenue BY queryString;

Figure 2 shows what a tuple in grouped_data looks like. Figure 2 also shows the result of joining our data sets on queryString. It is evident that `JOIN` is equivalent to `COGROUP`, followed by taking a cross product of the tuples in the nested bags.  
![join](/assets/2013-03-03-pig/join.png)

While joins are widely applicable, certain custom processing might require access to the tuples of the groups before the cross-product is taken, as shown by the following example.  

_Example 3._ Suppose we were trying to attribute search revenue to search-result urls to figure out the monetary worth of each url. To accomplish this task in Pig Latin, we can follow the `COGROUP` with the following statement:

    url_revenues = FOREACH grouped_data GENERATE
                   FLATTEN(distributeRevenue(results, revenue));

where _distributeRevenue_ is a `UDF` that accepts search results and revenue information for a query string at a time, and outputs a bag of urls and the revenue attributed to them. For example, _distributeRevenue_ might attribute revenue from the top slot entirely to the first search result, while the revenue from the side slot may be attributed equally to all the results. In this case, the output of the above statement for our example data is shown in Figure 2.

__3.5.1 Special Case of COGROUP: GROUP__  
A common special case of `COGROUP` is when there is only one data set involved. In this case, we can use the alternative, more intuitive keyword `GROUP`.

    grouped_revenue = GROUP revenue BY queryString;
    query_revenues = FOREACH grouped_revenue GENERATE queryString, 
                     SUM(revenue.amount) AS totalRevenue;

__3.5.2 JOIN in Pig Latin__  
For example,

    join_result = JOIN results BY queryString, revenue BY queryString;

It is easy to verify that `JOIN` is only a syntactic shortcut for `COGROUP` followed by flattening. The above join command is equivalent to:

    temp_var = COGROUP results BY queryString,revenue BY queryString;
    join_result = FOREACH temp_var GENERATE FLATTEN(results), FLATTEN(revenue);

__3.5.3 Map-Reduce in Pig Latin__  
With the `GROUP` and `FOREACH` statements, it is trivial to express a map-reduce program in Pig Latin.

    map_result = FOREACH input GENERATE FLATTEN(map(*)); 
    key_groups = GROUP map_result BY $0;
    output = FOREACH key_groups GENERATE reduce(*);

__3.6 Other Commands__

   * __UNION__:Returns the union of two or more bags.
   * __CROSS__:Returns the cross product of two or more bags.
   * __ORDER__:Orders a bag by the specified field(s).
   * __DISTINCT__:Eliminates duplicate tuples in a bag. This command is just a shortcut for grouping the bag by all fields, and then projecting out the groups.

__3.7 Nested Operations__  
When we have nested bags within tuples, either as a result of (co)grouping, or due to the base data being nested, we might want to harness the same power of Pig Latin to process even these nested bags. To allow such processing, Pig Latin allows some commands to be nested within a FOREACH command.

For example, continuing with the data set of Section 3.5, suppose we wanted to compute for each _queryString_, the total revenue due to the ‘top’ ad slot, and also the overall total revenue. This can be written in Pig Latin as follows:

    grouped_revenue = GROUP revenue BY queryString;
    query_revenues = FOREACH grouped_revenue{
        top_slot = FILTER revenue BY adSlot eq ‘top’;
                    GENERATE queryString,
                    SUM(top_slot.amount),
                    SUM(revenue.amount);
    };

__3.8 Asking for Output:STORE__  
The user can ask for the result of a Pig Latin expression sequence to be materialized to a file, by issuing the STORE command, e.g.,

    STORE query_revenues INTO ‘myoutput’ USING myStore();

## 4 IMPLEMENTATION
__4.1 Building a Logical Plan__  
As clients issue Pig Latin commands, the Pig interpreter first parses it, and verifies that the input files and bags being referred to by the command are valid. For example, if the user enters c = COGROUP a BY ..., b BY ..., Pig verifies that the bags a and b have already been defined. Pig builds a logical plan for every bag that the user defines. When a new bag is defined by a command, the logical plan for the new bag is constructed by combining the logical plans for the input bags, and the current command. Thus, in the above example, the logical plan for c consists of a cogroup command having the logical plans for a and b as inputs.

Note that no processing is carried out when the logical plans are constructed. Processing is triggered only when the user invokes a `STORE` command on a bag. At that point, the logical plan for that bag is compiled into a physical plan, and is executed. This lazy style of execution is beneficial because it permits in-memory pipelining, and other optimizations such as filter reordering across multiple Pig Latin commands.

Pig is architected such that the parsing of Pig Latin and the logical plan construction is independent of the execution platform. Only the compilation of the logical plan into a physical plan depends on the specific execution platform chosen.

__4.2 Map-Reduce Plan Compilation__  
Our compiler begins by converting each `(CO)GROUP` command in the logical plan into a distinct map-reduce job with its own map and reduce functions.

The map function for `(CO)GROUP` command C initially just assigns keys to tuples based on the BY clause(s) of C; the reduce function is initially a no-op. The map-reduce boundary is the cogroup command. The sequence of `FILTER`, and `FOREACH` commands from the `LOAD` to the first `COGROUP` operation C1, are pushed into the map function corresponding to C1 (see Figure 3). The commands that intervene between subsequent `COGROUP` commands Ci and Ci+1 can be pushed into either (a) the reduce function corresponding to Ci, or (b) the map function corresponding to Ci+1. Pig currently always follows option (a). Since grouping is often followed by aggregation, this approach reduces the amount of data that has to be materialized between map-reduce jobs.  
![mr](/assets/2013-03-03-pig/mr.png)

In the case of a `COGROUP` command with more than one input data set, the map function appends an extra field to each tuple that identifies the data set from which the tuple originated. The accompanying reduce function decodes this information and uses it to insert the tuple into the appropriate nested bag when cogrouped tuples are formed (recall Figure 2).

Parallelism for `LOAD` is obtained since Pig operates over files residing in the Hadoop distributed file system. We also automatically get parallelism for `FILTER` and `FOREACH `operations since for a given map-reduce job, several map and reduce instances are run in parallel. Parallelism for `(CO)GROUP` is achieved since the output from the multiple map instances is repartitioned in parallel to the multiple reduce instances.

The `ORDER` command is implemented by compiling into two map-reduce jobs. The first job samples the input to determine quantiles of the sort key. The second job range-partitions the input according to the quantiles (thereby ensuring roughly equal-sized partitions), followed by local sorting in the reduce phase, resulting in a globally sorted file.

[1]:http://incubator.apache.org/ambari/ "Apache Ambari"
[2]:http://flume.apache.org/ "Apache Flume"
[3]:http://hbase.apache.org/ "Apache Hbase"
[4]:http://wiki.apache.org/hadoop/MapReduce "Apache MapReduce"
[5]:http://hadoop.apache.org/docs/r1.1.1/hdfs_design.html "HDFS Architecture Guide"
[6]:http://zookeeper.apache.org/ "Apache Zookeeper"
[7]:http://pig.apache.org/ "Apache Pig"
[8]:http://hive.apache.org/ "Apache Hive"
[9]:http://oozie.apache.org/ "Apache Oozie"
[10]:http://sqoop.apache.org/ "Apache Sqoop"
[11]:http://incubator.apache.org/hcatalog/ "Apache Hcatalog"
[12]:http://whirr.apache.org/ "Apache whirr"
[13]:http://mahout.apache.org/ "Apache Mahout"

[20]:http://zhangjunhd.github.com/2013/02/24/apache-related-projects/
[21]:http://zhangjunhd.github.com/2013/02/25/apache-hbase/
[22]:http://zhangjunhd.github.com/2013/03/01/zookeeper/
[23]:http://zhangjunhd.github.com/2013/03/03/pig/
[24]:http://zhangjunhd.github.com/2013/03/04/oozie/
[25]:http://zhangjunhd.github.com/2013/03/04/hive/
[26]:http://zhangjunhd.github.com/2013/03/06/apache-related-projects2/
