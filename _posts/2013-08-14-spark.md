---
layout: post
title: "Spark: Cluster Computing with Working Sets"
description: ""
category: tech
tags: [spark, paper]
---
{% include JB/setup %}
paper review:[Spark: Cluster Computing with Working Sets](http://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-53.pdf)

<!--break-->

## 1 Introduction
In this paper, we focus on one such class of applications: those that reuse a working set of data across multiple parallel operations. This class of applications includes two use cases where we have seen Hadoop users in academia and industry report that MapReduce by itself is inadequate:

* **Iterative jobs**: Many common machine learning algorithms apply a function repeatedly to the same dataset to optimize a parameter (e.g., through gradient descent). While each iteration can be expressed as a MapReduce/Dryad job, each job must reload the data from disk, incurring a significant performance penalty.
* **Interactive analysis**: Hadoop is often used to perform ad-hoc exploratory queries on big datasets, through SQL interfaces such as Pig and Hive. Ideally, a user would be able to load a dataset of interest into memory across a number of machines and query it repeatedly. However, with Hadoop, each query incurs significant latency (tens of seconds) because it runs as a separate MapReduce job and reads data from disk.

The main abstraction in Spark is that of a resilient distributed dataset (`RDD`), which represents a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Users can explicitly cache an RDD in memory across machines and reuse it in multiple MapReduce-like *parallel operations*. RDDs achieve fault tolerance through a notion of *lineage*: if a partition of an RDD is lost, the RDD has enough information about how it was derived from other RDDs to be able to rebuild just that partition. 

##2 Programming Model
#### 2.1 Resilient Distributed Datasets (RDDs)
In Spark, each RDD is represented by a Scala object. Spark lets programmers construct RDDs in four ways:

* From a file in a shared file system, such as the Hadoop Distributed File System (HDFS).
* By “*parallelizing*” a Scala collection (e.g., an array) in the driver program, which means dividing it into a number of slices that will be sent to multiple nodes.
* By *transforming* an existing RDD. A dataset with elements of type A can be transformed into a dataset with elements of type B using an operation called flatMap, which passes each element through a user-provided function of type A ⇒ List[B]. Other transformations can be expressed using flatMap, including map (pass elements through a function of type A ⇒ B) and filter (pick elements matching a predicate).
* By changing the *persistence* of an existing RDD. By default, RDDs are lazy and ephemeral. That is, partitions of a dataset are materialized on demand when they are used in a parallel operation (e.g., by passing a block of a file through a map function),and are discarded from memory after use. However, a user can alter the persistence of an RDD through two actions:
  * The *cache* action leaves the dataset lazy, but hints that it should be kept in memory after the first time it is computed, because it will be reused.
  * The *save* action evaluates the dataset and writes it to a distributed filesystem such as HDFS. The saved version is used in future operations on it.

####2.2 Parallel Operations
Several parallel operations can be performed on RDDs:

* *reduce*: Combines dataset elements using an associative function to produce a result at the driver program.
* *collect*: Sends all elements of the dataset to the driver program. For example, an easy way to update an array in parallel is to parallelize, map and collect the array.
* *foreach*: Passes each element through a user provided function.This is only done for the side effects of the function (which might be to copy data to another system or to update a shared variable as explained below).

####2.3 Shared Variables
* *Broadcast variables*: If a large read-only piece of data (e.g., a lookup table) is used in multiple parallel operations, it is preferable to distribute it to the workers only once instead of packaging it with every closure. Spark lets the programmer create a “broadcast variable” object that wraps the value and ensures that it is only copied to each worker once.
* *Accumulators*: These are variables that workers can only “add” to using an associative operation, and that only the driver can read. They can be used to implement counters as in MapReduce and to provide a more imperative syntax for parallel sums. Accumulators can be defined for any type that has an “add” operation and a “zero” value. Due to their “add-only” semantics, they are easy to make fault-tolerant.

##4 Implementation
Spark is built on top of `Nexus`, a “cluster operating system” that lets multiple parallel applications share a cluster in a fine-grained manner and provides an API for applications to launch tasks on a cluster. 

The core of Spark is the implementation of resilient distributed datasets. As an example, suppose that we define a cached dataset called cachedErrs representing error messages in a log file, and that we count its elements using *map* and *reduce*:

    val file = spark.textFile("hdfs://...")
    val errs = file.filter(_.contains("ERROR"))
    val cachedErrs = errs.cache()
    val ones = cachedErrs.map(_ => 1)
    val count = ones.reduce(_+_)
These datasets will be stored as a chain of objects capturing the lineage of each RDD, shown in Figure 1. Each dataset object contains a pointer to its parent and information about how the parent was transformed. Internally, each RDD object implements the same simple interface, which consists of three operations:

* getPartitions, which returns a list of partition IDs.
* getIterator(partition), which iterates over a partition.
* getPreferredLocations(partition), which is used for task scheduling to achieve data locality.

![spark](/assets/2013-08-14-spark/spark.png)

When a parallel operation is invoked on a dataset, Spark creates a task to process each partition of the dataset and sends these tasks to worker nodes. We try to send each task to one of its preferred locations using a technique called `delay scheduling`. Once launched on a worker, each task calls getIterator to start reading its partition.