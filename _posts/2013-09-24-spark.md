---
layout: post
title: "Spark相关"
description: ""
category: tech
tags: [Spark, RDD, scala, Shark, Flumejava, streaming]
---
{% include JB/setup %}
Spark，FlumeJava相关文章 review 11-20

<!--break-->
####11 [Spark：一个高效的分布式计算系统][1]
* RDD的转换与操作：Transformations操作是Lazy的，Actions是触发Spark启动计算的动因。
* Lineage（血统）：Narrow Dependencies对于数据的重算开销要远小于Wide Dependencies的数据重算开销。
* 容错：logging the updates方式，通过记录跟踪所有生成RDD的转换（transformations）也就是记录每个RDD的lineage（血统）来重新计算生成丢失的分区数据。
* 编程接口：类似于DryadLINQ和FlumeJava，每个数据集都表示为RDD对象，对RDD的转换与操作通过Scala闭包(字面量函数)来表示，Scala使用Java对象来表示闭包且都是可序列化的，以此把对RDD的闭包操作发送到各Workers节点。
* Scala：提供了Spark-Shell。
* Java：Spark支持Java编程，但对于使用Java就没有了Spark-Shell这样方便的工具，其它与Scala编程是一样的。
* Python：使用py4j来实现python与java的互操作，提供了pyspark，一个Spark的python shell，可以以交互式的方式使用Python编写Spark程序。

####12 [Spark: Cluster Computing with Working Sets][2]
MapReduce可能对两类任务处理的不好：

* `Iterative jobs`: 一些machine learning的算法可能使用一个函数对同一个dataset反复调优一个参数。
* `Interactive analysis`: 对于某些ad-hoc query，可能请求的数据量也不是很大。

resilient distributed dataset (RDD)

它是只读对象集合，它被partition到各个机器上，当某个partition丢失后，可以rebuild，它是通过lineage实现的：当某个partition上的RDD丢失后，RDD有足够的信息直到它是从哪些RDDs衍生出来的，因此可以rebuild它。

每个RDD是一个scala对象。可以通过四种方式构建：

* 通过HDFS
* 通过 parallelizing 一个 Scala 集合(e.g., an array)，这表明需要将这个对象切片并分散到各个节点上。
* 有已知的RDD transforming得到。
* 通过对已知的RDD改变它的persistence得到。缺省情况下，RDD是lazy和ephemeral的。所以每个数据切片是通过一些并行操作组装而成，用完之后被丢弃。用户可以改变persistence，通过两种action：
  * cache，提示说暂存在内存中
  * save，需求持久化下来(存HDFS)，备用

####13 [FlumeJava: Easy, Efficient Data-Parallel Pipelines][3]

#####The FlumeJava Library

Core Abstractions

* `PCollection<T>`:immutable bag(ordered:sequence，unordered:collection)
* `PTable<K,V>`:immutable multi-map
* `parallelDo()`:core data-parallel primitive
* `DoFn<T, S>`:a function-like object defining how to map each value in the input `PCollection<T>` into zero or more values to appear in the output `PCollection<S>`

        PCollection<String> lines = readTextFileCollection("/gfs/data/shakes/hamlet.txt");
        
        PCollection<String> words = lines.parallelDo(new DoFn<String,String>() {
          void process(String line, EmitFn<String> emitFn) {
            for (String word : splitIntoWords(line)) {
              emitFn.emit(word);
            }
          }
        }, collectionOf(strings()));

* `groupByKey()`:converts a multi-map of type `PTable<K,V>` into a uni-map of type `PTable<K, Collection<V>>` where each key maps to an unordered, plain Java Collection of all the values with that key.


        PTable<URL,DocInfo> backlinks =
          docInfos.parallelDo(new DoFn<DocInfo, Pair<URL,DocInfo>>() {
            void process(DocInfo docInfo,EmitFn<Pair<URL,DocInfo>> emitFn) {
              for (URL targetUrl : docInfo.getLinks()) {
                emitFn.emit(Pair.of(targetUrl, docInfo));
              }
            }
          }, tableOf(recordsOf(URL.class),recordsOf(DocInfo.class)));

        PTable<URL,Collection<DocInfo>> referringDocInfos = backlinks.groupByKey();

* `combineValues()`:takes an input `PTable<K, Collection<V>>` and an associative combining function on Vs, and returns a `PTable<K, V>` where each input collection of values has been combined into a single output value.


        PTable<String,Integer> wordsWithOnes = words.parallelDo(
          new DoFn<String, Pair<String,Integer>>() {
            void process(String word, EmitFn<Pair<String,Integer>> emitFn) {
              emitFn.emit(Pair.of(word, 1));
            }
          }, tableOf(strings(), ints()));

        PTable<String,Collection<Integer>> groupedWordsWithOnes = wordsWithOnes.groupByKey();

        PTable<String,Integer> wordCounts = groupedWordsWithOnes.combineValues(SUM_INTS);

* `flatten()`:takes a list of `PCollection<T>`s and returns a single `PCollection<T>` that contains all the elements of the input PCollections.

Derived Operations

* count()
* join()
* top()

FlumeJava’s parallel operations are executed `lazily` using `deferred evaluation`. The result of executing a series of FlumeJava operations is thus a directed acyclic graph of deferred PCollections and operations; we call this graph the `execution plan`.

`PObject` thus acts much like a `future`.

    PTable<String,Integer> wordCounts = ...;
    PObject<Collection<Pair<String,Integer>>> result = wordCounts.asSequentialCollection();
    ...
    FlumeJava.run();
    for (Pair<String,Integer> count : result.getValue()) {
      System.out.print(count.first + ": " + count.second);
    }

#####Optimizer

ParallelDo Fusion: ParallelDo producer-consumer fusion and ParallelDo sibling fusion

![1](/assets/2013-09-24-spark/flumejava1.png)

MSCR Fusion:

![2](/assets/2013-09-24-spark/flumejava3.png)

####14 [Spark：大数据的“电光石火”][4]

计算范式和抽象

* Spark处理的是大数据，因此采用了粒度很粗的集合，叫做Resilient Distributed Datasets（RDD）。集合内的所有数据都经过同样的算子序列。数据并行可编程性好，易于获得高并行性.
* 数据并行的范式决定了 Spark无法完美支持细粒度、异步更新的操作。图计算就有此类操作，所以此时Spark不如GraphLab（一个大规模图计算框架）；还有一些应用， 需要细粒度的日志更新和数据检查点，它也不如RAMCloud（斯坦福的内存存储和计算研究项目）和Percolator（Google增量计算技术）。
* 所有算子都是幂等的，出现错误时只需把算子序列重新执行即可。
* Spark的突破在于，在保证容错的前提下，用内存来承载工作集。

编程模型

![3](http://cms.csdnimg.cn/article/201307/08/51da7fcc7be66.jpg)

* 输入算子（橘色箭头）,变换（transformation）算子（蓝色箭头）,缓存算子（例子中的cache算子，灰色箭头表示）将分区物化（materialize）存下来（灰色方块）。
* 这里很重要的设计是lazy evaluation：计算并不实际发生，只是不断地记录到元数据。
* Lineage一直增长，直到遇上行动（action）算子（图1中的绿色箭头），这时就要evaluate了，把刚才累积的所有算子一次性执行。
* 另一个要点是一旦行动算子产生原生数据，就必须退出RDD空间。
* 由于Spark并不提供控制流，在计算逻辑需要条件分支时，也必须回退到Scala的空间。


运行和调度

* worker节点上有任务线程（task thread）真正运行DAGScheduler生成的任务；还有块管理器（block manager）负责与master上的block manager master通信（完美使用了Scala的Actor模式），为任务线程提供数据块。
* 窄依赖指父RDD的每一个分区最多被一个子RDD的分区所用，表现为一个父RDD的分区对应于一个子RDD的分区，和两个父RDD的分区对应于一个子RDD 的分区。
* 宽依赖指子RDD的分区依赖于父RDD的所有分区，这是因为shuffle类操作。
* 如果子RDD的分区到父RDD的分区是窄依赖，就可以实施经典的fusion优化，Spark把这个叫做流水线 （pipeline）优化。
* 变换算子序列一碰上shuffle类操作，宽依赖就发生了，流水线优化终止。在具体实现中，DAGScheduler从当前算子往前回溯依赖图，一碰到宽依赖，就生成一个stage来容纳已遍历的算子序列。在这个stage里，可以安全地实施流水线优化。然后，又从那个宽依赖开始继续回溯，生成下一个stage。
* 宽/窄依赖的概念不止用在调度中，对容错也很有用。
![4](http://cms.csdnimg.cn/article/201307/08/51da6eacf2989.jpg)

####15 [Berkeley Data Analytics Stack (BDAS) Overview][5]

![5](/assets/2013-09-24-spark/1.png)

####16 [Parallel Programming With Spark][6]

Example: Mining Console Logs

![6](/assets/2013-09-24-spark/2.png)

####17 [Introduction to Spark Internals][7]

#####Components

![7](/assets/2013-09-24-spark/3.png)

#####Scheduling Process

![8](/assets/2013-09-24-spark/4.png)

#####RDD Interface

* Set of `partitions` (“splits”)
* List of `dependencies` on parent RDDs
* Function to `compute` a partition given parents
* Optional `preferred locations`
* Optional `partitioning info` (Partitioner)

#####DAG Scheduler

Interface: receives a “target” RDD, a function to run on each partition, and a listener for results 

Roles:

* Build stages of Task objects (code + preferred loc.)
* Submit them to TaskScheduler as ready
* Resubmit failed stages if outputs are lost

#####Scheduler Optimizations

* Pipelines narrow ops. within a stage
* Picks join algorithms based on partitioning (minimize shuffles)
* Reuses previously cached data

#####Task Details

Stage boundaries are only at input RDDs or “shuffle” operations(Note: we write shuffle outputs to RAM/disk to allow retries)

So, each task looks like this:

![9](/assets/2013-09-24-spark/5.png)

Each Task object is `self-contained`,Contains all transformation code up to input boundary (e.g. HadoopRDD => filter => map)

#####Event Flow

![10](/assets/2013-09-24-spark/6.png)

#####TaskScheduler

* Interface:
  * Given a TaskSet (set of Tasks), run it and report results
  * Report “fetch failed” errors when shuffle output lost
* Two main implementations:
  * LocalScheduler (runs locally)
  * ClusterScheduler (connects to a cluster manager using a pluggable “SchedulerBackend” API)

#####Worker

* Implemented by the Executor class
* Receives self-contained Task objects and calls run() on them in a thread pool
* Reports results or exceptions to master(Special case: FetchFailedException for shuffle)
* Pluggable ExecutorBackend for cluster

####18 [Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing][8]

RDD Operations:

![11](/assets/2013-09-24-spark/7.png)

Representing RDDs

![12](/assets/2013-09-24-spark/8.png)

####19 [Discretized Streams: An Efﬁcient and Fault-Tolerant Model for Stream Processing on Large Clusters][9]

The key idea behind D-Streams is to treat a streaming computation as a series of deterministic batch computations on small time intervals.

![13](/assets/2013-09-24-spark/9.png)

D-Streams introduce new stateful operators that work over multiple intervals. These include:

* `Windowing`: The window operator groups all of the records from a range of past time intervals into a single RDD. For example, in our earlier code, calling `pairs.window("5s").reduceByKey(_+_)` yields a D-Stream of word counts on intervals [0, 5), [1, 6), [2, 7), etc.
* `Incremental aggregation`:For example, one might write:
       
        pairs.reduceByWindow("5s", (a, b) => a + b)

* This computes a per-interval count for each time interval only once, but has to add the counts for the past five seconds repeatedly, as in Figure 2a. A more efficient version for invertible aggregation functions also takes a function for “subtracting” values and updates state incrementally (Figure 2b).
* `Time-skewed joins`: Users can join a stream against its own RDDs from some time in the past to compute trends—for example, how current page view counts compare to page views five minutes ago.

![14](/assets/2013-09-24-spark/10.png)

####20 [Shark: SQL and Rich Analytics at Scale][10]

#####Partial DAG Execution (PDE)

We currently apply partial DAG execution at blocking “shuffle" operator boundaries where data is exchanged and repartitioned, since these are typically the most expensive operations in Shark.

PDE modifies this mechanism in two ways. First, it gathers customizable statistics at global and per-partition granularities while materializing map output. Second, it allows the DAG to be altered based on these statistics, either by choosing different operators or altering their parameters (such as their degrees of parallelism).

These statistics are customizable using a simple, pluggable accumulator API. Some example statistics include:

* Partition sizes and record counts, which can be used to detect skew.
* Lists of “heavy hitters,” i.e., items that occur frequently in the dataset.
* Approximate histograms, which can be used to estimate partitions’ data’s distributions.


#####Data Co-partitioning
A technique commonly used by MPP databases is to co-partition the two tables based on their join key in the data loading process. 

    CREATE TABLE l_mem TBLPROPERTIES ("shark.cache"=true)
    AS SELECT * FROM lineitem DISTRIBUTE BY L_ORDERKEY;

    CREATE TABLE o_mem TBLPROPERTIES ("shark.cache"=true, "copartition"="l_mem")
    AS SELECT * FROM order DISTRIBUTE BY O_ORDERKEY;

#####Partition Statistics and Map Pruning

`Map pruning` is the process of pruning data partitions based on their natural clustering columns. Since Shark’s memory store splits data into small partitions, each block contains only one or few log- ical groups on such columns, and Shark can avoid scanning certain blocks of data if their values fall out of the query’s filter range.

To take advantage of these natural clusterings of columns, Shark’s memory store on each worker piggybacks the data loading process to collect statistics. The information collected for each partition include the range of each column and the distinct values if the number of distinct values is small (i.e., enum columns). The collected statistics are sent back to the master program and kept in memory for pruning partitions during query execution.

[1]: http://tech.uc.cn/?p=2116
[2]: http://www.cs.berkeley.edu/~matei/papers/2010/hotcloud_spark.pdf
[3]: http://pages.cs.wisc.edu/~akella/CS838/F12/838-CloudPapers/FlumeJava.pdf
[4]: http://www.csdn.net/article/2013-07-08/2816149
[5]: http://ampcamp.berkeley.edu/wp-content/uploads/2013/02/Berkeley-Data-Analytics-Stack-BDAS-Overview-Ion-Stoica-Strata-2013.pdf
[6]: http://ampcamp.berkeley.edu/wp-content/uploads/2012/06/matei-zaharia-part-1-amp-camp-2012-spark-intro.pdf
[7]: http://spark.incubator.apache.org/talks/dev-meetup-dec-2012.pptx
[8]: http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf
[9]: http://www.cs.berkeley.edu/~matei/papers/2012/hotcloud_spark_streaming.pdf
[10]: http://arxiv.org/pdf/1211.6176v1.pdf

