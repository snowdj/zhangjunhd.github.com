---
layout: post
title: "Chubby"
description: ""
category: tech
tags: [paper, google, chubby, paxos]
---
{% include JB/setup %}
paper review:[The Chubby lock service for loosely-coupled distributed systems](http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/zh-CN//archive/chubby-osdi06.pdf)

<!--break-->
##2 Design
####2.1 Rationale
Some decisions follow from our expected use and from our environment:

* A service advertising its primary via a Chubby file may have thousands of clients. Therefore, we must allow thousands of clients to observe this file, preferably without needing many servers.
* Clients and replicas of a replicated service may wish to know when the service’s primary changes. This suggests that an event notification mechanism would be useful to avoid polling.
* Even if clients need not poll files periodically, many will; this is a consequence of supporting many developers. Thus, caching of files is desirable.
* Our developers are confused by non-intuitive caching semantics, so we prefer consistent caching.
* To avoid both financial loss and jail time, we provide security mechanisms, including access control.

`Coarse-grained` locks impose far less load on the lock server. In particular, the lock-acquisition rate is usually only weakly related to the transaction rate of the client applications. Coarse-grained locks are acquired only rarely, so temporary lock server unavailability delays clients less. On the other hand, the transfer of a lock from client to client may require costly recovery procedures, so one would not wish a fail-over of a lock server to cause locks to be lost. Thus, it is good for coarse-grained locks to survive lock server failures, there is little concern about the overhead of doing so, and such locks allow many clients to be adequately served by a modest number of lock servers with somewhat lower availability.

`Fine-grained` locks lead to different conclusions. Even brief unavailability of the lock server may cause many clients to stall. Performance and the ability to add new servers at will are of great concern because the transaction rate at the lock service grows with the combined transaction rate of clients. It can be advantageous to reduce the overhead of locking by not maintaining locks across lock server failure, and the time penalty for drop- ping locks every so often is not severe because locks are held for short periods. (Clients must be prepared to lose locks during network partitions, so the loss of locks onlock server fail-over introduces no new recovery paths.)
Chubby is intended to provide only coarse-grained locking. 

####2.2 System structure

![1](/assets/2013-03-12-chubby/1.png)

A Chubby `cell` consists of a small set of servers (typically five) known as `replicas`, placed so as to reduce the likelihood of correlated failure (for example, in different racks). The replicas use a distributed consensus protocol to elect a `master`; the master must obtain votes from a majority of the replicas, plus promises that those replicas will not elect a different master for an interval of a few seconds known as the `master lease`. The master lease is periodically renewed by the replicas provided the master continues to win a majority of the vote.

The replicas maintain copies of a simple database, but only the master initiates reads and writes of this database. All other replicas simply copy updates from the master, sent using the consensus protocol.

Clients find the master by sending master location requests to the replicas listed in the DNS. Non-master replicas respond to such requests by returning the identity of the master. Once a client has located the master, the client directs all requests to it either until it ceases to respond, or until it indicates that it is no longer the master. Write requests are propagated via the consensus protocol to all replicas; such requests are acknowledged when the write has reached a majority of the replicas in the cell. Read requests are satisfied by the master alone; this is safe provided the master lease has not expired, as no other master can possibly exist. If a master fails, the other replicas run the election protocol when their master leases expire; a new master will typically be elected in a few seconds.

####2.3 Files, directories, and handles

The name space contains only files and directories, collectively called `nodes`. Every such node has only one name within its cell; there are no symbolic or hard links.

Nodes may be either `permanent` or `ephemeral`. Any node may be deleted explicitly, but ephemeral nodes are also deleted if no client has them open (and, for directories, they are empty). Ephemeral files are used as temporary files, and as indicators to others that a client is alive.

The per-node meta-data includes four monotonically-increasing 64-bit numbers that allow clients to detect changes easily:

* an instance number; greater than the instance number of any previous node with the same name.
* a content generation number (files only); this increases when the file’s contents are written.
* a lock generation number; this increases when the node’s lock transitions from `free` to `held`.
* an ACL generation number; this increases when the node’s ACL names are written.

Clients open nodes to obtain handles that are analogous to UNIX file descriptors. Handles include:

* check digits that prevent clients from creating or guessing handles, so full access control checks need be performed only when handles are created (compare with UNIX, which checks its permissions bits at open time, but not at each read/write because file descriptors cannot be forged).
* a sequence number that allows a master to tell whether a handle was generated by it or by a previous master.
* mode information provided at open time to allow the master to recreate its state if an old handle is presented to a newly restarted master.

####2.4 Locks and sequencers

Each Chubby file and directory can act as a reader-writer lock: either one client handle may hold the lock in exclusive (writer) mode, or any number of client handles may hold the lock in shared (reader) mode. Like the mutexes known to most programmers, locks are `advisory`.

At any time, a lock holder may request a `sequencer`, an opaque byte-string that describes the state of the lock immediately after acquisition. It contains the name of the lock, the mode in which it was acquired (exclusive or shared), and the lock generation number. The client passes the sequencer to servers (such as file servers) if it expects the operation to be protected by the lock. The recipient server is expected to test whether the sequencer is still valid and has the appropriate mode;
























































































































