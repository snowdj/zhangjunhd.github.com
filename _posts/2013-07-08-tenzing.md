---
layout: post
title: "Tenzing"
description: ""
category: tech
tags: [google, tenzing, paper]
---
{% include JB/setup %}

paper review:[Tenzing A SQL Implementation On The MapReduce Framework](https://www.google.com.hk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CC8QFjAA&url=%68%74%74%70%3a%2f%2f%72%65%73%65%61%72%63%68%2e%67%6f%6f%67%6c%65%2e%63%6f%6d%2f%70%75%62%73%2f%61%72%63%68%69%76%65%2f%33%37%32%30%30%2e%70%64%66&ei=sZfaUe2BM9DrlAXPmYCIDA&usg=AFQjCNFQQrbq6J8KMlzYr2SaAB0sW181rg)

<!--break-->

##1. INTRODUCTION
Tenzing, a SQL query execution engine built on top of MapReduce:

* With latency as low as **ten seconds**.
* A comprehensive SQL92 implementation with some SQL99 extensions.
* Supports efficiently querying data in row stores, column stores, Bigtable, GFS, text and protocol buffers.

The Tenzing service currently:

* With more than a thousand users and ten thousand analytic queries per day.
* Runs on two data centers with two thousand cores each.
* Serves queries on over 1.5 petabytes of compressed data in several different data sources and formats.

##2. HISTORY AND MOTIVATION
Faced the following issues:

* Increased cost of scalability: our need was to scale to petabytes of data, but the cost of doing so on DBMS-X was deemed unacceptably high.
* Rapidly increasing loading times: importing new data took hours each day and adding new data sources took proportionally longer. Further, import jobs competed with user queries for resources, leading to poor query performance during the import process.
* Analyst creativity was being stifled by the limitations of SQL and lack of access to multiple sources of data. An increasing number of analysts were being forced to write custom code for more complex analysis, often directly against the source (such as Sawzall against logs).

The new platform had to:

* Scale to thousands of cores, hundreds of users and petabytes of data.
* Run on unreliable off-the-shelf hardware, while continuing to be highly reliable.
* Match or exceed the performance of the existing platform.
* Have the ability to run directly off the data stored on Google systems, **to minimize expensive ETL processes**.
* Provide all the required SQL features to the analysts to minimize the learning curve, while also supporting more advanced functionality such as complex user- defined functions, prediction and mining.

##3. IMPLEMENTATION OVERVIEW
![tenzing arch](/assets/2013-07-08-tenzing/tenzing_arch.png)

* The `distributed worker pool`
  * The execution system which takes a query execution plan and executes the MapReduces. 
  * Consists of `master` and `worker` nodes, plus an overall gatekeeper called the `master watcher`. 

* The `backend storage`
  * Be a mix of various data stores, such as ColumnIO, Bigtable, GFS files, MySQL databases, etc.  
    
* The `query server`
  * Serves as the gateway between the client and the pool. 
  * Parses the query, applies optimizations and sends the plan to the master for execution.      
* The `metadata server`
  * Provides an API to store and fetch metadata such as table names and schemas, and pointers to the underlying data.
  * Be responsible for storing ACLs (Access Control Lists) and other security related information about the tables.
  * Uses Bigtable as the persistent backing store.  

* Several `client interfaces`
  * CLI
  * Web UI
  * API to directly execute queries on the pool
  * A standalone binary which does not need any server side components, but rather launches its own MapReduce jobs.  

####3.1 Life Of A Query
A typical Tenzing query goes through the following steps:

* A user (or another process) submits the query to the query server through the Web UI, CLI or API.
* The query server parses the query into an intermediate parse tree.
* The query server fetches the required metadata from the metadata server to create a more complete intermediate format.
* The optimizer goes through the intermediate format and applies various optimizations.
* The optimized execution plan consists of one or more MapReduces. For each MapReduce, the query server finds an available master using the master watcher and submits the query to it. At this stage, the execution has been physically partitioned into multiple units of work(i.e. shards).
* Idle workers poll the masters for available work. Reduce workers write their results to an intermediate storage.
* The query server monitors the intermediate area for results being created and gathers them as they arrive. The results are then streamed to the upstream client.

##4. SQL FEATURES
* Projection And Filtering
  * Supports all the standard SQL operators (arithmetic operators, IN, LIKE, BETWEEN, CASE, etc.) and functions.
  * Any built-in Sawzall function can be used.
  * Does several basic optimizations related to filtering.
* Aggregation
  * Supports all standard aggregate functions such as SUM, COUNT, MIN, MAX, etc. and the DISTINCT equivalents (e.g., COUNT DISTINCT).
  * Supports a significant number of statistical aggregate functions such as CORR, COVAR and STDDEV.
* Joins
  * Supports efficient joins across data sources, such as ColumnIO to Bigtable.
  * inner, left, right, cross, and full outer joins
  * equi semi-equi, non-equi and function based joins
* Analytic Functions
  * Supports all the major analytic functions, with a syntax similar to PostgreSQL / Oracle.
  * Some of the most commonly used analytic functions are RANK, SUM, MIN, MAX, LEAD, LAG and NTILE.
* OLAP Extensions
  * Supports the ROLLUP() and CUBE() OLAP ex- tensions to the SQL language.
* Set Operations
  * Supports all standard SQL set operations such as UNION, UNION ALL, MINUS and MINUS ALL.
* Nested Queries And Subqueries
  * Typically, each nested SQL gets converted to a separate MapReduce and the resultant intermediate table is substituted in the outer query.
  * The compiler can optimize away relatively simple nested queries such that extra MapReduce jobs need not be created.
* Handling Structured Data
  * Has read-only support for structured (nested and repeated) data formats such as complex protocol buffer structures.
  * The data is flattened by the reader at the lowest level and fed as multiple records to the engine.
  * The engine itself can only deal with flat relational data, **unlike Dremel**.
  * Selecting fields from different repetition levels in the same query is considered an error.
* Views
  * Views in Tenzing are predominantly used for security reasons: users can be given access to views without granting them access to underlying tables, enabling row and column level security of data.
* DML
  * Has basic support for DML operations INSERT, UPDATE and DELETE.
  * Tenzing is not ACID compliant - specifically, we are atomic, consistent and durable, but **do not support isolation**.
* DDL
  * Support a number of DDL operations, including CREATE [OR REPLACE] [EXTERNAL] [TEMPORARY] TABLE, DROP TABLE [IF EXISTS], RENAME TABLE, GENERATE STATISTICS, GRANT and REVOKE.
  * Has metadata discovery mechanisms built-in to simplify importing datasets into Tenzing.
* Table Valued Functions
  * Supports both scalar and table-valued user-defined functions, implemented by embedding a Sawzall interpreter in the Tenzing execution engine.
  * Integration of Lua and R has been proposed, and work is in progress.
* Data Formats
  * Supports direct querying of, loading data from, and downloading data into many formats.
  * For delimited text format, the user can specify the delimiter, encoding, quoting, escaping, headers, etc.

##5. PERFORMANCE
####5.1 MapReduce Enhancements
* Workerpool
  * One of the key challenges we faced was reducing latency from minutes to seconds.
  * Implement a solution which did not entail spawning of new binaries for each new Tenzing query.
  * A typical pool consists of three process groups:
    * `The master watcher`. The watcher is responsible for receiving a work request and assigning a free master for the task. The watcher also monitors the overall health of the pool such as free resources, number of running queries, etc. There is usually one one watcher process for one instance of the pool.
    * `The master pool`. This consists of a relatively small number of processes (usually a few dozen). The job of the master is to coordinate the execution of one query. The master receives the task from the watcher and distributes the tasks to the workers, and monitors their progress. Note that once a master receives a task, it takes over ownership of the task, and the death of the watcher process does not impact the query in any way.
    * `The worker pool`. This contains a set of workers (typi- cally a few thousand processes) which do all the heavy lifting of processing the data. Each worker can work as either a mapper or a reducer or both. Each worker constantly monitors a common area for new tasks and picks up new tasks as they arrive on a FIFO basis. We intend to implement a priority queue so that queries can be tiered by priority.
    * Using this approach, we were able to **bring down the latency of the execution of a Tenzing query itself to around 7 seconds**.
  * Streaming & In-memory Chaining.
    * Implementing streaming between MapReduces, i.e. the upstream and downstream MRs communicate using the network and only use GFS for backup.
  * Sort Avoidance.
    * Certain operators such as hash join and hash aggregation require shuffling, but not sorting. The MapReduce API was enhanced to automatically turn off sorting for these operations.
  * Block Shuffle.
    * Implemented a block-based shuffle mechanism on top of the existing row-based shuffler in MapReduce that combines many small rows into compressed blocks of roughly 1MB in size. By treating the entire block as one row and avoiding reducer side sorting, we were able to avoid some of the overhead associated with row serialization and deserialization in the underlying MapReduce framework code. This lead to 3X faster shuffling of data compared to row based shuffling with sorting.
  * Local Execution.
    * The backend can detect the size of the underlying data to be processed. If the size is under a threshold (typically 128 MB), the query is not sent to the pool, but executed directly in the client process. This reduces the query latency to about 2 seconds.
