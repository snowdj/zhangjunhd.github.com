---
layout: post
title: "Apache Hive"
description: ""
category: tech
tags: [Hadoop, Hive, Sqoop]
---
{% include JB/setup %}

##Apache Hadoop-Related Projects List

- [Ambari][1] : Deployment, configuration and monitoring, see [part1][20]
- [Flume][2]:Collection and import of log and event data, see [part1][20]
- [MapReduce][4]: Parallel computation on server clusters, see [part1][20]
- [HDFS][5] Distributed redundant filesystem for Hadoop, see [part1][20]
- [HBase][3]:Column-oriented database scaling to billions of rows, see [part2][21]
- [Zookeeper][6]:Configuration management and coordination, see [part3][22]
- [Pig][7]:High-level programming language for Hadoop computations, see [part4][23]
- [Hive][8]: Data warehouse with SQL-like access
- [Oozie][9]: Orchestration and workflow management, see [part6][24]
- [Sqoop][10]: Imports data from relational databases
- [HCatalog][11]: Schema and data type sharing over Pig, Hive and MapReduce, see [part8][26]
- [Whirr][12]: Cloud-agnostic deployment of clusters, see [part8][26]
- [Mahout][13]: Library of machine learning and data mining algorithms, see [part8][26]

<!--break-->

## 1 DATA LOAD
Load data into the table

    LOAD DATA LOCAL INPATH '/myhd/file_or_dir' INTO TABLE <tablename>
    LOAD DATA INPATH '/hdfs/file_or_dir' INTO TABLE <tablename>
    
Sqoop: SQL to Hadoop

    $ sqoop --connect jdbc:mysql://foo.com/corp \
        --table employees \
        --hive-import \
        --fields-terminated-by '\t' \
        --lines-terminated-by '\n'
![sqoop](/assets/2013-03-04-hive/sqoop.png)

## 2 DATA MODEL
Currently, the following primitive types are supported:  

* Integers – bigint(8 bytes), int(4 bytes), smallint(2 bytes), tinyint(1 byte). All integer types are signed.
* Floating point numbers – float(single double(double precision)
* String

Hive also natively supports the following complex types:

  * Associative arrays – map<key-type, value-type>
  * Lists – list<element-type>
  * Structs – struct<file-name: field-type, … >

The primary data units and their mappings in the hdfs name space are as follows:

* `Tables` – A table is stored in a directory in hdfs.
* `Partitions` – A partition of the table is stored in a sub-directory within a table's directory.
* `Buckets` – A bucket is stored in a file within the partition's or table's directory depending on whether the table is a partitioned table or not.

A table may be partitioned or non-partitioned. A partitioned table can be created by specifying the _PARTITIONED BY_ clause in the CREATE TABLE statement as shown below.
    
    CREATE TABLE test_part(c1 string, c2 int) PARTITIONED BY (ds string, hr int);

A new partition can be created through an _INSERT_ statement or through an _ALTER_ statement that adds a partition to the table. Both the following statements:

    INSERT OVERWRITE TABLE
    test_part PARTITION(ds='2009-01-01', hr=12)
    SELECT * FROM t;

    ALTER TABLE test_part
    ADD PARTITION(ds='2009-02-02', hr=11);

Both these statements end up creating the corresponding directories in the table’s hdfs directory.

     /user/hive/warehouse/test_part/ds=2009-01-01/hr=12
     /user/hive/warehouse/test_part/ds=2009-02-02/hr=11

The Hive compiler is able to use this information to prune the directories that need to be scanned for data in order to evaluate a query. In case of the test_part table, the query

    SELECT * FROM test_part WHERE ds='2009-02-02' AND hr=11;
will only scan all the files within the /user/hive/warehouse/test_part/ds=2009-01-01/hr=12 directory.

At the time the table is created, the user can specify the number of buckets needed and the column on which to bucket the data. In the current implementation this information is used to prune the data in case the user runs the query on a sample of data e.g. a table that is bucketed into 32 buckets can quickly generate a 1/32 sample by choosing to look at the first bucket of data. Similarly, the statement

    SELECT * FROM t TABLESAMPLE(2 OUT OF 32);
would scan the data present in the second bucket.

## 3 QUERY LANGUAGE

   * Hive provides a SQL-like query language called HiveQL which supports select, project, join, aggregate, union all and sub-queries in the from clause.
   * HiveQL supports data definition (`DDL`) statements to create tables with specific serialization formats, and partitioning and bucketing columns.
   * Users can load data from external sources and insert query results into Hive tables via the load and insert data manipulation (`DML`) statements respectively.
   * HiveQL currently does not support updating and deleting rows in existing tables.
   * HiveQL is also very extensible. It supports user defined column transformation (`UDF`) and aggregation (`UDAF`) functions implemented in Java. In addition, users can embed custom map-reduce scripts written in any language using a simple row-based streaming interface.

__Hive extension: multi-table insert__

    FROM(
      SELECT username, accessdate
      FROM logs WHERE url LIKE '%zhangjunhd'
      ) clicks

    INSERT OVERWRITE DIRECTORY 'count' SELECT count(1)

    INSERT OVERWRITE DIRECTORY 'list_users' SELECT DISTINCT clicks.username;

__Invoking custom map script__

    ADD FILE /tmp/map.py;
    INSERT OVERWRITE TABLE results_table SELECT transform(logdata.*) USING'./map.py' as 
    (output)
    FROM  (SELECT * FROM logs) logdata;

__Partitioning__

    CREATE TABLE logs (url STRING, user STRING) PARTITIONED BY (d STRING);
    LOAD DATA LOCAL INPATH '/tmp/new_logs.txt' INTO TABLE logs PARTITION (d='2010-04-01');

__Bucketing__

    CREATE TABLE tablename (columns) CLUSTERED BY (col) INTO N BUCKETS;
    SET hive.enforce.bucketing = true;

## 4 SERDE AND FILE FORMATS
The default SerDe implementation in Hive is called the `LazySerDe` – it deserializes rows into internal objects lazily so that the cost of deserialization of a column is incurred only if the column of the row is needed in some query expression.

The LazySerDe assumes that the data is stored in the file such that the rows are delimited by a newline (ascii code 13) and the columns within a row are delimited by ctrl-A (ascii code 1).

Hadoop files can be stored in different formats.Text files for example are stored in the TextInputFormat and binary files can be stored as SequenceFileInputFormat. Hive also provides an `RCFileInputFormat` which stores the data in a column oriented manner.Users can add their own file formats and associate them to a table as shown in the following statement.

    CREATE TABLE dest1(key INT, value STRING) STORED AS
    INPUTFORMA T 'org.apache.hadoop.mapred.SequenceFileInputFormat'
    OUTPUTFORMA T 'org.apache.hadoop.mapred.SequenceFileOutputFormat'

## 5 SYSTEM ARCHITECTURE
The main components of Hive are:

   * External Interfaces - Hive provides both user interfaces like command line (`CLI`) and web UI, and application programming interfaces (API) like JDBC and ODBC.
   * The `Hive Thrift Server` exposes a very simple client API to execute HiveQL statements. 
   * The `Metastore` is the system catalog. All other components of Hive interact with the metastore.
   * The `Driver` manages the life cycle of a HiveQL statement during compilation, optimization and execution. On receiving the HiveQL statement, from the thrift server or other interfaces, it creates a session handle which is later used to keep track of statistics like execution time, number of output rows, etc.
   * The `Compiler` is invoked by the driver upon receiving a HiveQL statement. The compiler translates this statement into a plan which consists of a DAG of map-reduce jobs.
   * The driver submits the individual map-reduce jobs from the DAG to the `Execution Engine` in a topological order. Hive currently uses Hadoop as its execution engine.  
![arch](/assets/2013-03-04-hive/arch.png)

__Metastore__  
The metastore is the system catalog which contains metadata about the tables stored in Hive. This metadata is specified during table creation and reused every time the table is referenced in HiveQL.

The metastore contains the following objects:

* Database - is a namespace for tables. The database ‘default’ is used for tables with no user supplied database name.
* Table - Metadata for table contains list of columns and their types, owner, storage and SerDe information. It can also contain any user supplied key and value data. Storage information includes location of the table’s data in the underlying file system, data formats and bucketing information. SerDe metadata includes the implementation class of serializer and deserializer methods and any supporting information required by that implementation. All this information can be provided during the creation of table.
* Partition - Each partition can have its own columns and SerDe and storage information.

__Compiler__  
Similar to compilers in traditional databases, the Hive compiler processes HiveQL statements in the following steps:

* Parse – Hive uses Antlr to generate the abstract syntax tree (AST) for the query.
* Type checking and Semantic Analysis – During this phase, the compiler fetches the information of all the input and output tables from the Metastore and uses that information to build a logical plan.
* Optimization – The optimization logic consists of a chain of transformations such that the operator DAG resulting from one transformation is passed as input to the next transformation.
* Generation of the physical plan - The logical plan generated at the end of the optimization phase is then split into multiple map/reduce and hdfs tasks.

Hive converts to a series of MapReduce phases:

* WHERE=>map
* GROUP BY/ORDER BY=> reduce
* JOIN=> map or reduce depending on optimizer

_Example1_ 

    SELECT * FROM purchases WHERE cost > 40
                      ORDER BY order_date DESC;
WHERE clause translates to a “map”, Mapper outputs order_date as key,Single reducer collects sorted rows.  
![mr1](/assets/2013-03-04-hive/mr1.png)  
![mr2](/assets/2013-03-04-hive/mr2.png)  
![mr3](/assets/2013-03-04-hive/mr3.png)  

_Example2_ We show a sample multi-table insert query and its corresponding physical plan after all optimizations below.

    FROM (SELECT a.status, b.school, b.gender FROM status_updates a JOIN profiles b
    ON (a.userid = b.userid
    AND a.ds='2009-03-20' )) subq1

    INSERT OVERWRITE TABLE gender_summary PARTITION(ds='2009-03-20')

    SELECT subq1.gender, COUNT(1) GROUP BY subq1.gender

    INSERT OVERWRITE TABLE school_summary PARTITION(ds='2009-03-20')

    SELECT subq1.school, COUNT(1) GROUP BY subq1.school
![sql](/assets/2013-03-04-hive/sql.png)

[1]:http://incubator.apache.org/ambari/ "Apache Ambari"
[2]:http://flume.apache.org/ "Apache Flume"
[3]:http://hbase.apache.org/ "Apache Hbase"
[4]:http://wiki.apache.org/hadoop/MapReduce "Apache MapReduce"
[5]:http://hadoop.apache.org/docs/r1.1.1/hdfs_desig5.html "HDFS Architecture Guide"
[6]:http://zookeeper.apache.org/ "Apache Zookeeper"
[7]:http://pig.apache.org/ "Apache Pig"
[8]:http://hive.apache.org/ "Apache Hive"
[9]:http://oozie.apache.org/ "Apache Oozie"
[10]:http://sqoop.apache.org/ "Apache Sqoop"
[11]:http://incubator.apache.org/hcatalog/ "Apache Hcatalog"
[12]:http://whirr.apache.org/ "Apache whirr"
[13]:http://mahout.apache.org/ "Apache Mahout"

[20]:http://zhangjunhd.github.com/2013/02/24/apache-related-projects/
[21]:http://zhangjunhd.github.com/2013/02/25/apache-hbase/
[22]:http://zhangjunhd.github.com/2013/03/01/zookeeper/
[23]:http://zhangjunhd.github.com/2013/03/03/pig/
[24]:http://zhangjunhd.github.com/2013/03/04/oozie/
[25]:http://zhangjunhd.github.com/2013/03/04/hive/
[26]:http://zhangjunhd.github.com/2013/03/06/apache-related-projects2/
