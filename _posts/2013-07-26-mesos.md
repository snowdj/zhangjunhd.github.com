---
layout: post
title: "Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center"
description: ""
category: tech
tags: [mesos, paper]
---
{% include JB/setup %}
paper review:[Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center
](http://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-87.pdf)

<!--break-->

##1 Introduction
Mesos lets frameworks choose which resources to use through a distributed scheduling mechanism called `resource offers`.

##3 Architecture
####3.1 Design Philosophy
Because cluster frameworks are both highly diverse and rapidly evolving, our overriding design philosophy has been to **define a minimal interface that enables efficient resource sharing, and otherwise push control to the frameworks**.

####3.2 Overview
Figure 2 shows the components of Mesos. The system consists a `master` process that manages `slave` daemons running on each cluster node. We use ZooKeeper to make the master fault tolerant. Frameworks running on Mesos consist of two components: a `scheduler` that registers with the master to be offered resources, and an `executor` process that is launched on slave nodes to run the framework’s tasks. The main role of the Mesos master is to offer available resources on slaves to framework schedulers through `resource offers`. Each resource offer contains a list of free resources on multiple slaves. Multiple offers describing *disjoint* resource sets can be outstanding at each time. A pluggable allocation module in the master determines *how many* resources to offer to each framework. Frameworks’ schedulers select *which* of the offered resources to use, and describe `tasks` to launch on those resources.
![mesos](/assets/2013-07-26-mesos/mesos.png)

Figure 3 shows an example of how a framework gets scheduled to run a task. 
![mesos2](/assets/2013-07-26-mesos/mesos2.png)

* In step (1), slave 1 reports to the master that it has 4 CPUs and 4 GB of memory free. The master then invokes the allocation policy module, which tells it that framework 1 should be offered all available resources. 
* In step (2) the master sends a resource offer describing what is available on slave 1 to framework 1. 
* In step (3), the framework’s scheduler replies to the master with information about two tasks to run on the slave, using ⟨2 CPUs, 1 GB RAM⟩ for the first task, and ⟨1 CPUs, 2 GB RAM⟩ for the second task. 
* Finally, in step (4), the master sends the tasks to the slave, which allocates appropriate resources to the framework’s executor, which in turn launches the two tasks (depicted with dotted-line borders in the figure). Because 1 CPU and 1 GB of RAM are still unallocated, the allocation module may now offer them to framework 2. In addition, this resource offer process repeats when tasks finish and new resources become free.

####3.3 Resource Allocation
Mesos delegates allocation decisions to a pluggable `allocation module`, so that organizations can tailor allocation to their needs.

**Dominant Resource Fairness (DRF)** We designed a fairness policy called dominant resource fairness (`DRF`), which attempts to equalize each framework’s fractional share of its dominant resource, which is the resource that it has the largest fractional share of. For example, if a cluster contains 100 CPUs and 100 GB of RAM, and framework F1 needs 4 CPUs and 1 GB RAM per task while F2 needs 1 CPU and 8 GB RAM per task, then DRF gives F1 20 tasks (80 CPUs and 20 GB) and gives F2 10 tasks (10 CPUs and 80 GB). This makes F1’s share of CPU equal to F2’s share of RAM, while fully utilizing one resource (RAM). DRF is a natural generalization of `max/min fairness`.

**Supporting Long Tasks** If long tasks are placed arbitrarily throughout the cluster, however, some nodes may become filled with them, preventing other frameworks from accessing local data. To address this problem, Mesos allows allocation modules to bound the total resources on each node that can run long tasks. The amount of long task resources still available on the node is reported to frameworks in resource offers. When a framework launches a task, it marks it as either long or short. **Short tasks can use any resources, but long tasks can only use up to the amount specified in the offer.**

**Revocation** 
Mesos gives its framework a `grace period` to clean it up. We leave it up to the allocation module to implement the `policy` for revoking tasks, but describe two related mechanisms here.

First, while killing a task has a low impact on many frameworks (e.g., MapReduce or stateless web servers), it is harmful for frameworks with interdependent tasks (e.g., MPI). We allow these frameworks to avoid being killed by letting allocation modules expose a `guaranteed allocation` to each framework – a quantity of resources that the framework may hold without losing tasks.

Second, to decide when to trigger revocation, allocation modules must know which frameworks would use more resources if they were offered them.

####3.4 Isolation
Mesos provides performance isolation between frame- work executors running on the same slave by leveraging existing OS isolation mechanisms. We support multiple isolation mechanisms through pluggable `isolation modules`.

####3.5 Making Resource Offers Scalable and Robust
First, because some frameworks will *always* reject certain resources, Mesos lets them short-circuit the rejection process and avoid communication by providing `filters` to the master. We support two types of filters: “only offer nodes from list L” and “only offer nodes with at least R resources free”. 

Second, because a framework may take time to respond to an offer, Mesos counts resources offered to a framework towards its share of the cluster for the purpose of allocation. This is a strong incentive for frameworks to respond to offers quickly and to filter out resources that they cannot use, so that they can get offers for more suitable resources faster.

Third, if a framework has not responded to an offer for a sufficiently long time, Mesos *rescinds* the offer and re-offers the resources to other frameworks.

    ￼￼￼Scheduler Callback       
    resource_offer(offer_id, offers) 
    offer_rescinded(offer_id) 
    status_update(task_id, status) 
    slave_lost(slave_id)
    
    ￼Scheduler Actions
    reply_to_offer(offer_id, tasks, needs_more_offers)
    request_offers() 
    set_filters(filters) 
    get_safe_share() 
    kill_task(task_id)
    
    ￼Executor Callbacks
    launch_task(task_descriptor) 
    kill_task(task_id)
    
    ￼Executor Actions
    send_status(task_id, status)

####3.6 Fault Tolerance
Since the master is a centerpiece of our architecture, we have made it fault-tolerant by pushing state to slaves and schedulers, making the master’s state `soft state`. 

Upon the failure of the master, the slaves and schedulers connect to the newly elected master and help restore its state.

Aside from handling master failures, Mesos reports task, slave and executor failures to frameworks’ schedulers. Frameworks can then react to failures using policies of their choice.

Finally, to deal with scheduler failures, Mesos can be extended to allow a framework to register multiple schedulers such that if one fails, another is notified by the Mesos master and takes over. Frameworks must use their own mechanisms to share state between their schedulers.

##4 Mesos Behavior
####4.1 Definitions, Metrics and Assumptions
In our discussion, we consider three metrics:

* `Framework ramp-up time`: time it takes a new framework to achieve its allocation (e.g., fair share);
* `Job completion time`:time it takes a job to complete, assuming one job per framework;
* `System utilization`: total cluster utilization.

We characterize workloads along four attributes:

* `Scale up`: Frameworks can elastically increase their allocation to take advantage of free resources.
* `Scale down`: Frameworks can relinquish resources without significantly impacting their performance.
* `Minimum allocation`: Frameworks require a certain minimum number of slots before they can start using their slots.
* `Task distribution`: The distribution of the task durations. We consider both homogeneous and heterogeneous distributions.

We also differentiate between two types of resources: *required* and *preferred*.

####4.2 Homogeneous Tasks
The challenge with Mesos is that the scheduler does not know the preferences of each framework. Fortunately, it turns out that there is an easy way to achieve the fair allocation of the preferred slots described above: simply offer slots to frameworks proportionally to their intended allocations. In particular, when a slot becomes available, Mesos offers that slot to framework i with probability `\(\frac{s_i}{\sum_{i=1}^n s_i}\)` , where n is the total number of frameworks in the system. Note that this scheme is similar to `lottery scheduling`.

####4.3 Heterogeneous Tasks
Mesos differentiates between short and long slots, and bounds the number of long slots on each node. This ensures there are enough short tasks on each node whose slots become available with high frequency, giving frameworks better opportunities to quickly acquire a slot on one of their preferred nodes. In addition, Mesos implements a revocation mechanism that does not differentiate between long and short tasks once a framework exceeds its allocation.

####4.4 Framework Incentives
As with any decentralized system, it is important to understand the incentives of various entities in the system. In this section, we discuss the incentives of a framework to improve the response times of its jobs.

* Short tasks
* No minimum allocation
* Scale down
* Do not accept unknown resources

####4.5 Limitations of Distributed Scheduling

* Fragmentation
* Interdependent framework constraints
* Framework complexity