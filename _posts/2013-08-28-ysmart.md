---
layout: post
title: "YSmart"
description: ""
category: tech
tags: [SQL, paper, facebook, MapReduce]
---
{% include JB/setup %}
paper review:[YSmart: Yet Another SQL-to-MapReduce Translator](http://www.cse.ohio-state.edu/hpcs/WWW/HTML/publications/papers/TR-11-7.pdf)

<!--break-->
##I. INTRODUCTION
####The Performance Gap
This query (Q-CSA) is used to answer “what is the average number of pages a user visits between a page in category X and a page in category Y?” based on a single click-stream table CLICKS(user id int, page id int, category id int, ts timestamp). It is a complex query that needs self-joins and multiple aggregations of the same table. Its SQL statement is shown in Fig. 1, and its execution plan tree is shown in Fig. 2(a). To demonstrate the performance gap, we also used a simple query (Q-AGG) that counts the number of clicks for each category. It only executes an aggregation with one pass of table scan on CLICKS.

![1](/assets/2013-08-28-ysmart/1.png)

Fig. 2(b) shows the experiment results. For the simple Q-AGG query, Hive has comparable performance with our hand-coded program. However, for query Q-CSA, the hand-coded MapReduce program outperforms Hive by almost three times. In fact, Hive generates a chain of MapReduce jobs according to the query plan, and each job is independently responsible for executing one operation in the plan tree. However, our hand-coded program, on the basis of query semantic analysis, uses only a single job to execute all the operations except the final aggregation (AGG4). This significantly reduces redundant computations and I/O operations in the MapReduce execution.

![2](/assets/2013-08-28-ysmart/2.png)

####Translating SQL to MapReduce: Where Is the Bottleneck?
In practice, when translating a query expressed by such a language into MapReduce programs, existing translators take a **one-operation-to-one-job approach**. For a query plan tree, each operation in the tree is replaced by a pre-implemented MapReduce program, and the tree is finally translated into a chain of programs. For example, Hive generates six jobs to execute the six operations (JOIN1, AGG1, AGG2, JOIN2, AGG3, and AGG4) in the plan tree shown in Fig. 2(a). Such a translation approach is inefficient since it can cause redundant table scans (e.g., both JOIN1 and JOIN2 need to scan CLICKS) and unnecessary data transfers among multiple jobs. Thus, existing translators cannot generate high-performance MapReduce programs for two reasons. First, they cannot address the limitations of the simple MapReduce structure for a complex query. Second, they cannot utilize the unique opportunities provided by intra-query correlations in a complex query.

##III. CORRELATION-AWARE MAPREDUCE: AN OVERVIEW
YSmart batch-processes multiple correlated query operations within a query thus significantly reduces unnecessary computations, disk I/Os and network transfers. During job generation, YSmart applies a set of optimization rules to merge multiple jobs, which otherwise would have been run independently without YSmart, into a common job. It provides a `Common MapReduce Framework` (CMF) that allows multiple types of jobs, e.g., a join job and an aggregation job, to be executed in a common job. The CMF has low overhead on managing multiple merged jobs.To achieve its goals, YSmart must address the following three issues (in the next three sections, respectively):

1. What types of correlations exist in a query and how can they affect query execution performance?
2. With the awareness of correlations, how to translate a query plan tree into efficient MapReduce programs?
3. How to design and implement the Common MapReduce Framework that need to merge different types of jobs with low overhead?

##IV. INTRA-QUERY CORRELATIONS AND THEIR OPTIMIZATION PRINCIPLES
####A. Types of Correlations and the Optimization Benefits
For an operation node in a query plan tree, YSmart introduces a property `Partition Key` (PK) to reflect how map output is partitioned in the operation execution with MapReduce’s key/value pair model. Since a map function is to transform (k1, v1) to (k2, v2), the partition key actually represents k2.

In a query plan tree, we define three correlations:

1. `Input Correlation`: Multiple nodes have input correlation (IC) if their input relation sets are not disjoint;
2. `Transit Correlation`: Multiple nodes have transit correlation (TC) if they have not only input correlation, but also the same partition key;
3. `Job Flow Correlation`: A node has job flow correlation (JFC) with one of its child nodes if it has the same partition key as that child node.

If an aggregation node has multiple partition key candidates, YSmart has to determine which one is its partition key. Currently YSmart does not seek a solution based on execution cost estimations due to the lack of statistics information of data sets. Rather, YSmart uses a simple heuristic by selecting the one that can connect the maximal number of nodes that can have these correlations.

These correlations between nodes provide an opportunity so that the jobs for the nodes can be batch-processed to improve efficiency. First, if two nodes have input correlation, then the corresponding two jobs can share the same table scan during the map phase. This can either save disk reads if the map is local or save network transfers if the map is remote. Second, if two nodes have transit correlation, then there exists overlapped data between map outputs of the jobs. Thus, during a map- to-reduce transition, redundant disk I/O and network transfers can be avoided. Finally, if a node has a job flow correlation with one of its child nodes, then it is possible that the node actually can be directly evaluated in the reduce phase of the job for the child node. Specifically, in this case of exploiting job flow correlation, there are following scenarios:

1. An aggregation node with grouping can be directly executed in the reduce function of its only child node;
2. A join node J1 has job flow correlation with only one of its child nodes C1. Thus as long as the job of another child node of this join node C2 has been completed, a single job is sufficient to execute both C1 and J1;
3. A join node J1 has job flow correlation with two child nodes C1 and C2. Then, according to the correlation definitions, C1 and C2 must have both input correlation and transit correlation. Thus a single job is sufficient to execute both C1 and C2. Besides, J1 can also be directly executed in the reduce phase of the job.

####B. An Example of Correlation Query and Its Optimization

![3](/assets/2013-08-28-ysmart/3.png)

Fig. 5 shows the three jobs: Job1 for AGG1, Job2 for JOIN1, and Job3 for JOIN2. In each job, the map function transforms an input record to a key/value pair. For example, Job1’s map function transforms a lineitem record to a key/value pair that uses column l partkey as the key and column l quantity as the value. The reduce function is the actual worker for aggregation or join. For example, Job1’s reduce function executes aggregation on l quantity for each unique input key (l partkey).

![4](/assets/2013-08-28-ysmart/4.png)

We can determine the correlations among the nodes by looking into their corresponding MapReduce jobs. First, both AGG1 and JOIN1 need the input of the lineitem table, which means these two nodes have input correlation. Second, AGG1 and JOIN1 have the same partition key l partkey. This fact can be reflected by the map output key/value pairs in Job1 and Job2. Both jobs use l partkey to partition their input table lineitem. Based on correlation definitions, AGG1 and JOIN1 have transit correlation. Finally, as the parent node of AGG1 and JOIN1, JOIN2 has the same partition key l partkey as all its child nodes. As shown in the map phase of Job3, l partkey is used to partition outer and inner, thus JOIN2 has job flow correlation with both AGG1 and JOIN1.

By exploiting these correlations, instead of generating three independent jobs, YSmart only needs to use a single MapReduce job to execute all functionalities of AGG1, JOIN1, and JOIN2, as shown in Fig. 6. Such job merging has two advantages. First, by exploiting input correlation and transit correlation, AGG1 and JOIN1 can share a single scan of the lineitem table, and remove redundant map outputs. Second, JOIN2 can be directly executed in the reduce phase of the job. Therefore, the persistence and re-partitioning of intermediate tables inner and outer are actually avoided, which can significantly boost the performance of the query.

![5](/assets/2013-08-28-ysmart/5.png)

##V. JOB GENERATION IN YSMART
####A. Primitive Job Types
Based on the programming flexibility of MapReduce, YSmart provides four types of MapReduce jobs for different operations.

* A SELECTION-PROJECTION (SP) Job is used to execute a simple query with only selection and projection operations on a base relation;
* An AGGREGATION (AGG) job is used to execute aggregation and grouping on an input relation;
* A JOIN job is used to execute an equi-join (inner or left/right/full outer) of two input relations;
* A SORT job is used to execute a sorting operation.

####B. Job Merging
With the awareness of the three intra-query correlations, YSmart provides a set of rules to merge multiple jobs into a common job. The merging of jobs can either be at the map phase or at the reduce phase, performed in two different steps – the first step applies for input correlation and transit correlation, and the second step applies for job flow correlation.

* Rule 1: If two jobs have input correlation and transit correlation, they will be merged into a common job. This is performed in the first step, where YSmart scans the chain of jobs generated from the above one-to-one translation. This process continues until there is no more input correlation and transit correlation between any jobs in the chain. After this step, YSmart will continue the second step to detect if there are jobs that can be merged in the reduce phase of a prior job.
* Rule 2: An AGGREGATION job that has job flow correlation with its only preceding job will be merged into this preceding job.
* Rule 3: For a JOIN job with job flow correlation with its two preceding jobs, the join operation will be merged into the reduce phase of the common job. In this case, there must be transit correlation between the two preceding jobs, and the two jobs have been merged into a common job in the first step. Based on this, the join operation can be put into the reduce phase of the common job.
* Rule 4: For a JOIN job that has job flow correlation with only one of its two preceding jobs, merge the JOIN job with the preceding job with job flow correlation – which has to be executed later than the other one. For example, a JOIN job J1 has job flow correlation with P1 but not P2. In this case, J1 can be merged into P1 only when P2 was finished before P1. In this case, YSmart needs to determine the sequence of executing two preceding jobs for a JOIN job. That is, the preceding job that has no job flow correlation with the JOIN job must be executed first. YSmart implements this rule when traversing the query plan tree with post-order. For a join node, its left child and right child can be exchanged in this case.

####C. An Example of Job Merging
We take the query plans shown in Fig. 7 as an example to demonstrate the job merging process. The difference between the two plans is that the left child and right child of node JOIN2 are exchanged. We assume that 1) JOIN1 and AGG2 have input correlation and transit correlation, 2) JOIN2 has job flow correlation with JOIN1 but not AGG1, and 3) JOIN3 has job flow correlation with both JOIN2 and AGG2. In the figure, we show the job number for each node.

![6](/assets/2013-08-28-ysmart/6.png)

For the plan in Fig. 7 (a), a post-order traverse will generatefive jobs in a sequence {J1, J2, J3, J4, J5}. In the first step to use input correlation and transit correlation, J1 and J4 will be merged. Thus, the job sequence becomes{J1+4,J2,J3,J5}. In the second step to use job flow correlation, J5 will be merged into J3 since when J3 begins J4 has already finished in the merged job J1+4. Thus, finally we get three jobs in a sequence{J1+4,J2,J3+5}. However, since YSmart uses Rule 4 to exchange J1 and J2, the plan can be automatically transformed to the plan in Fig. 7 (b).

For the plan in Fig. 7 (b), since J2 is finished before J1, the plan can be further optimized by maximally using job flow correlation. The initial job sequence is{J2,J1,J3,J4,J5}. After the first step that merges J1 and J4, the sequence is {J2,J1+4,J3,J5}. At the second step, since J2 has finished, J3 can be directly executed in the job J1+4. Furthermore, J5 can also be merged into the job. Therefore, the final job sequence is {J2, J1+4+3+5} with only two jobs.

##VI. THE COMMON MAPREDUCE FRAMEWORK
The `Common MapReduce Framework` (CMF) is the foundation of YSmart to use a common job to execute functionalities of multiple correlated jobs. CMF addresses two major requirements in optimizing and running translated jobs.

The first requirement is to provide a flexible framework to allow different types of MapReduce jobs, for example a JOIN job and an AGGREGATION job, to be plugged into a common job. Therefore, the map and reduce function of a common job must have the ability to execute multiple different codes belonging to independent jobs.

The second requirement is to execute multiple merged jobs in a common job with minimal overhead. Since a common job needs to manage all computations and input/output of its merged jobs, the common job needs to bookkeep necessary information to keep track of every piece of data and their corresponding jobs, and provides efficient data dispatching for merged jobs. Due to the intermediate materialization limitation of MapReduce, any additional information generated by the common job will be written to local disks and transferred over the network. Thus, CMF needs to minimize the bookkeeping information to minimize the overhead.

CMF provides a general template based approach to generate a common job that can merge a collection of correlated jobs. The template has the following structures. The `common mapper` executes operations (selection and/or projection operations) involved in the map functions of merged jobs. The `common reducer` executes all the operations (e.g. join or aggregation) involved in the reduce functions of merged jobs. The `post-job computation` is a subcomponent in the common reducer to execute further computations on the outputs of merged jobs.

####A. Common Mapper
A common map function accepts a line (a record) in the raw data file as an input. Then it emits a common key/value pair that would contain all the required data for all the merged jobs. (The pair could be null if nothing is selected.)

Since different merged jobs can have different projected columns, and different jobs can have different selection conditions, the common mapper needs to record which part should be dispatched to which query in the reduce phase. Such additional bookkeeping information can bring overhead caused by intermediate result materialization in MapReduce. To minimize the overhead, CMF takes the following approaches. First, the projection information is kept as a job-level configuration property since this information is fixed and record-independent for each job. Second, for each value in the output key/value pair, CMF adds a tag about which job should use this pair in the reduce phase. Since each tag is record-dependent, their aggregated size cannot be ignored if a large number of pairs are emitted by the common mapper. Therefore, in our implementation, a tag only records the IDs of jobs (if they exist) that should not see this pair in their reduce phases. This could support common cases with highly overlapped map outputs among jobs.

####B. Common Reducer and Post-job Computations
A common reduce function does not limit what a merged reducer (i.e., the reduce function of a merged job) can do. The core task of the common reducer is to iterate the input list of values, and dispatch each value with projections into the corresponding reducers that need the value. CMF requires a merged reducer be implemented with three interfaces: (1) an init function, (2) a next function driven by each value, and (3) a final function that does computations for all received values. This approach has two advantages: It is general and allows any types of reducers to be merged in the common reducer; It is efficient since it only needs one pass of iterations on the list of values. The common reducer outputs each result of a merged reducer to the HDFS, and an additional tag is used for each output key/value pair to distinguish its source.

However, in the common reduce function, if another job (say Ja) has job flow correlation to these merged jobs, it can be instantly executed by a post-job computation step in the function, so that Ja would not be initiated as an independent MapReduce job. In this case, the results of the merged jobs would not be outputted, but are treated as temporary results and consumed by Ja. Thus, the common reducer only outputs the results of Ja. (See Algorithm 1 for the workflow).

![7](/assets/2013-08-28-ysmart/7.png)



