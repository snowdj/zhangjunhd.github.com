---
layout: post
title: "RCFile"
description: ""
category: tech
tags: [column, facebook, MapReduce]
---
{% include JB/setup %}
paper review:[RCFile: A Fast and Space-efÔ¨Åcient Data Placement
Structure in MapReduce-based Warehouse Systems](http://www.cse.ohio-state.edu/hpcs/WWW/HTML/publications/papers/TR-11-4.pdf)

<!--break-->
##I. INTRODUCTION
####A. Big Data Processing Requirements

* Fast data loading.
* Fast query processing.
* Highly efficient storage space utilization.
* Strong adaptivity to highly dynamic workload patterns.

####B. Data Placement for MapReduce

* horizontal row-store structure
* vertical column-store structure
* hybrid PAX store structure

In this paper, we present our data placement structure, called RCFile (Record Columnar File), and its implementation in Hadoop.

* A table stored in RCFile is first horizontally partitioned into multiple `row groups`. Then, each row group is vertically partitioned so that each column is stored independently.
* RCFile utilizes a column-wise data compression within each row group, and provides a `lazy decompression` technique to avoid unnecessary column decompression during query execution.
* RCFile allows a flexible row group size. A default size is given considering both data compression performance and query execution performance. RCFile also allows users to select the row group size for a given table.

##II. MERITS AND LIMITATIONS OF EXISTING DATA PLACEMENT STRUCTURES
####A. Horizontal Row-store

![rc1](/assets/2013-03-07-rcfile/rc1.png)

The major weaknesses

* First, row-store cannot provide fast query processing due to unnecessary column reads if only a subset of columns in a table are needed in a query.
* Second, it is not easy for row-store to achieve a high data compression ratio (and thus a high storage space utilization) due to mixed columns with different data domains.

The major advantage of row-store for a Hadoop-based system is that it has fast data loading and strong adaptive ability to dynamic workloads. This is because row-store guarantees that all fields in the same record is located in the same cluster node since they are in the same HDFS block.

####B. Vertical Column-store
Basically, there are two schemes of vertical stores. One scheme is to put each column in one sub-relation, such as the Decomposition Storage Model (DSM), MonetDB. Another scheme is to organize all the columns of a relation into different column groups, and usually allow column overlapping among multiple column groups, such as C-store and Yahoo Zebra. In this paper, we call the first scheme the `column-store`, and the second one the `column-group`. 

Figure 2 shows an example on how a table is stored by column-group on HDFS.

![rc2](/assets/2013-03-07-rcfile/rc2.png)

Column-store can avoid reading unnecessary columns during a query execution, and can easily achieve a high compression ratio by compressing each column within the same data domain. However, it cannot provide fast query processing in Hadoop-based systems due to high overhead of a tuple reconstruction. Column-store cannot guarantee that all fields in the same record are located in the same cluster node. 

Since a column group is equivalent to a materialized view, it can avoid the overhead of a record reconstruction. However, it cannot satisfy the requirement of quickly adapting dynamic workloads, unless all column groups have been created with the pre-knowledge of possible queries.

####C. Hybrid Store: PAX
PAX and its improvement in Data Morphing use a hybrid placement structure aiming at improving CPU cache performance. For a record with multiple fields from different columns, instead of putting these fields into different disk pages, PAX puts them in a single disk page to save additional operations for record reconstructions. Within each disk page, PAX uses a mini-page to store all fields belonging to each column, and uses a page header to store pointers to mini-pages.

Like row-store, PAX has a strong adaptive ability to various dynamic query workloads.

PAX cannot directly satisfy the requirements of both high storage space utilization and fast query processing speed on large distributed systems.

##III. THE DESIGN AND IMPLEMENTATION OF RCFILE
####A. Data Layout and Compression

![rc3](/assets/2013-03-07-rcfile/rc3.png)

As demonstrated in the example shown in Figure 3, RCFile has the following data layout to store a table:

* According to the HDFS structure, a table can have multiple HDFS blocks.
* In each HDFS block, RCFile organizes records with the basic unit of a `row group`. That is to say, all the records stored in an HDFS block are partitioned into row groups. For a table, all row groups have the same size. 
* A row group contains three sections. The first section is a sync marker that is placed in the beginning of the row group. The sync marker is mainly used to separate two continuous row groups in an HDFS block. The second section is a metadata header for the row group. The metadata header stores the information items on how many records are in this row group, how many bytes are in each column, and how many bytes are in each field in a column. The third section is the table data section that is actually a column-store. 

We now introduce how data is compressed in RCFile.

* First, for the whole metadata header section, RCFile uses the RLE (Run Length Encoding) algorithm to compress data. 
* Second, the table data section is not compressed as a whole unit. Rather, each column is independently compressed with the Gzip compression algorithm.

####B. Data Appending
RCFile does not allow arbitrary data writing operations. Only an appending interface is provided for data writing in RCFile.

* RCFile creates and maintains an in-memory `column holder` for each column. When a record is appended, all its fields will be scattered, and each field will be appended into its corresponding column holder. In addition, RCFile will record corresponding metadata of each field in the metadata header.
* RCFile provides two parameters to control how many records can be buffered in memory before they are lushed into the disk. One parameter is the limit of the number of records, and the other parameter is the limit of the size of the memory buffer.
* RCFile first compresses the metadata header and stores it in the disk. Then it compresses each column holder separately, and flushes compressed column holders into one row group in the underlying file system.

####C. Data Reads and Lazy Decompression
When processing a row group, RCFile does not need to fully read the whole content of the row group into memory. Rather, it only reads the metadata header and the needed columns in the row group for a given query. Thus, it can skip unnecessary columns and gain the I/O advantages of column-store.

After the metadata header and data of needed columns have been loaded into memory, they are all in the compressed format and thus need to be decompressed. The metadata header is always decompressed and held in memory until RCFile processes the next row group. However, RCFile does not decompress all the loaded columns. Instead, it uses a lazy decompression technique.

Lazy decompression means that a column will not be decompressed in memory until RCFile has determined that the data in the column will be really useful for query execution. Lazy decompression is extremely useful due to the existence of various where conditions in a query. If a where condition cannot be satisfied by all the records in a row group, then RCFile does not decompress the columns that do not occur in the where condition. 

####D. Row Group Size
There are two considerations to determine the row group size:

* A large row group size can have better data compression efficiency than that of a small one. However, according to our observations of daily applications in Facebook, when the row group size reaches a threshold, increasing the row group size cannot further improve compression ratio with the Gzip algorithm.
* A large row group size may have lower read performance than that of a small size because a large size can decrease the performance benefits of lazy decompression. Furthermore, a large row group size would have a higher memory usage than a small size, and would affect executions of other co-running MapReduce jobs.
