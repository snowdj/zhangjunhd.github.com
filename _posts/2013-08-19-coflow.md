---
layout: post
title: "Coflow-An Application Layer Abstraction for Cluster Networking"
description: ""
category: tech
tags: [Coflow]
---
{% include JB/setup %}
paper review:[Coflow-An Application Layer Abstraction for Cluster Networkin](http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-184.pdf)

<!--break-->
##1 Introduction
Based on our observations, in this paper, we propose `coflow`, an application layer networking abstraction that captures diverse communication patterns observed in cluster computing applications.

##2 Communication in Cluster Applications
####2.1 MapReduce
Given m mappers and r reducers, a MapReduce job will create m Ã— r flows for the shuffle and at least r flows for output replication. The primary characteristic of communication in the MapReduce model is that the job will not finish until the last reducer has finished [12]. Consequently, there is an explicit barrier at the end of the job, which researchers have exploited for optimizing communication in this model [12]. 

####2.2 Dataflow Pipelines
* **Dataflow with Barriers** The most straightforward extension was to create dataflow pipelines with multiple stages by using MapReduce as the building block (e.g., Sawzall, Pig, Hive). This introduced barriers at the end of each these building blocks; communication optimizations for the MapReduce model are still valid in this model.
* **Streaming Dataflow** To avoid explicit barriers and to enable higher-level optimizations of the operators, streaming data flow pipelines were introduced (e.g., Dryad, DryadLINQ, SCOPE, FlumeJava, MapReduce Online). In this model, the next stage can start as soon as some input is available. Because there is no explicit barrier, optimization techniques that depend on it are not useful. Instead, networking researchers focused on understanding the internals of the communication and optimizing them for specific scenarios [6, 33].
* **Data flow with Cycles** Since traditional data flow pipelines do not support cycles, they depend on unrolling loops to support iterative computation requirements. Spark and its variants [30, 31] introduced cycles without unrolling by keeping states around in memory. In the process, they introduced communication primitives like broadcast and many-to-one aggregation. Implicit barriers at the end of each iteration allowed communication optimizations similar to that in MapReduce [12].

####2.3 Bulk Synchronous Parallel (BSP)
Bulk Synchronous Parallel or BSP is another popular model in cluster computing with specific focus in graph processing, matrix computation, and network algorithms (e.g., Pregel, Giraph, Hama). A BSP computation proceeds in a series of global supersteps, each containing three ordered stages: concurrent computation, communication between processes, and barrier synchronization. With explicit barriers at the end of each superstep, the communication stage can be globally optimized for the superstep.

####2.4 Asynchronous Models
There are asynchronous cluster computing frameworks as well. Sometimes complete information is not needed for reasonably good results and iterations can proceed with partial results. GraphLab is such a framework for machine learning and data mining on graphs. Unlike BSP supersteps, iterations can proceed with whatever information is available as long as it converging; missing information can asynchronously arrive later.

####2.5 Partition-Aggregate
User-facing online services (e.g., search results in Google or Bing, home feed in Facebook) receive requests from users and send it downward to the workers using an aggregation tree. At each level of the tree, each request generates activities in different partitions. Ultimately, components generated by the workers are aggregated and sent back to the user within strict deadlines. Components that cannot make it within the deadline are either left behind [27] or sent later asynchronously (e.g., Facebook home feed). Research in this direction looked at dropping flows [27], preemption [18], and cutting the tails [32]; however, they do not exploit any application-level information.

####2.6 Summary of Communication Requirements

![coflow1](/assets/2013-08-19-coflow/coflow1.png)

![coflow2](/assets/2013-08-19-coflow/coflow2.png)

##3 An Application Layer Abstraction
Any collection of flows will not do, however. Three conditions must be satisfied: first, there must be higher-level semantics - made available by the application layer - that apply to all the flows in a collection and allow a combined decision; second, flows have to be concurrent within a reasonable time window for them to be considered together; finally, all the flows should have a shared objective that allows joint optimazations on the aggregate. We refere to such collections of flows as `coflows` and define them as the following:
     
    A coflow is a semantically-bound collection of concurrent flows 
    with a shared objective.

![coflow3](/assets/2013-08-19-coflow/coflow3.png)

##4 Applications of the Abstraction
####4.2 Interactions Between Coflows
* Sharing
* Prioritization
* Preemption
* Ordering

####4.4 Immediate Research Agenda
* Decide on and Develop a Coordination Mechanism
* Optimize Common Coflows
* Define Interfaces

##References
* [12] M.Chowdhury et al. Managing data transfers in computer clusters with Orchestra. In SIGCOMM, 2011.
* [6] S.Agarwal et al. Reoptimizing data parallel computing.In NSDI, 2012.
* [33] J.Zhang et al. Optimizing data shuffling in data-parallel computation by understanding user-defined functions. In NSDI, 2012.
* [30] M. Zaharia et al. Discretized streams: An efficient and fault-tolerant model for stream processing on large clusters. In HotCloud, 2012.
* [31] M. Zaharia et al. Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing. In NSDI, 2012.
* [18] C.-Y.Hong,M.Caesar,andP.B.Godfrey. Finishing flows quickly with preemptive scheduling. In SIGCOMM, 2012.
* [27] C. Wilson et al. Better never than late: Meeting deadlines in datacenter networks. In SIGCOMM, 2011.
* [32] D.Zatsetal.DeTail: Reducing the flow completion time tail in datacenter networks. In SIGCOMM, 2012.