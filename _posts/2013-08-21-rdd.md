---
layout: post
title: "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing"
description: ""
category: tech
tags: [paper, spark]
---
{% include JB/setup %}
paper review:[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computin](http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf)

<!--break-->
##2 Resilient Distributed Datasets (RDDs)
####2.1 RDD Abstraction
Formally, an RDD is a read-only, partitioned collection of records. RDDs can only be created through deterministic operations on either (1) data in stable storage or (2) other RDDs. We call these operations `transformations` to differentiate them from other operations on RDDs. Examples of transformations include map, filter, and join.

RDDs do not need to be materialized at all times. Instead, an RDD has enough information about how it was derived from other datasets (its `lineage`) to compute its partitions from data in stable storage. This is a powerful property: in essence, a program cannot reference an RDD that it cannot reconstruct after a failure.

Finally, users can control two other aspects of RDDs: `persistence` and `partitioning`. Users can indicate which RDDs they will reuse and choose a storage strategy for them (e.g., in-memory storage). They can also ask that an RDD’s elements be partitioned across machines based on a key in each record. This is useful for placement optimizations, such as ensuring that two datasets that will be joined together are hash-partitioned in the same way.

####2.2 Spark Programming Interface
Programmers start by defining one or more RDDs through `transformations` on data in stable storage (e.g., `map` and `filter`). They can then use these RDDs in `actions`, which are operations that return a value to the application or export data to a storage system. Examples of actions include `count` (which returns the number of elements in the dataset), `collect` (which returns the elements themselves), and `save` (which outputs the dataset to a storage system).

![rdd1](/assets/2013-08-21-rdd/rdd1.png)

Scala code:
    
    lines = spark.textFile("hdfs://...")
    errors = lines.filter(_.startsWith("ERROR"))
    errors.persist()
    
Line 1 defines an RDD backed by an HDFS file (as a collection of lines of text), while line 2 derives a filtered RDD from it. Line 3 then asks for errors to persist in memory so that it can be shared across queries. 

At this point, no work has been performed on the cluster. However, the user can now use the RDD in actions, e.g., to count the number of messages:

    errors.count()

The user can also perform further transformations on the RDD and use their results, as in the following lines:

    // Count errors mentioning MySQL:
    errors.filter(_.contains("MySQL")).count()
    // Return the time fields of errors mentioning
    // HDFS as an array (assuming time is field
    // number 3 in a tab-separated format):
    errors.filter(_.contains("HDFS"))
      .map(_.split(’\t’)(3))
      .collect()

After the first action involving errors runs, Spark will store the partitions of errors in memory, greatly speeding up subsequent computations on it. Note that the base RDD, lines, is not loaded into RAM. This is desirable because the error messages might only be a small fraction of the data (small enough to fit into memory).

Finally, to illustrate how our model achieves fault tolerance, we show the lineage graph for the RDDs in our third query in Figure 1. In this query, we started with errors, the result of a filter on lines, and applied a further filter and map before running a collect. The Spark scheduler will pipeline the latter two transformations and send a set of tasks to compute them to the nodes holding the cached partitions of errors. In addition, if a partition of errors is lost, Spark rebuilds it by applying a filter on only the corresponding partition of lines.

####2.3 Advantages of the RDD Model
The main difference between RDDs and DSM(distributed shared memory) is that RDDs can only be created (“written”) through coarse-grained transformations, while DSM allows reads and writes to each memory location. This restricts RDDs to applications that perform bulk writes, but allows for more efficient fault tolerance.

A second benefit of RDDs is that their immutable nature lets a system mitigate slow nodes (stragglers) by running backup copies of slow tasks as in MapReduce.

Finally, RDDs provide two other benefits over DSM. First, in bulk operations on RDDs, a runtime can schedule tasks based on data locality to improve performance. Second, RDDs degrade gracefully when there is not enough memory to store them, as long as they are only being used in scan-based operations. Partitions that do not fit in RAM can be stored on disk and will provide similar performance to current data-parallel systems.

####2.4 Applications Not Suitable for RDDs
RDDs would be less suitable for applications that make asynchronous fine-grained updates to shared state, such as a storage system for a web application or an incremental web crawler. For these applications, it is more efficient to use systems that perform traditional update logging and data check-pointing, such as databases, RAMCloud, Percolator and Piccolo. 

##3 Spark Programming Interface
To use Spark, developers write a driver program that connects to a cluster of workers, as shown in Figure 2. The driver defines one or more RDDs and invokes actions on them. Spark code on the driver also tracks the RDDs’ lineage. The workers are long-lived processes that can store RDD partitions in RAM across operations.

![rdd2](/assets/2013-08-21-rdd/rdd2.png)

####3.1 RDD Operations in Spark
Table 2 lists the main RDD transformations and actions available in Spark. We give the signature of each operation, showing type parameters in square brackets. Recall that `transformations` are lazy operations that define a new RDD, while `actions` launch a computation to return a value to the program or write data to external storage.

![rdd3](/assets/2013-08-21-rdd/rdd3.png)

In addition to these operators, users can ask for an RDD to persist. Furthermore, users can get an RDD’s partition order, which is represented by a Partitioner class, and partition another dataset according to it. Operations such as groupByKey, reduceByKey and sort automatically result in a hash or range partitioned RDD.

##4 Representing RDDs
n a nutshell, we propose representing each RDD through a common interface that exposes five pieces of information: a set of partitions, which are atomic pieces of the dataset; a set of dependencies on parent RDDs; a function for computing the dataset based on its parents; and metadata about its partitioning scheme and data placement. For example, an RDD representing an HDFS file has a partition for each block of the file and knows which machines each block is on. Meanwhile, the result of a map on this RDD has the same partitions, but applies the map function to the parent’s data when computing its elements. We summarize this interface in Table 3.

![rdd4](/assets/2013-08-21-rdd/rdd4.png)

The most interesting question in designing this interface is how to represent dependencies between RDDs. We found it both sufficient and useful to classify dependencies into two types: `narrow` dependencies, where each partition of the parent RDD is used by at most one partition of the child RDD, `wide` dependencies, where multiple child partitions may depend on it. For example, map leads to a narrow dependency, while join leads to to wide dependencies (unless the parents are hash-partitioned). Figure 4 shows other examples.

![rdd5](/assets/2013-08-21-rdd/rdd5.png)

We sketch some RDD implementations below.

* **HDFS files**: The input RDDs in our samples have been files in HDFS. For these RDDs, partitions returns one partition for each block of the file (with the block’s offset stored in each Partition object), preferredLocations gives the nodes the block is on, and iterator reads the block.
* **map**: Calling map on any RDD returns a MappedRDD object. This object has the same partitions and preferred locations as its parent, but applies the function passed to map to the parent’s records in its iterator method.
* **union**: Calling union on two RDDs returns an RDD whose partitions are the union of those of the parents. Each child partition is computed through a narrow dependency on the corresponding parent.
* **sample**: Sampling is similar to mapping, except that the RDD stores a random number generator seed for each partition to deterministically sample parent records.
* **join**: Joining two RDDs may lead to either two narrow dependencies (if they are both hash/range partitioned with the same partitioner), two wide dependencies, or a mix (if one parent has a partitioner and one does not). In either case, the output RDD has a partitioner (either one inherited from the parents or a default hash partitioner).

##5 Implementation
####5.1 Job Scheduling
Spark’s scheduler uses our representation of RDDs, described in Section 4.

![rdd6](/assets/2013-08-21-rdd/rdd6.png)

Overall, our scheduler is similar to Dryad’s, but it additionally takes into account which partitions of persistent RDDs are available in memory. Whenever a user runs an action (e.g., count or save) on an RDD, the scheduler examines that RDD’s lineage graph to build a DAG of stages to execute, as illustrated in Figure 5. Each stage contains as many pipelined transformations with narrow dependencies as possible. The boundaries of the stages are the shuffle operations required for wide dependencies, or any already computed partitions that can shortcircuit the computation of a parent RDD. The scheduler then launches tasks to compute missing partitions from each stage until it has computed the target RDD.

Our scheduler assigns tasks to machines based on data locality using delay scheduling. If a task needs to process a partition that is available in memory on a node, we send it to that node. Otherwise, if a task processes a partition for which the containing RDD provides preferred locations (e.g., an HDFS file), we send it to those.

For wide dependencies (i.e., shuffle dependencies), we currently materialize intermediate records on the nodes holding parent partitions to simplify fault recovery, much like MapReduce materializes map outputs.

If a task fails, we re-run it on another node as long as its stage’s parents are still available. If some stages have become unavailable (e.g., because an output from the “map side” of a shuffle was lost), we resubmit tasks to compute the missing partitions in parallel. We do not yet tolerate scheduler failures, though replicating the RDD lineage graph would be straightforward.

Finally, although all computations in Spark currently run in response to actions called in the driver program, we are also experimenting with letting tasks on the cluster (e.g., maps) call the lookup operation, which provides random access to elements of hash-partitioned RDDs by key. In this case, tasks would need to tell the scheduler to compute the required partition if it is missing.